#+TITLE: High-performance ocean wave simulation model for studying marine object behaviour
#+AUTHOR: Ivan Gankevich
#+DATE: St. Petersburg, 2016
#+LANGUAGE: en
#+LATEX_CLASS: gost
#+LATEX_CLASS_OPTIONS: [hidelinks,fontsize=14pt,paper=a4,pagesize,DIV=calc]
#+LATEX_HEADER_EXTRA: \input{preamble}
#+LATEX_HEADER_EXTRA: \organization{Saint Petersburg State University}
#+LATEX_HEADER_EXTRA: \manuscript{}
#+LATEX_HEADER_EXTRA: \degree{thesis for candidate of sciences degree}
#+LATEX_HEADER_EXTRA: \speciality{Speciality 05.13.18\\Mathematical modeling, numerical methods and programme complexes}
#+LATEX_HEADER_EXTRA: \supervisor{Supervisor\\Alexander Degtyarev}
#+LATEX_HEADER_EXTRA: \newcites{published}{Publications on the subject of thesis}
#+OPTIONS: todo:nil title:nil ':t H:5
#+STARTUP: indent

* Config                                                           :noexport:
** Produce data for Q-Q and ACF plots
#+begin_src sh :exports none :results verbatim
root=$(pwd)
for testname in propagating_wave standing_wave
do
    wd=$root/build/$testname
    rm -rf $wd
    mkdir -p $wd
    cd $wd
    arma -c $root/config/$testname.arma 2>&1
done
#+end_src

#+RESULTS:
#+begin_example
Input file                     = /home/igankevich/workspace/phd-diss/config/propagating_wave.arma
ACF grid size                  = (20,10,10)
ACF grid patch size            = (0.526316,0.555556,0.555556)
Output grid size               = (200,40,40)
Output grid patch size         = (1,1,1)
AR order                       = (10,10,10)
Do least squares               = 0
ACF function                   = propagating_wave
Model                          = MA
MA algorithm                   = fixed_point_iteration
Verification scheme            = manual
ACF variance = 5
fixed_point_iteration:Iteration=0, var_wn=2.70831
fixed_point_iteration:Iteration=1, var_wn=1.93791
fixed_point_iteration:Iteration=2, var_wn=1.54801
fixed_point_iteration:Iteration=3, var_wn=1.31202
fixed_point_iteration:Iteration=4, var_wn=1.15328
fixed_point_iteration:Iteration=5, var_wn=1.0386
fixed_point_iteration:Iteration=6, var_wn=0.951442
fixed_point_iteration:Iteration=7, var_wn=0.882674
fixed_point_iteration:Iteration=8, var_wn=0.82688
fixed_point_iteration:Iteration=9, var_wn=0.780623
fixed_point_iteration:Iteration=10, var_wn=0.74161
fixed_point_iteration:Iteration=11, var_wn=0.708244
fixed_point_iteration:Iteration=12, var_wn=0.679374
fixed_point_iteration:Iteration=13, var_wn=0.654145
fixed_point_iteration:Iteration=14, var_wn=0.63191
fixed_point_iteration:Iteration=15, var_wn=0.612168
fixed_point_iteration:Iteration=16, var_wn=0.594523
fixed_point_iteration:Iteration=17, var_wn=0.578663
fixed_point_iteration:Iteration=18, var_wn=0.564333
fixed_point_iteration:Iteration=19, var_wn=0.551325
fixed_point_iteration:Iteration=20, var_wn=0.539469
fixed_point_iteration:Iteration=21, var_wn=0.528623
fixed_point_iteration:Iteration=22, var_wn=0.518666
fixed_point_iteration:Iteration=23, var_wn=0.509497
fixed_point_iteration:Iteration=24, var_wn=0.50103
fixed_point_iteration:Iteration=25, var_wn=0.493191
fixed_point_iteration:Iteration=26, var_wn=0.485916
fixed_point_iteration:Iteration=27, var_wn=0.479148
fixed_point_iteration:Iteration=28, var_wn=0.472841
fixed_point_iteration:Iteration=29, var_wn=0.466951
fixed_point_iteration:Iteration=30, var_wn=0.461442
fixed_point_iteration:Iteration=31, var_wn=0.456279
fixed_point_iteration:Iteration=32, var_wn=0.451435
fixed_point_iteration:Iteration=33, var_wn=0.446882
fixed_point_iteration:Iteration=34, var_wn=0.442597
fixed_point_iteration:Iteration=35, var_wn=0.43856
fixed_point_iteration:Iteration=36, var_wn=0.434752
fixed_point_iteration:Iteration=37, var_wn=0.431155
fixed_point_iteration:Iteration=38, var_wn=0.427755
fixed_point_iteration:Iteration=39, var_wn=0.424538
fixed_point_iteration:Iteration=40, var_wn=0.42149
fixed_point_iteration:Iteration=41, var_wn=0.418601
fixed_point_iteration:Iteration=42, var_wn=0.415859
fixed_point_iteration:Iteration=43, var_wn=0.413256
fixed_point_iteration:Iteration=44, var_wn=0.410782
fixed_point_iteration:Iteration=45, var_wn=0.40843
fixed_point_iteration:Iteration=46, var_wn=0.406191
fixed_point_iteration:Iteration=47, var_wn=0.404059
fixed_point_iteration:Iteration=48, var_wn=0.402029
fixed_point_iteration:Iteration=49, var_wn=0.400092
fixed_point_iteration:Iteration=50, var_wn=0.398246
fixed_point_iteration:Iteration=51, var_wn=0.396483
fixed_point_iteration:Iteration=52, var_wn=0.3948
fixed_point_iteration:Iteration=53, var_wn=0.393193
fixed_point_iteration:Iteration=54, var_wn=0.391656
fixed_point_iteration:Iteration=55, var_wn=0.390188
fixed_point_iteration:Iteration=56, var_wn=0.388782
fixed_point_iteration:Iteration=57, var_wn=0.387438
fixed_point_iteration:Iteration=58, var_wn=0.386151
fixed_point_iteration:Iteration=59, var_wn=0.384918
fixed_point_iteration:Iteration=60, var_wn=0.383738
fixed_point_iteration:Iteration=61, var_wn=0.382606
fixed_point_iteration:Iteration=62, var_wn=0.381522
fixed_point_iteration:Iteration=63, var_wn=0.380482
fixed_point_iteration:Iteration=64, var_wn=0.379485
fixed_point_iteration:Iteration=65, var_wn=0.378528
fixed_point_iteration:Iteration=66, var_wn=0.37761
fixed_point_iteration:Iteration=67, var_wn=0.376729
fixed_point_iteration:Iteration=68, var_wn=0.375882
fixed_point_iteration:Iteration=69, var_wn=0.37507
fixed_point_iteration:Iteration=70, var_wn=0.374289
fixed_point_iteration:Iteration=71, var_wn=0.373539
fixed_point_iteration:Iteration=72, var_wn=0.372818
fixed_point_iteration:Iteration=73, var_wn=0.372126
fixed_point_iteration:Iteration=74, var_wn=0.37146
fixed_point_iteration:Iteration=75, var_wn=0.37082
fixed_point_iteration:Iteration=76, var_wn=0.370204
fixed_point_iteration:Iteration=77, var_wn=0.369612
fixed_point_iteration:Iteration=78, var_wn=0.369042
fixed_point_iteration:Iteration=79, var_wn=0.368494
fixed_point_iteration:Iteration=80, var_wn=0.367966
fixed_point_iteration:Iteration=81, var_wn=0.367458
fixed_point_iteration:Iteration=82, var_wn=0.366969
fixed_point_iteration:Iteration=83, var_wn=0.366499
fixed_point_iteration:Iteration=84, var_wn=0.366046
fixed_point_iteration:Iteration=85, var_wn=0.36561
fixed_point_iteration:Iteration=86, var_wn=0.365189
fixed_point_iteration:Iteration=87, var_wn=0.364785
fixed_point_iteration:Iteration=88, var_wn=0.364395
fixed_point_iteration:Iteration=89, var_wn=0.364019
fixed_point_iteration:Iteration=90, var_wn=0.363657
fixed_point_iteration:Iteration=91, var_wn=0.363309
fixed_point_iteration:Iteration=92, var_wn=0.362973
fixed_point_iteration:Iteration=93, var_wn=0.362649
fixed_point_iteration:Iteration=94, var_wn=0.362337
fixed_point_iteration:Iteration=95, var_wn=0.362036
fixed_point_iteration:Iteration=96, var_wn=0.361746
fixed_point_iteration:Iteration=97, var_wn=0.361466
fixed_point_iteration:Iteration=98, var_wn=0.361197
fixed_point_iteration:Iteration=99, var_wn=0.360937
fixed_point_iteration:Iteration=100, var_wn=0.360686
fixed_point_iteration:Iteration=101, var_wn=0.360444
fixed_point_iteration:Iteration=102, var_wn=0.360211
fixed_point_iteration:Iteration=103, var_wn=0.359986
fixed_point_iteration:Iteration=104, var_wn=0.359769
fixed_point_iteration:Iteration=105, var_wn=0.35956
fixed_point_iteration:Iteration=106, var_wn=0.359358
fixed_point_iteration:Iteration=107, var_wn=0.359163
fixed_point_iteration:Iteration=108, var_wn=0.358975
fixed_point_iteration:Iteration=109, var_wn=0.358794
fixed_point_iteration:Iteration=110, var_wn=0.358619
fixed_point_iteration:Iteration=111, var_wn=0.35845
fixed_point_iteration:Iteration=112, var_wn=0.358288
fixed_point_iteration:Iteration=113, var_wn=0.35813
fixed_point_iteration:Iteration=114, var_wn=0.357979
fixed_point_iteration:Iteration=115, var_wn=0.357832
fixed_point_iteration:Iteration=116, var_wn=0.357691
fixed_point_iteration:Iteration=117, var_wn=0.357555
fixed_point_iteration:Iteration=118, var_wn=0.357423
fixed_point_iteration:Iteration=119, var_wn=0.357296
fixed_point_iteration:Iteration=120, var_wn=0.357173
fixed_point_iteration:Iteration=121, var_wn=0.357055
fixed_point_iteration:Iteration=122, var_wn=0.356941
fixed_point_iteration:Iteration=123, var_wn=0.356831
fixed_point_iteration:Iteration=124, var_wn=0.356724
fixed_point_iteration:Iteration=125, var_wn=0.356621
fixed_point_iteration:Iteration=126, var_wn=0.356522
fixed_point_iteration:Iteration=127, var_wn=0.356426
fixed_point_iteration:Iteration=128, var_wn=0.356334
fixed_point_iteration:Iteration=129, var_wn=0.356244
fixed_point_iteration:Iteration=130, var_wn=0.356158
fixed_point_iteration:Iteration=131, var_wn=0.356075
fixed_point_iteration:Iteration=132, var_wn=0.355994
fixed_point_iteration:Iteration=133, var_wn=0.355917
fixed_point_iteration:Iteration=134, var_wn=0.355842
fixed_point_iteration:Iteration=135, var_wn=0.355769
fixed_point_iteration:Iteration=136, var_wn=0.355699
fixed_point_iteration:Iteration=137, var_wn=0.355632
fixed_point_iteration:Iteration=138, var_wn=0.355567
fixed_point_iteration:Iteration=139, var_wn=0.355504
fixed_point_iteration:Iteration=140, var_wn=0.355443
fixed_point_iteration:Iteration=141, var_wn=0.355384
fixed_point_iteration:Iteration=142, var_wn=0.355327
fixed_point_iteration:Iteration=143, var_wn=0.355273
fixed_point_iteration:Iteration=144, var_wn=0.35522
fixed_point_iteration:Iteration=145, var_wn=0.355169
fixed_point_iteration:Iteration=146, var_wn=0.355119
fixed_point_iteration:Iteration=147, var_wn=0.355072
fixed_point_iteration:Iteration=148, var_wn=0.355026
fixed_point_iteration:Iteration=149, var_wn=0.354981
fixed_point_iteration:Iteration=150, var_wn=0.354938
fixed_point_iteration:Iteration=151, var_wn=0.354897
fixed_point_iteration:Iteration=152, var_wn=0.354856
fixed_point_iteration:Iteration=153, var_wn=0.354818
fixed_point_iteration:Iteration=154, var_wn=0.35478
fixed_point_iteration:Iteration=155, var_wn=0.354744
fixed_point_iteration:Iteration=156, var_wn=0.354709
fixed_point_iteration:Iteration=157, var_wn=0.354676
fixed_point_iteration:Iteration=158, var_wn=0.354643
fixed_point_iteration:Iteration=159, var_wn=0.354612
fixed_point_iteration:Iteration=160, var_wn=0.354581
fixed_point_iteration:Iteration=161, var_wn=0.354552
fixed_point_iteration:Iteration=162, var_wn=0.354524
fixed_point_iteration:Iteration=163, var_wn=0.354496
fixed_point_iteration:Iteration=164, var_wn=0.35447
fixed_point_iteration:Iteration=165, var_wn=0.354444
fixed_point_iteration:Iteration=166, var_wn=0.35442
fixed_point_iteration:Iteration=167, var_wn=0.354396
fixed_point_iteration:Iteration=168, var_wn=0.354373
fixed_point_iteration:Iteration=169, var_wn=0.35435
fixed_point_iteration:Iteration=170, var_wn=0.354329
fixed_point_iteration:Iteration=171, var_wn=0.354308
fixed_point_iteration:Iteration=172, var_wn=0.354288
fixed_point_iteration:Iteration=173, var_wn=0.354269
fixed_point_iteration:Iteration=174, var_wn=0.35425
fixed_point_iteration:Iteration=175, var_wn=0.354232
fixed_point_iteration:Iteration=176, var_wn=0.354214
fixed_point_iteration:Iteration=177, var_wn=0.354198
fixed_point_iteration:Iteration=178, var_wn=0.354181
fixed_point_iteration:Iteration=179, var_wn=0.354165
fixed_point_iteration:Iteration=180, var_wn=0.35415
fixed_point_iteration:Iteration=181, var_wn=0.354136
fixed_point_iteration:Iteration=182, var_wn=0.354121
fixed_point_iteration:Iteration=183, var_wn=0.354108
fixed_point_iteration:Iteration=184, var_wn=0.354094
fixed_point_iteration:Iteration=185, var_wn=0.354082
fixed_point_iteration:Iteration=186, var_wn=0.354069
fixed_point_iteration:Iteration=187, var_wn=0.354057
fixed_point_iteration:Iteration=188, var_wn=0.354046
fixed_point_iteration:Iteration=189, var_wn=0.354034
fixed_point_iteration:Iteration=190, var_wn=0.354024
fixed_point_iteration:Iteration=191, var_wn=0.354013
fixed_point_iteration:Iteration=192, var_wn=0.354003
fixed_point_iteration:Iteration=193, var_wn=0.353994
WN variance = 0.353994
Input file                     = /home/igankevich/workspace/phd-diss/config/standing_wave.arma
ACF grid size                  = (10,10,10)
ACF grid patch size            = (0.277778,0.555556,0.555556)
Output grid size               = (200,40,40)
Output grid patch size         = (1,1,1)
AR order                       = (7,7,7)
Do least squares               = 0
ACF function                   = standing_wave
Model                          = AR
MA algorithm                   = fixed_point_iteration
Verification scheme            = manual
ACF variance = 5
WN variance = 0.00261323
Zeta size = (193,33,33)
NaN: 29, -nan, 1.798e+36, -1.04284e+38, inf, -1.798e+36, -1.798e+36
#+end_example

* Introduction
**** Topic relevance.
Software programmes, which simulates vessel behaviour in sea waves, are widely
used to model ship motion, estimate impact of external forces on floating
platform or other marine object, and estimate capsize probability under given
weather conditions; however, to model ocean waves most of the simulation codes
use linear wave theory
cite:shin2003nonlinear,van2007forensic,kat2001prediction,van2002development, in
the framework of which it is difficult to reproduce certain peculiarities of
wind wave climate. Among them are transition between normal and storm weather,
and sea composed of multiple wave systems --- both wind waves and swell ---
heading from multiple directions. Another shortcoming of linear wave theory is
an assumption, that wave amplitude is small compared to wave length. This makes
calculations imprecise when modelling ship motion in irregular waves, for which
the assumption does not hold. So, studying new and more advanced models and
methods for ocean simulation software may increase number of its application
scenarios and foster a study of ship motion in extreme conditions in particular.

**** State-of-the-art.
Autoregressive moving average (ARMA) model emerged in response to difficulties
encountered by practitioners who used wave simulation models developed in the
framework of linear wave theory. The problems they have encountered with
Longuet---Higgins model (a model which is entirely based on linear wave theory)
can be summarised as the following.
1. /Periodicity/. Linear wave theory approximates waves by a sum of harmonics,
   so period of the whole wavy surface realisation depends on the number of
   harmonics in the model. The more realisation size is, the more coefficients
   are required to eliminate periodicity, therefore, generation time grows
   non-linearly with realisation size. This in turn results in overall low
   efficiency of any model based on this theory, no matter how optimised the
   software implementation is.
2. /Linearity/. Linear wave theory gives mathematical definition for ocean waves
   which have small amplitudes compared to their lengths. Waves of this type
   occur mostly in the ocean, so near-shore waves as well as storm waves, for
   which this assumption does not hold, are not perfectly captured by linear
   theory.
3. /Probabilistic convergence/. Phase of a wave, which is often generated by
   pseudo random number generator (PRNG), has uniform distribution, and this
   makes wavy surface characteristics (average wave height, wave period, wave
   length etc.) sometimes converge slowly to the desired values. Convergence
   rate depends on the values generated by PRNG, so high convergence rate is not
   guaranteed.

These difficulties became a starting point in search for a new model which is
not based on linear wave theory. Autoregressive moving average (ARMA) process
studies were found to have all the required mathematical apparatus.
1. ARMA process takes auto-covariate function (ACF) as an input parameter, and
   this function can be directly obtained from wave energy or
   frequency-directional spectrum (which is the input for Longuet---Higgins
   model). So, inputs for one model can easily be converted to each other.
2. There is no small-amplitude waves assumption. Wave may have any amplitude,
   and can be generated as steep as it is possible with real ocean wave ACF.
3. Period of the realisation equals the period of PRNG, so generation time grows
   linearly with the realisation size.
4. White noise --- the only probabilistic term in ARMA process --- has
   Gaussian distribution; so, convergence rate is not probabilistic.

**** Goals and objectives.
ARMA process became the basis for ARMA ocean simulation model, however, there
was still much work to be done to make it useful in practice.
1. One have to investigate how different ACF shapes affect the choice of ARMA
   parameters (the number of moving average and autoregressive processes
   coefficients).
2. Then, investigate a possibility to generate waves of arbitrary profile, not
   only cosines (which means taking into account asymmetric distribution of wavy
   surface elevation).
3. Then, derive formulae to determine pressure field under wavy surface.
   Usually, such formulae are derived for a particular model by substituting
   wave profile into the eq. eqref:eq:problem, however, ARMA process does not
   provide explicit wave profile formula, so this problem had to be solved for
   general wavy surface (which is not defined by an analytic formula),
   without linearisation of boundaries and assumption of small-amplitude waves.
4. Finally, verify wavy surface integral characteristics to match the ones of
   real ocean waves.
5. In the final stage, develop software programme that implements ARMA model and
   pressure calculation method, and allows running simulations on both shared
   memory (SMP) and distributed memory (MPP) computer systems.

**** Scientific novelty.
ARMA model, as opposed to other ocean simulation models, does not use linear
wave theory. This makes it capable of
- generating waves with arbitrary amplitudes by adjusting wave steepness via
  ACF;
- generating waves with arbitrary profiles by adjusting asymmetry of wave
  elevation distribution via non-linear inertia-less transform (NIT).
This makes it possible to use ARMA process to model transition between normal
and storm weather taking into account climate spectra and assimilation data of a
particular ocean region, which is not possible with models based on linear wave
theory.

**** Theoretical and practical significance.
Implementing ARMA model, that does not use assumptions of linear wave theory,
will increase quality of ship motion and marine object behaviour simulation
software.

1. Since pressure field formula is derived for discrete wavy surface and without
   assumptions about wave amplitudes, it is applicable to any wavy surface of
   incompressible inviscid fluid (in particular, it is applicable to wavy
   surface generated by LH model). This allows using pressure field formula
   without being tied to ARMA model.
2. From computational point of view this formula is more efficient than the
   corresponding formula for LH model, because integrals in it are reduced to
   Fourier transforms, for which there is fast Fourier transform (FFT) family of
   algorithms, optimised for different processor architectures.
3. Since the formula is explicit, there is no need in data exchange between
   parallel processes, which allows to achieve high scalability on computer
   clusters.
4. Finally, ARMA model is itself more efficient than LH model due to vicinity of
   trigonometric functions in its formula: In fact, wavy surface is computed as
   a sum of large number of polynomials, for which there is low-level assembly
   instruction (Fused Multiply-Add) giving native performance on CPUs.

**** Methodology and research methods.
Software implementation of ARMA model and pressure field formula was created
incrementally: a prototype written in high-level engineering language
cite:mathematica10,octave2015 was rewritten in lower level language (C++).
Implementation of the same algorithm and formulae in languages of varying
levels (which involves usage of different abstractions and language primitives)
allows correcting errors, which would left unnoticed otherwise. Wavy surface,
generated by ARMA model, as well as all input parameters (ACF, distribution of
wave elevation etc.) were inspected via graphical means built into the
programming language allowing visual control of programme correctness.

**** Theses for the defence.
- Wind wave model which allows generating wavy surface realisations with large
  period and consisting of wave of arbitrary amplitudes;
- Pressure field formulae derived for this model without assumptions of linear
  wave theory;
- Software implementation of the model and the formula for shared memory (SMP)
  and distributed memory (MPP) systems.

**** Results verification and approbation.
ARMA model is verified by comparing generated wavy surface integral
characteristics (distribution of wave elevation, wave heights and lengths etc.)
to the ones of real ocean waves. Pressure field formula is derived in
Mathematica language, where resulting formulae are verified by built-in
graphical means.

ARMA model and pressure field formula were incorporated into Large Amplitude
Motion Programme (LAMP) --- an ship motion simulation software programme ---
where they were compared to previously used LH model. Preliminary numerical
experiments showed higher computational efficiency of ARMA model.

* Problem statement
The aim of the study reported here is to investigate possibilities of applying
ARMA process mathematical apparatus to ocean wave modelling and to derive formula
for pressure field under generated wavy surface without assumptions of linear
wave theory.
- In case of small-amplitude waves resulting formula must correspond to the
  one from linear wave theory; in all other cases the formula must not diverge.
- Integral characteristics of generated wavy surface must match the ones of real
  ocean waves.
- Software implementation of ARMA model and pressure field formula must work on
  shared memory (SMP) and distributed memory (MPP) systems.

**** Pressure field formula.
The problem of finding pressure field under wavy sea surface represents inverse
problem of hydrodynamics for incompressible inviscid fluid. System of equations
for it in general case is written as cite:kochin1966theoretical
\begin{align}
    & \nabla^2\phi = 0,\nonumber\\
    & \phi_t+\frac{1}{2} |\vec{\upsilon}|^2 + g\zeta=-\frac{p}{\rho}, & \text{на }z=\zeta(x,y,t),\label{eq:problem}\\
    & D\zeta = \nabla \phi \cdot \vec{n}, & \text{на }z=\zeta(x,y,t),\nonumber
\end{align}
where $\phi$ --- velocity potential, $\zeta$ --- elevation ($z$ coordinate) of
wavy surface, $p$ --- wave pressure, $\rho$ --- fluid density, $\vec{\upsilon} =
(\phi_x, \phi_y, \phi_z)$ --- velocity vector, $g$ --- acceleration of gravity,
and $D$ --- substantial (Lagrange) derivative. The first equation is called
continuity (Laplace) equation, the second one is the conservation of momentum
law (the so called dynamic boundary condition); the third one is
kinematic boundary condition for free wavy surface, which states that rate of
change of wavy surface elevation ($D\zeta$) equals to the change of velocity
potential derivative along the wavy surface normal ($\nabla \phi \cdot
\vec{n}$).

Inverse problem of hydrodynamics consists in solving this system of equations
for $\phi$. In this formulation dynamic boundary condition becomes explicit
formula to determine pressure field using velocity potential derivatives
obtained from the remaining equations. So, from mathematical point of view
inverse problem of hydrodynamics reduces to Laplace equation with mixed boundary
condition --- Robin problem.

* Related work
** Ocean wave models analysis
Pressure computation is only possible when the shape of wavy surface is known.
It is defined either at discrete grid points, or continuously via some analytic
formula. As will be shown in section [[#linearisation]], such formula may simplify
pressure computation by effectively reducing the task to pressure field
generation, instead of wavy surface generation.

*** Longuet---Higgins model
The simplest model, formula of which is derived in the framework of linear wave
theory, is Longuet---Higgins (LH) model cite:longuet1957statistical. In-depth
comparative analysis of this model and ARMA model is done in
cite:degtyarev2011modelling,boukhanovsky1997thesis.

LH model represents ocean wavy surface as a superposition of
sine waves with random amplitudes $c_n$ and phases $\epsilon_n$, continuously
distributed on interval $[0,2\pi]$. Wavy surface elevation ($z$ coordinate) is
defined by
#+name: eq:longuet-higgins
\begin{equation}
    \zeta(x,y,t) = \sum\limits_n c_n \cos(u_n x + v_n y - \omega_n t + \epsilon_n).
\end{equation}
Here wave numbers $(u_n,v_n)$ are continuously distributed on plane $(u,v)$,
i.e. area $du \times dv$ contains infinite quantity of wave numbers. Frequency
is related to wave numbers via dispersion relation $\omega_n=\omega(u_n,v_n)$.
Function $\zeta(x,y,t)$ is a three-dimensional ergodic stationary homogeneous
Gaussian process defined by
\begin{equation*}
    2E_\zeta(u,v)\, du\,  dv = \sum\limits_n c_n^2,
\end{equation*}
where $E_\zeta(u,v)$ --- two-dimensional wave energy spectral density.
Coefficients $c_n$ are derived from wave energy spectrum $S(\omega)$ via
\begin{equation*}
    c_n = \sqrt{ \textstyle\int\limits_{\omega_n}^{\omega_{n+1}} S(\omega) d\omega}.
\end{equation*}

*** Disadvantages of Longuet-Higgins model
Although LH model is simple and easy to understand, there are shortcomings that
appear in practice.

1. The model simulates only stationary Gaussian process. This is consequence of
   central limit theorem (CLT): sum of large number of sines with random
   amplitudes and phases has normal distribution, no matter what spectrum is
   used as the model input. Using lower number of coefficients may solve the
   problem, but also make realisation period smaller. So, using LH model to
   simulate waves with non-Gaussian distribution of elevation --- a distribution
   which real ocean waves have cite:huang1980experimental,рожков1996теория ---
   is impractical.
2. From computational point of view, the deficiency of the model is non-linear
   increase of wavy surface generation time with the increase of realisation
   size. The larger the size of the realisation, the higher number of
   coefficients (discrete points of frequency-directional spectrum) is needed to
   eliminate periodicity. This makes LH model inefficient for long-time
   simulations.
3. Finally, there are peculiarities which make LH model unsuitable base for
   building more advanced simulation models.
   - In software implementation convergence rate of ([[eq:longuet-higgins]]) may be
     low due to randomness of phases $\epsilon_n$.
   - It is difficult to generalise LH model for non-Gaussian processes as it
     involves incorporating non-linear terms in ([[eq:longuet-higgins]]) for which
     there is no known formula to determine coefficients
     cite:рожков1990вероятностные.

To summarise, LH model is linear, computationally inefficient for long-time
simulations, and difficult to use as a base for more advanced models.

*** ARMA model
In cite:spanos1982arma ARMA model is used to generate time series spectrum of
which is compatible with Pierson---Moskowitz (PM) approximation of ocean wave
spectrum. The authors carry out experiments for one-dimensional AR, MA and ARMA
models. They mention excellent agreement between target and initial spectra and
higher performance of ARMA model compared to models based on summing large
number of harmonic components with random phases. The also mention that in order
to reach agreement between target and initial spectrum MA model require lesser
number of coefficients than AR model. In cite:spanos1996efficient the authors
generalise ARMA model coefficients determination formulae for multi-variate
(vector) case.

One thing that distinguishes present work with respect to afore-mentioned ones
is the study of three-dimensional (2D in space and 1D in time) ARMA model, which
is mostly a different problem.
1. Yule---Walker system of equations, which are used to determine AR
   coefficients, has complex block-block structure.
2. Optimal model order (in a sense that target spectrum agrees with initial) is
   determined manually.
3. Instead of PM spectrum, analytic formulae for standing and propagating
   waves ACF are used as the model input.
4. Three-dimensional wavy surface should be compatible with real ocean surface
   not only in terms of spectral characteristics, but also in the shape of wave
   profiles. So, model verification includes distributions of various parameters
   of generated waves (lengths, heights, periods etc.).
Multi-dimensionality of investigated model not only complexifies the task, but
also allows carrying out visual validation of generated wavy surface. It is the
opportunity to visualise output of the programme that allowed to ensure that
generated surface is compatible with real ocean surface, and is not abstract
multi-dimensional stochastic process that is real only statistically.

In cite:fusco2010short AR model is used to predict swell waves to control
wave-energy converters (WEC) in real-time. In order to make WEC more efficient
its internal oscillator frequency should match the one of ocean waves. The
authors treat wave elevation as time series and compare performance of AR model,
neural networks and cyclical models in forecasting time series future values. AR
model gives the most accurate prediction of low-frequency swell waves for up to
two typical wave periods. It is an example of successful application of AR
process to ocean wave modelling.

** Pressure field determination formulae
*** Small amplitude waves theory
In cite:stab2012,детярев1998моделирование,degtyarev1997analysis the authors
propose a solution for inverse problem of hydrodynamics of potential flow in the
framework of small-amplitude wave theory (under assumption that wave length is
much larger than height: $\lambda \gg h$). In that case inverse problem is
linear and reduces to Laplace equation with mixed boundary conditions, and
equation of motion is solely used to determine pressures for calculated velocity
potential derivatives. The assumption of small amplitudes means the slow decay
of wind wave coherence function, i.e. small change of local wave number in time
and space compared to the wavy surface elevation ($z$ coordinate). This
assumption allows calculating elevation $z$ derivative as $\zeta_z=k\zeta$,
where $k$ is wave number. In two-dimensional case the solution is written
explicitly as
\begin{align}
    \left.\frac{\partial\phi}{\partial x}\right|_{x,t}= &
        -\frac{1}{\sqrt{1+\alpha^{2}}}e^{-I(x)}
            \int\limits_{0}^x\frac{\partial\dot{\zeta}/\partial
                z+\alpha\dot{\alpha}}{\sqrt{1+\alpha^{2}}}e^{I(x)}dx,\label{eq:old-sol-2d}\\
    I(x)= & \int\limits_{0}^x\frac{\partial\alpha/\partial z}{1+\alpha^{2}}dx,\nonumber
\end{align}

where $\alpha$ is wave slope. In three-dimensional case solution is written in
the form of elliptic partial differential equation (PDE):
\begin{align*}
    & \frac{\partial^2 \phi}{\partial x^2} \left( 1 + \alpha_x^2 \right) +
    \frac{\partial^2 \phi}{\partial y^2} \left( 1 + \alpha_y^2 \right) +
    2\alpha_x\alpha_y \frac{\partial^2 \phi}{\partial x \partial y} + \\
    & \left(
        \frac{\partial \alpha_x}{\partial z} +
        \alpha_x \frac{\partial \alpha_x}{\partial x} +
        \alpha_y \frac{\partial \alpha_x}{\partial y}
    \right) \frac{\partial \phi}{\partial x} + \\
    & \left(
        \frac{\partial \alpha_y}{\partial z} +
        \alpha_x \frac{\partial \alpha_y}{\partial x} +
        \alpha_y \frac{\partial \alpha_y}{\partial y}
    \right) \frac{\partial \phi}{\partial y} + \\
    & \frac{\partial \dot{\zeta}}{\partial z} +
    \alpha_x \dot{\alpha_x} + \alpha_y \dot{\alpha_y} = 0.
\end{align*}
The authors suggest transforming this equation to finite differences and solve
it numerically.

As will be shown in [[#sec:compare-formulae]] that eqref:eq:old-sol-2d diverges when
attempted to calculate velocity field for large-amplitude waves, and this is the
reason that it can not be used together with ARMA model, that generates
arbitrary-amplitude waves.

*** Linearisation of boundary condition
:PROPERTIES:
:CUSTOM_ID: linearisation
:END:

LH model allows deriving an explicit formula for velocity field by linearising
kinematic boundary condition. Velocity potential formula is written as
\begin{equation*}
\phi(x,y,z,t) = \sum_n \frac{c_n g}{\omega_n}
     e^{\sqrt{u_n^2+v_n^2} z}
     \sin(u_n x + v_n y - \omega_n t + \epsilon_n).
\end{equation*}
This formula is differentiated to obtain velocity potential derivatives, which
are plugged to dynamic boundary condition to obtain pressures.

* ARMA model for ocean wave simulation
** Governing equations for 3-dimensional ARMA process
*** Three possible processes
ARMA ocean simulation model defines ocean wavy surface as three-dimensional (two
dimensions in space and one in time) autoregressive moving average process:
every surface point is represented as a weighted sum of previous in time and
space points plus weighted sum of previous in time and space normally
distributed random impulses. The governing equation for 3-D ARMA process is
\begin{equation}
    \zeta_{\vec i}
    =
    \sum\limits_{\vec j = \vec 0}^{\vec N}
    \Phi_{\vec j} \zeta_{\vec i - \vec j}
    +
    \sum\limits_{\vec j = \vec 0}^{\vec M}
    \Theta_{\vec j} \epsilon_{\vec i - \vec j}
    ,
    \label{eq:arma-process}
\end{equation}
where $\zeta$ --- wave elevation, $\Phi$ --- AR process coefficients, $\Theta$
--- MA process coefficients, $\epsilon$ --- white noise with Gaussian
distribution, $\vec N$ --- AR process order, $\vec M$ --- MA process order, and
$\Phi_{\vec{0}}\equiv{0}$, $\Theta_{\vec{0}}\equiv{0}$. Here arrows denote
multi-component indices with a component for each dimension. In general, any
scalar quantity can be a component (temperature, salinity, concentration of some
substance in water etc.). Equation parameters are AR and MA process coefficients
and order.

**** Autoregressive (AR) process.
AR process is ARMA process with only one random impulse instead of theirs
weighted sum:
\begin{equation}
    \zeta_{\vec i}
    =
    \sum\limits_{\vec j = \vec 0}^{\vec N}
    \Phi_{\vec j} \zeta_{\vec i - \vec j}
    +
    \epsilon_{i,j,k}
    .
    \label{eq:ar-process}
\end{equation}
The coefficients $\Phi$ are calculated from ACF via three-dimensional
Yule---Walker equations, which are obtained after multiplying both parts of the
previous equation by $\zeta_{\vec{i}-\vec{k}}$ and computing the expected value.
Generic form of YW equations is
\begin{equation}
    \label{eq:yule-walker}
    \gamma_{\vec k}
    =
    \sum\limits_{\vec j = \vec 0}^{\vec N}
    \Phi_{\vec j}
    \text{ }\gamma_{\vec{k}-\vec{j}}
    +
    \Var{\epsilon} \delta_{\vec{k}},
    \qquad
    \delta_{\vec{k}} =
    \begin{cases}
        1, \quad \text{if } \vec{k}=0 \\
        0, \quad \text{if } \vec{k}\neq0,
    \end{cases}
\end{equation}
where $\gamma$ --- ACF of process $\zeta$, $\Var{\epsilon}$ --- white noise
variance. Matrix form of three-dimensional YW equations, which is used in the
present work, is
\begin{equation*}
    \Gamma
    \left[
        \begin{array}{l}
            \Phi_{\vec 0}\\
            \Phi_{0,0,1}\\
            \vdotswithin{\Phi_{\vec 0}}\\
            \Phi_{\vec N}
        \end{array}
    \right]
    =
    \left[
        \begin{array}{l}
            \gamma_{0,0,0}-\Var{\epsilon}\\
            \gamma_{0,0,1}\\
            \vdotswithin{\gamma_{\vec 0}}\\
            \gamma_{\vec N}
        \end{array}
    \right],
    \qquad
    \Gamma=
    \left[
        \begin{array}{llll}
            \Gamma_0 & \Gamma_1 & \cdots & \Gamma_{N_1} \\
            \Gamma_1 & \Gamma_0 & \ddots & \vdotswithin{\Gamma_0} \\
            \vdotswithin{\Gamma_0} & \ddots & \ddots & \Gamma_1 \\
            \Gamma_{N_1} & \cdots & \Gamma_1 & \Gamma_0
        \end{array}
    \right],
\end{equation*}
where $\vec N = \left( p_1, p_2, p_3 \right)$ and
\begin{equation*}
    \Gamma_i =
    \left[
    \begin{array}{llll}
        \Gamma^0_i & \Gamma^1_i & \cdots & \Gamma^{N_2}_i \\
        \Gamma^1_i & \Gamma^0_i & \ddots & \vdotswithin{\Gamma^0_i} \\
        \vdotswithin{\Gamma^0_i} & \ddots & \ddots & \Gamma^1_i \\
        \Gamma^{N_2}_i & \cdots & \Gamma^1_i & \Gamma^0_i
    \end{array}
    \right]
    \qquad
    \Gamma_i^j=
    \left[
    \begin{array}{llll}
        \gamma_{i,j,0} & \gamma_{i,j,1} & \cdots & \gamma_{i,j,N_3} \\
        \gamma_{i,j,1} & \gamma_{i,j,0} & \ddots &x \vdotswithin{\gamma_{i,j,0}} \\
        \vdotswithin{\gamma_{i,j,0}} & \ddots & \ddots & \gamma_{i,j,1} \\
        \gamma_{i,j,N_3} & \cdots & \gamma_{i,j,1} & \gamma_{i,j,0}
    \end{array}
    \right],
\end{equation*}
Since $\Phi_{\vec 0}\equiv0$, the first row and column of $\Gamma$ can be
eliminated. Matrix $\Gamma$ is block-toeplitz, positive definite and symmetric,
hence the system is efficiently solved by Cholesky decomposition, which is
particularly suitable for these types of matrices.

After solving this system of equations white noise variance is estimated from
eqref:eq:yule-walker by plugging $\vec k = \vec 0$:
\begin{equation*}
    \Var{\epsilon} =
    \Var{\zeta}
    -
    \sum\limits_{\vec j = \vec 0}^{\vec N}
    \Phi_{\vec j}
    \text{ }\gamma_{\vec{j}}.
\end{equation*}

**** Moving average (MA) process.
MA process is ARMA process with $\Phi\equiv0$:
\begin{equation}
    \zeta_{\vec i}
    =
    \sum\limits_{\vec j = \vec 0}^{\vec M}
    \Theta_{\vec j} \epsilon_{\vec i - \vec j}
    .
    \label{eq:ma-process}
\end{equation}
MA coefficients $\Theta$ are defined implicitly via the following non-linear
system of equations:
\begin{equation*}
  \gamma_{\vec i} =
	\left[
		\displaystyle
    \sum\limits_{\vec j = \vec i}^{\vec M}
    \Theta_{\vec j}\Theta_{\vec j - \vec i}
	\right]
  \Var{\epsilon}.
\end{equation*}
The system is solved numerically by fixed-point iteration method via the
following formulae
\begin{equation*}
  \Theta_{\vec i} =
    -\frac{\gamma_{\vec 0}}{\Var{\epsilon}}
		+
    \sum\limits_{\vec j = \vec i}^{\vec M}
    \Theta_{\vec j} \Theta_{\vec j - \vec i}.
\end{equation*}
Here coefficients $\Theta$ are calculated from back to front: from
$\vec{i}=\vec{M}$ to $\vec{i}=\vec{0}$. White noise variance is estimated by
\begin{equation*}
    \Var{\epsilon} = \frac{\gamma_{\vec 0}}{
		1
		+
    \sum\limits_{\vec j = \vec 0}^{\vec M}
    \Theta_{\vec j}^2
    }.
\end{equation*}
Authors of cite:box1976time suggest using Newton---Raphson method to solve this
equation with higher precision, however, this method does not work in three
dimensions. Using slower method does not have dramatic effect on the overall
programme performance, because the number of coefficients is small and most of
the time is spent generating wavy surface.

**** TODO Stationarity and invertibility of AR and MA processes
**** Mixed autoregressive moving average (ARMA) process.
:PROPERTIES:
:CUSTOM_ID: sec:how-to-mix-ARMA
:END:
Generally speaking, ARMA process is obtained by plugging MA generated wavy
surface as random impulse to AR process, however, in order to get the process
with desired ACF one should re-compute AR coefficients before plugging. There
are several approaches to "mix" AR and MA processes.
- The approach proposed in cite:box1976time which involves dividing ACF into MA
  and AR part along each dimension is not applicable here, because in three
  dimensions such division is not possible: there always be parts of the ACF
  that are not taken into account by AR and MA process.
- The alternative approach is to use the same (undivided) ACF for both AR and MA
  processes but use different process order, however, then realisation
  characteristics (mean, variance etc.) become skewed: these are characteristics
  of the two overlapped processes.
For the first approach there is a formula to re-compute ACF for AR process, but
there is no such formula for the second approach. So, the best solution for now
is to simply use AR and MA process exclusively.

*** Process selection criteria for different wave profiles
One problem of ARMA model application to ocean wave generation is that for
different types of wave profiles different processes /must/ be used: standing
waves are modelled by AR process, and propagating waves by MA process. This
statement comes from practice: if one tries to use the processes the other way
round, the resulting realisation either diverges or does not correspond to real
ocean waves. (The latter happens for non-invertible MA process, as it is always
stationary.) So, the best way to apply ARMA model to ocean wave generation is to
use AR process for standing waves and MA process for progressive waves.

The other problem is inability to automatically determine optimal number of
coefficients for three-dimensional AR and MA processes. For one-dimensional
processes this can be achieved via iterative methods cite:box1976time, but they
diverge in three-dimensional case.

The final problem, which is discussed in [[#sec:how-to-mix-ARMA]], is inability to
"mix" AR and MA process in three dimensions.

In practice some statements made for AR and MA processes in cite:box1976time
should be flipped for three-dimensional case. For example, the authors say that
ACF of MA process cuts at $q$ and ACF of AR process decays to nought infinitely,
but in practice making ACF of 3-dimensional MA process not decay results in it
being non-invertible and producing realisation that does not look like real
ocean waves, whereas doing the same for ACF of AR process results in stationary
process and adequate realisation. Also, the authors say that one
should allocate the first $q$ points of ACF to MA process (as it often needed to
describe the peaks in ACF) and leave the rest points to AR process, but in
practice in case of ACF of a propagating wave AR process is stationary only for
the first time slice of the ACF, and the rest is left to MA process.

To summarise, the only established scenario of applying ARMA model to ocean wave
generation is to use AR process for standing waves and MA process for
propagating waves. With new formulae for 3 dimensions a single mixed ARMA
process might increase model precision, which is one of the objectives of the
future research.

** Modelling non-linearity of ocean waves
ARMA model allows modelling asymmetry of wave elevation distribution, i.e.
generate ocean waves, distribution of z-coordinate of which has non-nought
kurtosis and asymmetry. Such distribution is inherent to real ocean waves
cite:longuet1963nonlinear.

Wave asymmetry is modelled by non-linear inertia-less transform (NIT) of
stochastic process, however, transforming resulting wavy surface means
transforming initial ACF. In order to alleviate this, ACF must be preliminary
transformed as shown in cite:boukhanovsky1997thesis.

**** Wavy surface transformation.
Explicit formula $z=f(y)$ that transforms wavy surface to desired
one-dimensional distribution $F(z)$ is the solution of non-linear transcendental
equation $F(z)=\Phi(y)$, where $\Phi(y)$ --- one-dimensional Gaussian
distribution. Since distribution of wave elevation is often given by some
approximation based on field data, this equation is solved numerically with
respect to $z_k$ in each grid point $y_k|_{k=0}^N$ of generated wavy surface. In
this case equation is rewritten as
\begin{equation}
    \label{eq:distribution-transformation}
    F(z_k)
    =
    \frac{1}{\sqrt{2\pi}}
    \int\limits_0^{y_k} \exp\left[ -\frac{t^2}{2} \right] dt
    .
\end{equation}
Since, distribution functions are monotonic, the simplest interval halving
(bisection) numerical method is used to solve this equation.

**** Preliminary ACF transformation.
In order to transform ACF $\gamma_z$ of the process, it should be expanded in
series of Hermite polynomials (Gram---Charlier series)
\begin{equation*}
    \gamma_z \left( \vec u \right)
    =
    \sum\limits_{m=0}^{\infty}
    C_m^2 \frac{\gamma_y^m \left( \vec u \right)}{m!},
\end{equation*}
where
\begin{equation*}
    C_m = \frac{1}{\sqrt{2\pi}}
  \int\limits_{0}^\infty
    f(y) H_m(y) \exp\left[ -\frac{y^2}{2} \right],
\end{equation*}
$H_m$ --- Hermite polynomial, and $f(y)$ --- solution to equation
eqref:eq:distribution-transformation. Plugging polynomial approximation
$f(y)\approx\sum\limits_{i}d_{i}y^i$ and analytic formulae for Hermite
polynomial yields
\begin{equation*}
    \frac{1}{\sqrt{2\pi}}
    \int\limits_\infty^\infty
    y^k \exp\left[ -\frac{y^2}{2} \right]
    =
    \begin{cases}
        (k-1)!! & \text{if }k\text{ is even},\\
        0       & \text{if }k\text{ is odd},
    \end{cases}
\end{equation*}
which simplifies the former equation. Optimal number of coefficients $C_m$ is
determined by computing them sequentially and stopping when variances of both
fields become equal with desired accuracy $\epsilon$:
\begin{equation*}
    \left| \Var{z} - \sum\limits_{k=0}^m
    \frac{C_k^2}{k!} \right| \leq \epsilon.
\end{equation*}

In cite:boukhanovsky1997thesis the author suggests using polynomial
approximation $f(y)$ also for wavy surface transformation, however, in practice
ocean surface realisation often contains points, where z-coordinate is beyond
the limits of the approximation, which makes solution wrong. In these points it
is more efficient to solve equation eqref:eq:distribution-transformation by
bisection method. Using the same approximation in Gram---Charlier series does
not lead to such errors.

** Determining wave pressures for discretely given wavy surface
Analytic solutions to boundary problems in classical equations are often used to
study different properties of the solution, and for that purpose general
solution formula is too difficult to study, as it contains integrals of unknown
functions. Fourier method is one of the methods to find analytic solutions to
PDE. It is based on application of Fourier transform to each part of PDE, which
reduces the equation to algebraic, and the solution is written as inverse
Fourier transform of some function (which may contain Fourier transforms of
other functions). Since, it is not possible to write analytic forms of these
Fourier transforms in all cases, unique solutions are found and their behaviour
is studied in different domains instead. At the same time, computing discrete
Fourier transforms on the computer is possible for any discretely defined
function and efficient when using FFT algorithms. These algorithms use symmetry
of complex exponentials to decrease asymptotic complexity from
$\mathcal{O}(n^2)$ to $\mathcal{O}(n\log_{2}n)$. So, even if general solution
contains Fourier transforms of unknown functions, they still can be computed
numerically, and FFT family of algorithms makes this approach efficient.

Alternative approach to solve PDE is to reduce it to difference equations, which
are solved by constructing various numerical schemes. This approach leads to
approximate solution, and asymptotic complexity of corresponding algorithms is
comparable to that of FFT. For example, stationary elliptic PDE transforms to
implicit numerical scheme which is solved by iterative method on each step of
which a tridiagonal of five-diagonal system of algebraic equations is solved by
Thomas algorithm. Asymptotic complexity of this approach is
$\mathcal{O}({n}{m})$, where $n$ --- number of wavy surface grid points, $m$ ---
number of iterations. Despite their wide spread, iterative algorithms are
inefficient on parallel computer architectures; in particular, their mapping to
co-processors may involve copying data in and out of the co-processor in each
iteration, which negatively affects their performance. At the same time, high
number of Fourier transforms in the solution is an advantage, rather than a
disadvantage. First, solutions obtained by Fourier method are explicit, hence
their implementations scales with the large number of parallel computer cores.
Second, there are implementations of FFT optimised for different processor
architectures as well as co-processors (GPU, MIC) which makes it easy to get
high performance on any computing platform. These advantages substantiate the
choice of Fourier method to obtain explicit analytic solution to the problem of
determining pressures under wavy ocean surface.

*** Two-dimensional velocity field
:PROPERTIES:
:CUSTOM_ID: sec:pressure-2d
:END:
**** Formula for infinite depth fluid.
Two-dimensional Laplace equation with Robin boundary condition is written as
\begin{align}
    \label{eq:problem-2d}
    & \phi_{xx}+\phi_{zz}=0,\\
    & \zeta_t + \zeta_x\phi_x = \frac{\zeta_x}{\sqrt{1 + \zeta_x^2}} \phi_x - \phi_z, & \text{на }z=\zeta(x,t).\nonumber
\end{align}
Use Fourier method to solve this problem. Applying Fourier transform to both
sides of the equation yields
\begin{equation*}
    -4 \pi^2 \left( u^2 + v^2 \right)
    \FourierY{\phi(x,z)}{u,v} = 0,
\end{equation*}
hence $v = \pm i u$. Hereinafter we use the following symmetric form of Fourier
transform:
\begin{equation*}
    \FourierY{f(x,y)}{u,v} =
    \iint\limits_{-\infty}^{\phantom{--}\infty}
    f(x,y)
    e^{-2\pi i (x u + y v)}
    dx dy.
\end{equation*}
We seek solution in the form of inverse Fourier transform
$\phi(x,z)=\InverseFourierY{E(u,v)}{x,z}$. Plugging[fn::$v={-i}{u}$ is not
applicable because velocity potential must go to nought when depth goes to
infinity.] $v={i}{u}$ into the formula yields
\begin{equation}
    \label{eq:guessed-sol-2d}
    \phi(x,z) = \InverseFourierY{e^{2\pi u z}E(u)}{x}.
\end{equation}
In order to make substitution $z=\zeta(x,t)$ not interfere with Fourier
transforms, we rewrite eqref:eq:guessed-sol-2d as a convolution:
\begin{equation*}
    \phi(x,z)
    =
    \Fun{z}
    \ast
    \InverseFourierY{E(u)}{x},
\end{equation*}
where $\Fun{z}$ --- a function, form of which is defined in section
[[#sec:compute-delta]] and which satisfies equation
$\FourierY{\Fun{z}}{u}=e^{2\pi{u}{z}}$. Plugging formula $\phi$ into the boundary
condition yields
\begin{equation*}
    \zeta_t
    =
    \left( i f(x) - 1 \right)
    \left[
        \Fun{z}
        \ast
        \InverseFourierY{2\pi u E(u)}{x}
    \right],
\end{equation*}
where $f(x)={\zeta_x}/{\sqrt{1+\zeta_x^2}}-\zeta_x$. Applying Fourier transform
to both sides of this equation yields formula for coefficients $E$:
\begin{equation*}
    E(u) =
    \frac{1}{2\pi u}
    \frac{
    \FourierY{\zeta_t / \left(i f(x) - 1\right)}{u}
    }{
    \FourierY{\Fun{z}}{u}
    }
\end{equation*}
Finally, substituting $z$ for $\zeta(x,t)$ and plugging resulting equation into
eqref:eq:guessed-sol-2d yields formula for $\phi(x,z)$:
\begin{equation}
    \label{eq:solution-2d}
    \boxed{
        \phi(x,z)
        =
        \InverseFourierY{
            \frac{e^{2\pi u z}}{2\pi u}
            \frac{
            \FourierY{ \zeta_t / \left(i f(x) - 1\right) }{u}
            }{
            \FourierY{ \Fun{\zeta(x,t)} }{u}
            }
        }{x}.
    }
\end{equation}

Multiplier $e^{2\pi{u}{z}}/(2\pi{u})$ makes graph of a function to which Fourier
transform of which is applied asymmetric with respect to $OY$ axis. This makes
it difficult to apply FFT which expects periodic function with nought on both
ends of the interval. Using numerical integration instead of FFT is not faster
than solving the initial system of equations with numerical schemes. This
problem is alleviated by using formula eqref:eq:solution-2d-full for finite
depth fluid with wittingly large depth $h$. This formula is derived in the
following section.

**** Formula for finite depth fluid.
On the sea bottom vertical fluid velocity component equals nought: $\phi_z=0$ on
$z=-h$, where $h$ --- water depth. In this case equation $v=-{i}{u}$, which came
from Laplace equation, can not be neglected, hence the solution is sought in the
following form:
\begin{equation}
    \phi(x,z)
    =
    \InverseFourierY{
        \left( C_1 e^{2\pi u z} + C_2 e^{-2\pi u z} \right)
        E(u)
    }{x}.
    \label{eq:guessed-sol-2d-full}
\end{equation}
Plugging $\phi$ into the boundary condition on the sea bottom yields
\begin{equation*}
    C_1 e^{-2\pi u h} - C_2 e^{2\pi u h} = 0,
\end{equation*}
hence $C_1=\frac{1}{2}C{e}^{2\pi{u}{h}}$ and
$C_2=-\frac{1}{2}C{e}^{-2\pi{u}{h}}$. Constant $C$ may take arbitrary value
here, because after plugging it becomes part of unknown coefficients $E(u)$.
Plugging formulae for $C_1$ and $C_2$ into eqref:eq:guessed-sol-2d-full yields
\begin{equation*}
    \phi(x,z) = \InverseFourierY{ \Sinh{2\pi u (z+h)} E(u) }{x}.
\end{equation*}
Plugging $\phi$ into the boundary condition on the free surface yields
\begin{equation*}
    \zeta_t = f(x) \InverseFourierY{ 2\pi i u \Sinh{2\pi u (z+h)} E(u) }{x}
            - \InverseFourierY{ 2\pi u \SinhX{2\pi u (z+h)} E(u) }{x}.
\end{equation*}
Here $\sinh$ and $\cosh$ give similar results near free surface, and since this
is the main area of interest in practical applications, we assume that
$\Sinh{2\pi{u}(z+h)}\approx\SinhX{2\pi{u}(z+h)}$. Performing analogous to the
previous section transformations yields final formula for $\phi(x,z)$:
\begin{equation}
\boxed{
    \phi(x,z,t)
    =
  \InverseFourierY{
        \frac{\Sinh{2\pi u (z+h)}}{2\pi u}
        \frac{
            \FourierY{ \zeta_t / \left(i f(x) - 1\right) }{u}
        }{
            \FourierY{ \FunSecond{\zeta(x,t)} }{u}
        }
    }{x},
}
    \label{eq:solution-2d-full}
\end{equation}
where $\FunSecond{z}$ --- a function, form of which is defined in section
[[#sec:compute-delta]] and which satisfies equation
$\FourierY{\FunSecond{z}}{u}=\Sinh{2\pi{u}{z}}$.

**** Reducing to the formulae from linear wave theory.
Check the validity of derived formulae by substituting $\zeta(x,t)$ with known
analytic formula for plain waves. Symbolic computation of Fourier transforms in
this section were performed in Mathematica cite:mathematica10. In the framework
of linear wave theory assume that waves have small amplitude compared to their
lengths, which allows us simplifying initial system of equations
eqref:eq:problem-2d to
\begin{align*}
    & \phi_{xx}+\phi_{zz}=0,\\
    & \zeta_t = -\phi_z & \text{на }z=\zeta(x,t),
\end{align*}
solution to which is written as
\begin{equation*}
    \phi(x,z,t)
    =
    -\InverseFourierY{
        \frac{e^{2\pi u z}}{2\pi u}
        \FourierY{\zeta_t}{u}
    }{x}
    .
\end{equation*}
Propagating wave profile is defined as $\zeta(x,t)=A\cos(2\pi(kx-t))$. Plugging
this formula into eqref:eq:solution-2d yields
$\phi(x,z,t)=-\frac{A}{k}\sin(2\pi(kx-t))\Sinh{2\pi{k}{z}}$. In order to reduce
it to the formula from linear wave theory, rewrite hyperbolic sine in
exponential form, discard the term containing $e^{-2\pi{k}{z}}$ as contradicting
condition $\phi\underset{z\rightarrow-\infty}{\longrightarrow}0$. Taking real
part of the resulting formula yields
$\phi(x,z,t)=\frac{A}{k}e^{2\pi{k}{z}}\sin(2\pi(kx-t))$, which corresponds to
the known formula from linear wave theory. Similarly, under small-amplitude
waves assumption the formula for finite depth fluid eqref:eq:solution-2d-full is
reduced to
\begin{equation*}
    \phi(x,z,t)
    =
    -\InverseFourierY{
        \frac{\Sinh{2\pi u (z+h)}}{2\pi u \Sinh{2\pi u h}}
        \FourierY{\zeta_t}{u}
    }{x}.
\end{equation*}
Substituting $\zeta(x,t)$ with propagating plain wave profile formula yields
\begin{equation}
    \label{eq:solution-2d-linear}
    \phi(x,z,t)=\frac{A}{k}
    \frac{\Sinh{2 \pi k (z+h)}}{ \Sinh{2 \pi k h} }
    \sin(2 \pi (k x-t)),
\end{equation}
which corresponds to the formula from linear wave theory for finite depth fluid.

Different forms of Laplace equation solutions, in which decaying exponent is
written with either "+" or "-" signs, may cause incompatibilities between
formulae from linear wave theory and formulae derived in this work, where
$\sinh$ is used instead of $\cosh$. Equality
$\frac{\Sinh{2\pi{k}(z+h)}}{\Sinh{2\pi{k}{h}}}\approx\frac{\sinh(2\pi{k}(z+h))}{\sinh(2\pi{k}{h})}$
becomes strict on the free surface, and difference between left-hand and
right-hand sides increases when approaching sea bottom (for sufficiently large
depth difference near free surface is negligible). So, for sufficiently large
depth any function ($\cosh$ or $\sinh$) may be used for velocity potential
computation near free surface.

Reducing eqref:eq:solution-2d и eqref:eq:solution-2d-full to the known formulae
from linear wave theory shows, that formula for infinite depth
eqref:eq:solution-2d is not suitable to compute velocity potentials with Fourier
method, because it does not have symmetry, which is required for Fourier
transform. However, formula for finite depth can be used instead by setting $h$
to some characteristic water depth. For standing wave reducing to linear wave
theory formulae is made under the same assumptions.

*** Three-dimensional velocity field
Three-dimensional version of eqref:eq:problem is written as
\begin{align}
    \label{eq:problem-3d}
    & \phi_xx + \phi_yy + \phi_zz = 0,\\
    & \zeta_t + \zeta_x\phi_x + \zeta_y\phi_y
    =
    \frac{\zeta_x}{\sqrt{1 + \zeta_x^2}} \phi_x
    +\frac{\zeta_y}{\sqrt{\vphantom{\zeta_x^2}\smash[b]{1 + \zeta_y^2}}} \phi_y
    - \phi_z, & \text{на }z=\zeta(x,y,t).\nonumber
\end{align}
Again, use Fourier method to solve it. Applying Fourier transform to both sides
of Laplace equation yields
\begin{equation*}
    -4 \pi^2 \left( u^2 + v^2 + w^2 \right)
    \FourierY{\phi(x,y,z)}{u,v,w} = 0,
\end{equation*}
hence $w=\pm{i}\sqrt{u^2+v^2}$. We seek solution in the form of inverse Fourier
transform $\phi(x,y,z)=\InverseFourierY{E(u,v,w)}{x,y,z}$. Plugging
$w=i\sqrt{u^2+v^2}$ into the formula yields
\begin{equation*}
    \phi(x,y,z) = \InverseFourierY{
        \left(
            C_1 e^{2\pi \sqrt{u^2+v^2} z}
            -C_2 e^{-2\pi \sqrt{u^2+v^2} z}
        \right)
        E(u,v)
    }{x,y}.
\end{equation*}
Plugging $\phi$ into the boundary condition on the sea bottom (analogous to
two-dimensional case) yields
\begin{equation}
    \label{eq:guessed-sol-3d}
    \phi(x,y,z) = \InverseFourierY{
        \Sinh{2\pi \sqrt{u^2+v^2} (z+h)} E(u,v)
    }{x,y}.
\end{equation}
Plugging $\phi$ into the boundary condition on the free surface yields
\begin{equation*}
    \arraycolsep=1.4pt
    \begin{array}{rl}
        \zeta_t = & i f_1(x,y) \InverseFourierY{2 \pi u \Sinh{2\pi \sqrt{u^2+v^2} (z+h)}E(u,v)}{x,y} \\
        + & i f_2(x,y) \InverseFourierY{2 \pi v \Sinh{2\pi \sqrt{u^2+v^2} (z+h)}E(u,v)}{x,y} \\
        - & \InverseFourierY{2 \pi \sqrt{u^2+v^2} \SinhX{2\pi \sqrt{u^2+v^2} (z+h)}E(u,v)}{x,y}
    \end{array}
\end{equation*}
where $f_1(x,y)={\zeta_x}/{\sqrt{1+\zeta_x^2}}-\zeta_x$ and
$f_2(x,y)={\zeta_y}/{\sqrt{\vphantom{\zeta_x^2}\smash[b]{1+\zeta_y^2}}}-\zeta_y$.
Applying Fourier transform to both sides of the equation yields formula for
coefficients $E$:
\begin{equation*}
    \arraycolsep=1.4pt
    \begin{array}{rl}
        \FourierY{\zeta_t}{u,v} = &
        \FourierY{i f_1(x,y) \InverseFourierY{2 \pi u \Sinh{2\pi \sqrt{u^2+v^2} (z+h)} E(u,v)}{x,y}}{u,v}  \\
        + & \FourierY{i f_2(x,y) \InverseFourierY{2 \pi v \Sinh{2\pi \sqrt{u^2+v^2} (z+h)} E(u,v)}{x,y}}{u,v}  \\
        - & 2 \pi \sqrt{u^2+v^2} \SinhX{2\pi \sqrt{u^2+v^2} (z+h)} E(u,v)
    \end{array}
\end{equation*}
Final solution is obtained after plugging $E(u,v)$ into eqref:eq:guessed-sol-3d.

* Numerical methods and experimental results
** The shape of ACF for different types of waves
*** Two methods to find ocean waves ACF
**** Analytic method of finding the ACF.
The straightforward way to find ACF for a given ocean wave profile is to apply
Wiener---Khinchin theorem. According to this theorem the autocorrelation $K$ of
a function $\zeta$ is given by the Fourier transform of the absolute square of
the function:
\begin{equation}
  K(t) = \Fourier{\left| \zeta(t) \right|^2}.
  \label{eq:wiener-khinchin}
\end{equation}
When $\zeta$ is replaced with actual wave profile, this formula gives you
analytic formula for the corresponding ACF.

For three-dimensional wave profile (2D in space and 1D in time) analytic formula
is a polynomial of high order and is best obtained via symbolic computation
programme. Then for practical usage it can be approximated by superposition of
exponentially decaying cosines (which is how ACF of a stationary ARMA process
looks like cite:box1976time).

**** Empirical method of finding the ACF.
However, for three-dimensional case there exists simpler empirical method which
does not require sophisticated software to determine shape of the ACF. It is
known that ACF represented by exponentially decaying cosines satisfies first
order Stokes' equations for gravity waves cite:boccotti1983wind. So, if the
shape of the wave profile is the only concern in the simulation, then one can
simply multiply it by a decaying exponent to get appropriate ACF. This ACF does
not reflect other wave profile parameters, such as wave height and period, but
opens possibility to simulate waves of a particular non-analytic shape by
"drawing" their profile, then multiplying it by an exponent and using the
resulting function as ACF. So, this empirical method is imprecise but offers
simpler alternative to Wiener---Khinchin theorem approach; it is mainly useful
to test ARMA model.

*** Examples of ACFs for various types of wave profiles
**** Standing wave.
For three-dimensional plain standing wave the profile is given by
\begin{equation}
  \zeta(t, x, y) = A \sin (k_x x + k_y y) \sin (\sigma t).
  \label{eq:standing-wave}
\end{equation}
Find ACF via analytic method. Multiplying the formula by a decaying exponent
(because Fourier transform is defined for a function $f$ that
$f\underset{x\rightarrow\pm\infty}{\longrightarrow}0$) yields
\begin{equation}
  \zeta(t, x, y) =
  A
  \exp\left[-\alpha (|t|+|x|+|y|) \right]
  \sin (k_x x + k_y y) \sin (\sigma t).
  \label{eq:decaying-standing-wave}
\end{equation}
Then, apply 3D Fourier transform to both sides of the equation via symbolic
computation programme, fit the resulting polynomial to the following
approximation:
\begin{equation}
  K(t,x,y) =
  \gamma
  \exp\left[-\alpha (|t|+|x|+|y|) \right]
  \cos \beta t
  \cos \left[ \beta x + \beta y \right].
  \label{eq:standing-wave-acf}
\end{equation}
So, after applying Wiener---Khinchin theorem we get initial formula but with
cosines instead of sines. This difference is important because the value of ACF
at $(0,0,0)$ equals to the ARMA process variance, and if one used sines the
value would be wrong.

If one tries to replicate the same formula via empirical method, the usual way
is to adapt eqref:eq:decaying-standing-wave to match eqref:eq:standing-wave-acf.
This can be done either by changing the phase of the sine, or by substituting
sine with cosine to move the maximum of the function to the origin of
coordinates.

**** Propagating wave.
Three-dimensional profile of plain propagating wave is given by
\begin{equation}
  \zeta(t, x, y) = A \cos (\sigma t + k_x x + k_y y).
  \label{eq:propagating-wave}
\end{equation}
For the analytic method repeating steps from the previous two paragraphs yields
\begin{equation}
  K(t,x,y) =
  \gamma
  \exp\left[-\alpha (|t|+|x|+|y|) \right]
  \cos\left[\beta (t+x+y) \right].
  \label{eq:propagating-wave-acf}
\end{equation}
For the empirical method the wave profile is simply multiplied by a decaying
exponent without need to adapt the maximum value of ACF (as it is required for
standing wave).

*** Comparison of studied methods
To summarise, the analytic method of finding ocean wave's ACF reduces to the
following steps.
- Make wave profile decay when approaching $\pm\infty$ by multiplying it by
  a decaying exponent.
- Apply Fourier transform to the absolute square of the resulting equation using
  symbolic computation programme.
- Fit the resulting polynomial to the appropriate ACF approximation.

Two examples in this section showed that in case of standing and propagating
waves their decaying profiles resemble the corresponding ACFs with the exception
that the ACF's maximum should be moved to the origin to preserve simulated
process variance. Empirical method of finding ACF reduces to the following
steps.
- Make wave profile decay when approaching $\pm\infty$ by multiplying it by
  a decaying exponent.
- Move maximum value of the resulting function to the origin by using
  trigonometric identities to shift the phase.

** Additional formulae, methods and algorithms for ARMA model
*** Wave elevation distribution approximation
One of the parameters of ocean wavy surface generator is probability density
function (PDF) of the surface elevation. This distribution is given by either
polynomial approximation of /in situ/ data or analytic formula.

**** Gram---Charlier series expansion.
In cite:huang1980experimental the authors experimentally show, that PDF of sea
surface elevation is distinguished from normal distribution by non-nought
kurtosis and skewness. In cite:рожков1996теория the authors show, that this type
of PDF expands in Gram---Charlier series:
\begin{align}
    \label{eq:skew-normal-1}
    F(z; \gamma_1, \gamma_2) & = \phi(z)
        - \gamma_1 \frac{\phi'''(z)}{3!}
        + \gamma_2 \frac{\phi''''(z)}{4!} \nonumber \\
    & =
    \frac{1}{2} \text{erf}\left[\frac{z}{\sqrt{2}}\right]
    -
    \frac{e^{-\frac{z^2}{2}}}{\sqrt{2\pi}}
    \left[
        \frac{1}{6} \gamma_1 \left(z^2-1\right)
        + \frac{1}{24} \gamma_2 z \left(z^2-3\right)
    \right]
    ,\nonumber \\
    f(z; \gamma_1, \gamma_2) & =
    \frac{e^{-\frac{z^2}{2}}}{\sqrt{2 \pi }}
    \left[
        \frac{1}{6} \gamma_1 z \left(z^2-3\right)
        + \frac{1}{24} \gamma_2 \left(z^4-6z^2+3\right)
        +1
    \right],
\end{align}
where $\phi(z)=\frac{1}{2}\mathrm{erf}(z/\sqrt{2})$, $\gamma_1$ --- skewness,
$\gamma_2$ --- kurtosis, $f$ --- PDF, $F$ --- cumulative distribution function
(CDF). According to cite:рожков1990вероятностные for ocean waves skewness is
selected from interval $0.1\leq\gamma_1\leq{0.52}]$ and kurtosis from interval
$0.1\leq\gamma_2\leq{0.7}$. Family of probability density functions for
different parameters is shown in [[fig:skew-normal-1]].

#+name: fig:skew-normal-1
#+begin_src R :results output graphics :exports results :file build/skew-normal-1.pdf
source(file.path("R", "common.R"))
x <- seq(-3, 3, length.out=100)
params <- data.frame(
  skewness = c(0.00, 0.52, 0.00, 0.52),
  kurtosis = c(0.00, 0.00, 0.70, 0.70),
  linetypes = c("solid", "dashed", "dotdash", "dotted")
)
arma.skew_normal_1_plot(x, params)
legend(
  "topleft",
  mapply(
    function (s, k) {
      as.expression(bquote(list(
        gamma[1] == .(arma.fmt(s, 2)),
        gamma[2] == .(arma.fmt(k, 2))
      )))
    },
    params$skewness,
    params$kurtosis
  ),
  lty = paste(params$linetypes)
)
#+end_src

#+caption: Probability density function eqref:eq:skew-normal-1 of ocean wavy surface elevation for different values of skewness $\gamma_1$ and kurtosis $\gamma_2$.
#+RESULTS: fig:skew-normal-1
[[file:build/skew-normal-1.pdf]]

**** Skew-normal distribution.
Alternative approach is to approximate distribution of ocean wavy surface
elevation by skew-normal distribution:
\begin{align}
    \label{eq:skew-normal-2}
    F(z; \alpha) & = \frac{1}{2}
   \mathrm{erfc}\left[-\frac{z}{\sqrt{2}}\right]-2 T(z,\alpha ), \nonumber \\
    f(z; \alpha) & = \frac{e^{-\frac{z^2}{2}}}{\sqrt{2 \pi }}
   \mathrm{erfc}\left[-\frac{\alpha z}{\sqrt{2}}\right],
\end{align}
where $T$ --- Owen \(T\)-function cite:owen1956tables. Using this formula it is
impossible to specify skewness and kurtosis separately --- both values are
adjusted via $\alpha$ parameter. The only advantage of the formula is its
relative computational simplicity: this function is available in some programmes
and mathematical libraries. Its graph for different values of $\alpha$ is shown
in [[fig:skew-normal-2]].

#+name: fig:skew-normal-2
#+begin_src R :results output graphics :exports results :file build/skew-normal-2.pdf
source(file.path("R", "common.R"))
x <- seq(-3, 3, length.out=100)
alpha <- c(0.00, 0.87, 2.25, 4.90)
params <- data.frame(
  alpha = alpha,
  skewness = arma.bits.skewness_2(alpha),
  kurtosis = arma.bits.kurtosis_2(alpha),
  linetypes = c("solid", "dashed", "dotdash", "dotted")
)
arma.skew_normal_2_plot(x, params)
legend(
  "topleft",
  mapply(
    function (a, s, k) {
      as.expression(bquote(list(
        alpha == .(arma.fmt(a, 2)),
        gamma[1] == .(arma.fmt(s, 2)),
        gamma[2] == .(arma.fmt(k, 2))
      )))
    },
    params$alpha,
    params$skewness,
    params$kurtosis
  ),
  lty = paste(params$linetypes)
)
#+end_src

#+caption: Probability density function eqref:eq:skew-normal-2 of ocean wavy surface for different values of skewness coefficient $\alpha$.
#+RESULTS: fig:skew-normal-2
[[file:build/skew-normal-2.pdf]]

**** Evaluation.
Equation eqref:eq:distribution-transformation with selected wave elevation
distribution may be solved either in every point of generated wavy surface,
which gives the most accurate results, or in every fixed grid point
interpolating result via least-squares (LS) polynomial. In the second case
precision is lower. For example, interpolating 12^th order polynomial on a fixed
grid of 500 points on interval $-5\sigma_z\leq{z}\leq{5}\sigma_z$ gives error of
$\approx{0.43}\cdot10^{-3}$. Increasing polynomial order leads to either numeric
overflows during LS interpolation, or more coefficient close to nought;
increasing the size of the grid has insignificant effect on the result. In the
majority of cases three Gram---Charlier series coefficients is enough to
transform ACF; relative error without interpolation is $10^{-5}$.

*** White noise generation
In order to eliminate periodicity from generated wavy surface, it is imperative
to use PRNG with sufficiently large period to generate white noise. Parallel
Mersenne Twister cite:matsumoto1998mersenne with a period of $2^{19937}-1$ is
used as a generator in this work. It allows producing aperiodic ocean wavy
surface realisations in any practical usage scenarios.

There is no guarantee that multiple Mersenne Twisters executed in parallel
threads with distinct initial states produce uncorrelated pseudo-random number
sequences, however, algorithm of dynamic creation of Mersenne Twisters
cite:matsumoto1998dynamic may be used to provide such guarantee. The essence of
the algorithm is to find matrices of initial generator states, that give
maximally uncorrelated pseudo-random number sequences when Mersenne Twisters are
executed in parallel with these initial states. Since finding such initial
states consumes considerable amount of processor time, vector of initial states
is created preliminary with knowingly larger number of parallel threads and
saved to a file, which is then read before starting white noise generation.

*** Wavy surface generation
In ARMA model value of wavy surface elevation at a particular point depends on
previous in space and time points, as a result the so called /ramp-up interval/
(see fig. [[fig:ramp-up-interval]]), in which realisation does not correspond to
specified ACF, forms in the beginning of the realisation. There are several
solutions to this problem which depend on the simulation context.

If realisation is used in the context of ship stability simulation without
manoeuvring, ramp-up interval will not affect results of the simulation, because
it is located on the border (too far away from the studied marine object). If
ship stability with manoeuvring is studied, then the interval may be simply
discarded from the realisation (the size of the interval approximately equals
the number of AR coefficients in each dimension). However, this may lead to loss
of a very large number of points, because discarding occurs for each dimension.
Alternative approach is to generate ocean wavy surface on ramp-up interval with
LH model and generate the rest of the realisation with ARMA model.

Algorithm of wavy surface generation is data-parallel: realisation is divided
into equal parts each of which is generated independently, however, in the
beginning of each realisation there is ramp-up interval. To eliminate it
/overlap-add/ method
cite:oppenheim1989discrete,svoboda2011efficient,pavel2013algorithms (a popular
method in signal processing) is used. The essence of the method is to add
another interval, size of which is equal to the ramp-up interval size, to the
end of each part. Then wavy surface is generated in each point of each part
(including points from the added interval), the interval at the end of part $N$
is superimposed on the ramp-up interval at the beginning of the part $N+1$, and
values in corresponding points are added.

#+name: fig:ramp-up-interval
#+begin_src R :results output graphics :exports results :file build/ramp-up-interval.pdf
source(file.path("R", "common.R"))
arma.plot_ramp_up_interval()
#+end_src

#+caption: Ramp-up interval at the beginning of the $OX$ axis of the realisation.
#+RESULTS: fig:ramp-up-interval
[[file:build/ramp-up-interval.pdf]]

*** Velocity potential normalisation formulae
:PROPERTIES:
:CUSTOM_ID: sec:compute-delta
:END:

In solutions eqref:eq:solution-2d and eqref:eq:solution-2d-full to
two-dimensional pressure determination problem there are functions
$\Fun{z}=\InverseFourierY{e^{2\pi{u}{z}}}{x}$ and
$\FunSecond{z}=\InverseFourierY{\Sinh{2\pi{u}{z}}}{x}$ which has multiple
analytic representations and are difficult to compute. Each function is a
Fourier transform of linear combination of exponents which reduces to poorly
defined Dirac delta function of a complex argument (see [[tab:delta-functions]]).
The usual way of handling this type of functions is to write them as
multiplication of Dirac delta functions of real and imaginary part, however,
this approach does not work here, because applying inverse Fourier transform to
this representation does not produce exponent, which severely warp resulting
velocity field. In order to get unique analytic definition normalisation factor
$1/\Sinh{2\pi{u}{h}}$ (which is also included in formula for $E(u)$) may be
used. Despite the fact that normalisation allows obtaining adequate velocity
potential field, numerical experiments show that there is little difference
between this field and the one produced by formulae from linear wave theory, in
which terms with $\zeta$ are omitted.

#+name: tab:delta-functions
#+caption: Formulae for computing $\Fun{z}$ and $\FunSecond{z}$ from [[#sec:pressure-2d]], that use normalisation to eliminate uncertainty from definition of Dirac delta function of complex argument.
#+attr_latex: :booktabs t
| Function        | Without normalisation                                      | Normalised                                                                                                                           |
|-----------------+------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------|
| $\Fun{z}$       | $\delta (x+i z)$                                           | $\frac{1}{2 h}\mathrm{sech}\left(\frac{\pi  (x-i (h+z))}{2 h}\right)$                                                                |
| $\FunSecond{z}$ | $\frac{1}{2}\left[\delta (x-i z) + \delta (x+i z) \right]$ | $\frac{1}{4 h}\left[\text{sech}\left(\frac{\pi  (x-i (h+z))}{2 h}\right)+\text{sech}\left(\frac{\pi  (x+i(h+z))}{2 h}\right)\right]$ |

** ARMA model verification
:PROPERTIES:
:CUSTOM_ID: sec:verification
:END:

In cite:degtyarev2011modelling,degtyarev2013synoptic,boukhanovsky1997thesis AR
model the following items are verified experimentally:
- probability distributions of different wave characteristics (wave heights,
  lengths, crests, periods, slopes, three-dimensionality),
- dispersion relation,
- retention of integral characteristics for mixed wave sea state.
In this work both AR and MA model are verified by comparing probability
distributions of different wave characteristics.

*** Verification of wavy surface integral characteristics
In cite:рожков1990вероятностные the authors show that several ocean wave
characteristics (listed in table [[tab:weibull-shape]]) have Weibull distribution,
and wavy surface elevation has Gaussian distribution. In order to verify that
distributions corresponding to generated realisation are correct,
quantile-quantile plots are used (plots where analytic quantile values are used
for $OX$ axis and estimated quantile values for $OY$ axis). If the estimated
distribution matches analytic then the graph has the form of the straight line.
Tails of the graph may diverge from the straight line, because they can not be
reliably estimated from the finite-size realisation. Different methods of
extracting waves from realisation produce variations in quantile function tails,
it is probably impractical to extract every possible wave from realisation since
they may (and often) overlap.

#+name: tab:weibull-shape
#+caption: Values of Weibull shape parameter for different wave characteristics.
#+attr_latex: :booktabs t
| Characteristic       | Weibull shape ($k$) |
|----------------------+---------------------|
| Wave height          |                   2 |
| Wave length          |                 2.3 |
| Crest length         |                 2.3 |
| Wave period          |                   3 |
| Wave slope           |                 2.5 |
| Three-dimensionality |                 2.5 |

Verification was performed for standing and propagating waves. The corresponding
ACFs and quantile-quantile plots of wave characteristics distributions are shown
in fig. [[acf-slices]], [[standing-wave-distributions]], [[propagating-wave-distributions]].

#+name: propagating-wave-distributions
#+begin_src R :results output graphics :exports results :file build/propagating-wave-qqplots.pdf
source(file.path("R", "common.R"))
par(pty="s", mfrow=c(2, 2))
arma.qqplot_grid(
  file.path("build", "propagating_wave"),
  c("elevation", "heights_y", "lengths_y", "periods"),
  c("elevation", "height Y", "length Y", "period"),
  xlab="x",
  ylab="y"
)
#+end_src

#+caption: Quantile-quantile plots for propagating waves.
#+RESULTS: propagating-wave-distributions
[[file:build/propagating-wave-qqplots.pdf]]

#+name: standing-wave-distributions
#+begin_src R :results output graphics :exports results :file build/standing-wave-qqplots.pdf
source(file.path("R", "common.R"))
par(pty="s", mfrow=c(2, 2))
arma.qqplot_grid(
  file.path("build", "standing_wave"),
  c("elevation", "heights_y", "lengths_y", "periods"),
  c("elevation", "height Y", "length Y", "period"),
  xlab="x",
  ylab="y"
)
#+end_src

#+caption: Quantile-quantile plots for standing waves.
#+RESULTS: standing-wave-distributions
[[file:build/standing-wave-qqplots.pdf]]

#+name: acf-slices
#+header: :width 6 :height 9
#+begin_src R :results output graphics :exports results :file build/acf-slices.pdf
source(file.path("R", "common.R"))
propagating_acf <- read.csv(file.path("build", "propagating_wave", "acf.csv"))
standing_acf <- read.csv(file.path("build", "standing_wave", "acf.csv"))
par(mfrow=c(5, 2), mar=c(0,0,0,0))
for (i in seq(0, 4)) {
  arma.wavy_plot(standing_acf, i, zlim=c(-5,5))
  arma.wavy_plot(propagating_acf, i, zlim=c(-5,5))
}
#+end_src

#+caption: Time slices of ACF for standing (left column) and propagating waves (right column).
#+RESULTS: acf-slices
[[file:build/acf-slices.pdf]]

*** TODO Discuss graphs
*** Verification of velocity potential fields
:PROPERTIES:
:CUSTOM_ID: sec:compare-formulae
:END:

Comparing obtained generic formulae eqref:eq:solution-2d and
eqref:eq:solution-2d-full to the known formulae from linear wave theory allows
seeing the difference between velocity fields for both large and small amplitude
waves. In general analytic formula for velocity potential in not known, even for
plain waves, so comparison is done numerically. Taking into account conclusions
of [[#sec:pressure-2d]], only finite depth formulae are compared.

**** The difference with linear wave theory formulae.
The experiment shows that velocity potential fields produced by formula
eqref:eq:solution-2d-full for finite depth fluid and formula
eqref:eq:solution-2d-linear from linear wave theory are qualitatively different
(fig. [[fig:potential-field-nonlinear]]). First, velocity potential contours have
sinusoidal shape, which is different from oval shape described by linear wave
theory. Second, velocity potential decays more rapidly than in linear wave
theory as getting closer to the bottom, and the region where the majority of
wave energy is concentrated is closer to the wave crest. Similar numerical
experiment, in which all terms of eqref:eq:solution-2d-full that are neglected
in the framework of linear wave theory are eliminated, shows no difference (as
much as machine precision allows) in resulting velocity potential fields.

#+name: fig:potential-field-nonlinear
#+caption: Velocity potential field of propagating wave $\zeta(x,y,t) = \cos(2\pi x - t/2)$. Field produced by formula eqref:eq:solution-2d-full (left) and linear wave theory formula (right).
#+attr_latex: :width 0.47\textwidth
#+begin_figure
[[file:graphics/pressure/potential-5.eps]]
[[file:graphics/pressure/potential-6.eps]]
#+end_figure

**** The difference with small-amplitude wave theory.
The experiment shows that velocity fields produced by formula
eqref:eq:solution-2d-full and eqref:eq:old-sol-2d correspond to each other for
small-amplitude waves. Two ocean wavy surface realisations are made by AR model:
one contains small-amplitude waves, other contains large-amplitude waves.
Integration in formula eqref:eq:solution-2d-full is done over wave numbers range
extracted from the generated wavy surface. For small-amplitude waves both
formulae show comparable results (the difference in the velocity is attributed
to stochastic nature of AR model), whereas for large-amplitude waves stable
velocity field is produced only by formula eqref:eq:solution-2d-full (fig.
[[fig:velocity-field-2d]]). So, generic formula eqref:eq:solution-2d-full gives
satisfactory results without restriction on wave amplitudes.

#+name: fig:velocity-field-2d
#+caption: Comparison of velocity field on the ocean wavy surface obtained by generic formula ($u_1$) and formula for small-amplitude waves ($u_2$). Velocity field for realisations containing small-amplitude (left) and large-amplitude (right) waves.
#+begin_figure
[[file:build/low-amp-nocolor.eps]]
[[file:build/high-amp-nocolor.eps]]
#+end_figure
*** Non-physical nature of ARMA model
ARMA model, owing to its non-physical nature, does not have the notion of ocean
wave; it simulates wavy surface as a whole instead. Motions of individual waves
and their shape are often rough, and the total number of waves can not be
determined precisely. However, integral characteristics of wavy surface match
the ones of real ocean waves.

Theoretically, ocean waves themselves can be chosen as ACFs, the only
pre-processing step is to make them decay exponentially. This may allow
generating waves of arbitrary profiles, and is one of the directions of future
work.

* High-performance software implementation of ocean wave simulation
** Computational model
*** Mapping wavy surface generation algorithm on computational model
Software implementation of ARMA model works as a computational pipeline, in
which each joint applies some function to the output coming from the pipe of the
previous joint. Joints are distributed across computer cluster nodes to enable
function parallelism, and then data flowing through the joints is distributed
across processor cores to enable data parallelism. Figure [[fig:pipeline]] shows a
diagram of data processing pipeline in which rectangles with rounded corners
denote joints, regular rectangles denote arrays of problem domain objects
flowing from one joint to another, and arrows show flow direction. Some joints
are divided into /sections/ each of which process a separate part of the array.
If joints are connected without a /barrier/ (horizontal or vertical bar), then
transfer of separate objects between them is done in parallel to computations,
as they become available. Sections work in parallel on each processor core (or
node of the cluster). There is surjective mapping between a set of processor
cores, a set of pipeline joint sections and objects, i.e. each processor core
may run several sections, each of which may sequentially process several
objects, but a section can not work simultaneously on several processor cores,
and an object can not be processed simultaneously by several sections.

#+name: fig:pipeline
#+begin_src dot :exports results :file build/pipeline.pdf
digraph {

  node [fontsize=14,margin="0.055,0"]
  graph [nodesep="0.25",ranksep="0.25",rankdir="TB"]
  edge [arrowsize=0.66]

  # data
  subgraph xcluster_linear {
    label="Linear model"

    start [label="",shape=circle,style=filled,fillcolor=black,width=0.23]
    spectrum [label="S(ω,θ)",shape=box]
    acf [label="K(i,j,k)",shape=box]
    phi [label="Φ(i,j,k)",shape=box]

    # transformations
    fourier_transform [label="Fourier transform",shape=box,style=rounded]
    solve_yule_walker [label="Solve Yule—Walker\nequations",shape=box,style=rounded]

    subgraph cluster_nonlinear_1 {
      label="Simulate non-linearity\l"
      labeljust=left
      style=filled
      color=lightgrey
      acf2 [label="K*(i,j,k)",shape=box]
      transform_acf [label="Transform ACF",shape=box,style=rounded]
    }
  }

  subgraph xcluster_linear2 {

    eps_parts [label="<e1> ε₁|<e2> ε₂|<e3> …|<e4> εₙ|<e> ε(t,x,y)",shape=record]
    end [label="",shape=doublecircle,style=filled,fillcolor=black,width=0.23]

    generate_white_noise [label="<g1> g₁|<g2> g₂|<g3> …|<g4> gₙ|<gen> Generate\lwhite noise",shape=record,style=rounded]
    generate_zeta [label="<g1> g₁|<g2> g₂|<g3> …|<g4> gₙ|<gen> Generate ocean\lwavy surface parts\l",shape=record,style=rounded]

    zeta_parts [label="<g1> ζ₁|<g2> ζ₂|<g3> …|<g4> ζₙ|<gen> Non-crosslinked\lrealisation parts",shape=record]
    overlap_add [label="<g1> ζ₁|<g2> ζ₂|<g3> …|<g4> ζₙ|<gen> Crosslink realisation\lparts\l",shape=record,style=rounded]

    zeta_parts:g1->overlap_add:g1
    zeta_parts:g2->overlap_add:g2
    zeta_parts:g3->overlap_add:g3
    zeta_parts:g4->overlap_add:g4

    zeta_parts:g2->overlap_add:g1 [constraint=false]
    zeta_parts:g3->overlap_add:g2 [constraint=false]
    zeta_parts:g4->overlap_add:g3 [constraint=false]

    overlap_add:g1->zeta2_parts:g1
    overlap_add:g2->zeta2_parts:g2
    overlap_add:g3->zeta2_parts:g3
    overlap_add:g4->zeta2_parts:g4

    zeta2_parts:g1->transform_zeta:g1->zeta3_parts:g1->write_zeta:g1->eps_end
    zeta2_parts:g2->transform_zeta:g2->zeta3_parts:g2->write_zeta:g2->eps_end
    zeta2_parts:g3->transform_zeta:g3->zeta3_parts:g3->write_zeta:g3->eps_end
    zeta2_parts:g4->transform_zeta:g4->zeta3_parts:g4->write_zeta:g4->eps_end

  }

  subgraph part3 {

    zeta2_parts [label="<g1> ζ₁|<g2> ζ₂|<g3> …|<g4> ζₙ|<gen> Wavy surface with\lGaussian distribution\l",shape=record]

    subgraph cluster_nonlinear_2 {
      label="Simulate non-linearity\r"
      labeljust=right
      style=filled
      color=lightgrey
      zeta3_parts [label="<g1> ζ₁|<g2> ζ₂|<g3> …|<g4> ζₙ|<gen> ζ(t,x,y)",shape=record]
      transform_zeta [label="<g1> g₁|<g2> g₂|<g3> …|<g4> gₙ|<gen> Transform wavy\lsurface elevation\lprobability distribution\l",shape=record,style=rounded]
    }

    # barriers
    eps_start [label="",shape=box,style=filled,fillcolor=black,height=0.05]
    eps_end [label="",shape=box,style=filled,fillcolor=black,height=0.05]

    write_zeta [label="<g1> g₁|<g2> g₂|<g3> …|<g4> gₙ|<gen> Write finished\lparts to a file\l",shape=record,style=rounded]
  }

  # edges
  start->spectrum->fourier_transform->acf->transform_acf
  transform_acf->acf2
  acf2->solve_yule_walker
  solve_yule_walker->phi
  phi->eps_start [constraint=false]
  eps_start->generate_white_noise:g1
  eps_start->generate_white_noise:g2
  eps_start->generate_white_noise:g3
  eps_start->generate_white_noise:g4
  generate_white_noise:g1->eps_parts:e1->generate_zeta:g1->zeta_parts:g1
  generate_white_noise:g2->eps_parts:e2->generate_zeta:g2->zeta_parts:g2
  generate_white_noise:g3->eps_parts:e3->generate_zeta:g3->zeta_parts:g3
  generate_white_noise:g4->eps_parts:e4->generate_zeta:g4->zeta_parts:g4

  eps_end->end
}
#+end_src

#+caption: Diagram of data processing pipeline, that implements ocean wavy surface generation via AR model.
#+RESULTS: fig:pipeline
[[file:build/pipeline.pdf]]

Object pipeline may be seen as an improvement of BSP (Bulk Synchronous Parallel)
model cite:valiant1990bridging, which is used in graph processing
cite:malewicz2010pregel,seo2010hama. Pipeline eliminates global synchronisation
(where it is possible) after each sequential computation step by doing data
transfer between joints in parallel to computations, whereas in BSP model global
synchronisation occurs after each step.

Object pipeline speeds up the programme by parallel execution of code blocks
that work with different compute devices: while the current part of wavy surface
is generated by a processor, the previous part is written to a disk. This
approach allows getting speed-up because compute devices operate asynchronously,
and their parallel usage increases the whole programme performance.

Since data transfer between pipeline joints is done in parallel to computations,
the same pipeline may be used to run several copies of the application but with
different parameters (generate several ocean wavy surfaces having different
characteristics). In practise, high-performance applications do not always
consume 100% of processor time spending a portion of time on synchronisation of
parallel processes and writing data to disk. Using pipeline in this case allows
running several computations on the same set of processes, and use all of the
computer devices at maximal efficiency. For example, when one object writes data
to a file, the other do computations on the processor in parallel. This
minimises downtime of the processor and other computer devices and increases
throughput of the computer cluster.

Pipelining of otherwise sequential steps is beneficial not only for code work
with different devices, but for code different branches of which are suitable
for execution by multiple hardware threads of the same processor core, i.e.
branches accessing different memory blocks or performing mixed arithmetic
(integer and floating point). Code branches which use different modules of
processor are good candidates to run in parallel on a processor core with
multiple hardware threads.

So, computational model with a pipeline can be seen as /bulk-asynchronous
model/, because of the parallel nature of programme steps. This model is the
basis of the fault-tolerance model which will be described later.

**** Software implementation.
For efficiency reasons object pipeline and fault tolerance techniques (which
will be described later) are implemented in the C++ framework: From the authors'
perspective C language is deemed low-level for distributed programmes, and Java
incurs too much overhead and is not popular in HPC community. As of now, the
framework runs in the same process as an parallel application that uses it. The
framework is called Factory, it is now in proof-of-concept development stage.

*** Computational model overview
The key feature that is missing in the current parallel programming technologies
is a possibility to specify hierarchical dependencies between parallel tasks.
When one has such dependency, it is trivial to determine which task should be
responsible for re-executing a failed task on one of the survived nodes. To
re-execute the task on the top of the hierarchy, a backup task is created and
executed on a different node. There exists a number of systems that are capable
of executing directed acyclic graphs of tasks in parallel
cite:acun2014charmpp,islam2012oozie, but graphs are not suitable to infer
principal-subordinate relationship between tasks, because a node in the graph
may have multiple parent nodes.

The main purpose of the model is to simplify development of distributed batch
processing applications and middleware. The main focus is to make application
resilient to failures, i.e. make it fault tolerant and highly available, and do
it transparently to a programmer. The implementation is divided into two layers:
the lower layer consists of routines and classes for single node applications
(with no network interactions), and the upper layer for applications that run on
an arbitrary number of nodes. There are two kinds of tightly coupled entities in
the model --- /control flow objects/ (or /kernels/) and /pipelines/ --- which
are used together to compose a programme.

Kernels implement control flow logic in theirs ~act~ and ~react~ methods and
store the state of the current control flow branch. Both logic and state are
implemented by a programmer. In ~act~ method some function is either directly
computed or decomposed into nested functions (represented by a set of
subordinate kernels) which are subsequently sent to a pipeline. In ~react~
method subordinate kernels that returned from the pipeline are processed by
their parent. Calls to ~act~ and ~react~ methods are asynchronous and are made
within threads attached to a pipeline. For each kernel ~act~ is called only
once, and for multiple kernels the calls are done in parallel to each other,
whereas ~react~ method is called once for each subordinate kernel, and all the
calls are made in the same thread to prevent race conditions (for different
parent kernels different threads may be used).

Pipelines implement asynchronous calls to ~act~ and ~react~, and try to make as
many parallel calls as possible considering concurrency of the platform (no. of
cores per node and no. of nodes in a cluster). A pipeline consists of a kernel
pool, which contains all the subordinate kernels sent by their parents, and a
thread pool that processes kernels in accordance with rules outlined in the
previous paragraph. A separate pipeline is used for each device: There are
pipelines for parallel processing, schedule-based processing (periodic and
delayed tasks), and a proxy pipeline for processing of kernels on other cluster
nodes (see figure [[fig:subord-ppl]]).

In principle, kernels and pipelines machinery reflect the one of procedures and
call stacks, with the advantage that kernel methods are called asynchronously
and in parallel to each other (as much as programme logic allows). Kernel field
is the stack, ~act~ method is a sequence of processor instructions before nested
procedure call, and ~react~ method is a sequence of processor instructions after
the call. Constructing and sending subordinate kernels to the pipeline is nested
procedure call. Two methods are necessary to make calls asynchronous, and
replace active wait for completion of subordinate kernels with passive one.
Pipelines, in turn, allow implementing passive wait, and call correct kernel
methods by analysing their internal state.

#+name: fig:subord-ppl
#+begin_src dot :exports results :file build/subord-ppl.pdf
graph G {

  node [fontsize=14,margin="0.055,0",shape=box]
  graph [nodesep="0.25",ranksep="0.25",rankdir="LR"]
  edge [arrowsize=0.66]

  subgraph cluster_daemon {
    label="Daemon process"
    style=filled
    color=lightgrey

    factory [label="Factory"]
    parallel_ppl [label="Parallel pipeline"]
    io_ppl [label="I/O pipeline"]
    sched_ppl [label="Schedule-based pipeline"]
    net_ppl [label="Network pipeline"]
    proc_ppl [label="Process pipeline"]

    upstream [label="Upstream thread pool"]
    downstream [label="Downstream thread pool"]
  }

  factory--parallel_ppl
  factory--io_ppl
  factory--sched_ppl
  factory--net_ppl
  factory--proc_ppl

  subgraph cluster_hardware {
    label="Compute devices"
    style=filled
    color=lightgrey

    cpu [label="CPU"]
    core0 [label="Core 0"]
    core1 [label="Core 1"]
    core2 [label="Core 2"]
    core3 [label="Core 3"]

    storage [label="Storage"]
    disk0 [label="Disk 0"]

    network [label="Network"]
    nic0 [label="NIC 0"]

    timer [label="Timer"]

  }

  core0--cpu
  core1--cpu
  core2--cpu
  core3--cpu

  disk0--storage
  nic0--network

  parallel_ppl--upstream
  parallel_ppl--downstream

  upstream--{core0,core1,core2,core3} [style="dashed"]
  downstream--core0 [style="dashed"]

  io_ppl--core0 [style="dashed"]
  io_ppl--disk0 [style="dashed"]
  sched_ppl--core0 [style="dashed"]
  sched_ppl--timer [style="dashed"]
  net_ppl--core0 [style="dashed"]
  net_ppl--nic0 [style="dashed"]
  proc_ppl--core0 [style="dashed"]

  subgraph cluster_children {
    style=filled
    color=white

    subgraph cluster_child0 {
      label="Child process 0"
      style=filled
      color=lightgrey
      labeljust=right

      app0_factory [label="Factory"]
      app0 [label="Child process\rpipeline"]
    }

    subgraph cluster_child1 {
      label="Child process 1"
      style=filled
      color=lightgrey
      labeljust=right

      app1_factory [label="Factory"]
      app1 [label="Child process\rpipeline"]
    }
  }

  proc_ppl--app0
  proc_ppl--app1

  app0_factory--app0 [constraint=false]
  app1_factory--app1 [constraint=false]

}
#+end_src

#+caption: Mapping of parent and child process pipelines to compute devices.
#+RESULTS: fig:subord-ppl
[[file:build/subord-ppl.pdf]]

*** Governing principles
Data processing pipeline model is based on the following principles, following
which maximises efficiency of a programme.
- There is no notion of a message in the model, a kernel is itself a message
  that can be sent over network to another node and directly access any kernel
  on the local node. Only programme logic may guarantee the existence of the
  kernel.
- A kernel is a /cooperative routine/, which is submitted to kernel pool upon the
  call and is executed asynchronously by a scheduler. There can be any number of
  calls to other subroutines inside routine body. Every call submits
  corresponding subroutine to kernel pool and returns immediately. Kernels in the
  pool can be executed in any order; this fact is used by a scheduler to exploit
  parallelism offered by the computer by distributing kernels from the pool
  across available cluster nodes and processor cores.
- Asynchronous execution prevents the use of explicit synchronisation after the
  call to subroutine is made; system scheduler returns control flow to the
  routine each time one of its subroutine returns. Such cooperation transforms
  each routine which calls subroutines into event handler, where each event is a
  subroutine and the handler is the routine that called them.
- The routine may communicate with any number of local kernels, addresses of
  which it knows; communication with kernels which are not adjacent in the call
  stack complexifies control flow and call stack looses its tree shape. Only
  programme logic may guarantee presence of communicating kernels in memory. One
  way to ensure this is to perform communication between subroutines which are
  called from the same routine. Since such communication is possible within
  hierarchy through parent routine, it may treated as an optimisation that
  eliminates overhead of transferring data over intermediate node. The situation
  is different for interactive or event-based programmes (e.g. servers and
  programmes with graphical interface) in which this is primary type of
  communication.
- In addition to this, communication which does not occur along hierarchical
  links and executed over cluster network complexify design of resiliency
  algorithms. Since it is impossible to ensure that a kernel resides in memory
  of a neighbour node, because a node may fail in the middle of its execution of
  the corresponding routine. As a result, upon failure of a routine all of its
  subroutines must be restarted. This encourages a programmer to construct
  - deep tree hierarchies of tightly-coupled kernels (which communicate on the
    same level of hierarchy) to reduce overhead of recomputation;
  - fat tree hierarchies of loosely-coupled kernels, providing maximal degree of
    parallelism.
  Deep hierarchy is not only requirement of technology, it helps optimise
  communication of large number of cluster nodes reducing it to communication of
  adjacent nodes.

So, control flow objects (or kernels) possess properties of both cooperative
routines and event handlers.

** SMP implementation
*** Load balancing algorithm
The simplest approach to balance the load on a multi-processor system is to
split data into equal parts (or a task into homogeneous subtasks) and to
distribute them evenly between processor cores and cluster nodes, however, this
approach does not work efficiently in all cases. First, the total number of
parts, into which input data is split, is often dictated by the problem being
solved, rather than computer system architecture. Such load balancing may not
efficient from the computer system point of view: the number of parts is either
too large compared to the number of processors working in parallel, which
increases data transfer overhead, or too small, which prevents using all
available processor cores. Second, restrictions of problem being solved may not
allow splitting input data into even parts which may result in load imbalance
across processor cores. Third, there are multiple components in the system aside
from the processor that take part in the computation (such as vector
co-processors and storage devices), and the problem solution time depends on the
performance of all the components involved. So, how to make load balancing
algorithm more efficient in the presence of non-homogeneous input data parts and
take into account all the devices involved in the computation?

The load balancing algorithm consists of two stages. In the first stage, the
algorithm places input data part (or a subtask) wrapped in a kernel into an
appropriate kernel pool: there is a separate pool for each device and an
associated thread pool. In the second stage, a kernel is retrieved from the pool
by one of the threads and processed. Due to separate thread pools all devices
work in parallel to each other, lowering overall system resources downtime
compared to using all devices from a single thread.

In order to take into account non-homogeneous input data parts or tasks, one may
predict execution time of each task. Relevant study is done in
cite:degtyarev2016balance since ARMA model implementation includes mostly
homogeneous tasks.

So, load balancing is done in two stages: in the first stage the task wrapped in
the kernel is routed to the appropriate device and in the second stage the
kernel is routed to one of the thread from the device thread pool.
Non-homogeneous kernels may be handled by predicting their execution time, but
such kernels are not present in ARMA model implementation.

*** Evaluation
**** Performance of MPI, OpenMP, OpenCL implementations.
ARMA model does not require highly optimised software implementation to be
efficient, its performance is high even without use of co-processors; there are
two main causes of that. First, ARMA model itself does not use transcendental
functions (sines, cosines and exponents) as opposed to LH model. All
calculations (except model coefficients) are done via polynomials, which can be
efficiently computed on modern processors using a series of FMA instructions.
Second, pressure computation is done via explicit analytic formula using nested
FFTs. Since two-dimensional FFT of the same size is repeatedly applied to every
time slice, its coefficients (complex exponents) are pre-computed for all
slices, and computations are performed with only a few transcendental functions.
In case of MA model, performance is also increased by doing convolution with FFT
transforms. So, high performance of ARMA model is due to scarce use of
transcendental functions and heavy use of FFT, not to mention that high
convergence rate and non-existence of periodicity allows using far fewer
coefficients compared to LH model.

ARMA implementation uses several libraries of reusable mathematical functions
and numerical algorithms (listed in [[tab:arma-libs]]), and was implemented using
several parallel programming technologies (MPI, OpenMP, OpenCL) to find the most
efficient one.

#+name: tab:arma-libs
#+caption: A list of mathematical libraries used in ARMA model implementation.
#+attr_latex: :booktabs t :align lp{0.6\linewidth}
| Library                                                | What it is used for             |
|--------------------------------------------------------+---------------------------------|
| DCMT cite:matsumoto1998dynamic                         | parallel PRNG                   |
| Blitz cite:veldhuizen1997will,veldhuizen2000techniques | multidimensional arrays         |
| GSL cite:gsl2008scientific                             | PDF, CDF, FFT computation       |
|                                                        | checking process stationarity   |
| LAPACK, GotoBLAS cite:goto2008high,goto2008anatomy     | finding AR coefficients         |
| GL, GLUT cite:kilgard1996opengl                        | three-dimensional visualisation |

**** Performance of load balancing algorithm.
Software implementation of wavy surface generation is balanced in terms of the
load on processor cores, however, as shown by tests, has high load on storage
device. Before testing wavy surface generation was implemented using OpenMP for
parallel computations and in order to implement load balancing algorithm was
rewritten using POSIX threads. Performance of the two implementations was
compared on the platform with the configuration listed in table
[[tab:multicore-specs]].

#+name: tab:multicore-specs
#+caption: Multi-core system configuration.
#+attr_latex: :booktabs t
| Component                 | Details                          |
|---------------------------+----------------------------------|
| Programming language      | C++11                            |
| Threading library         | C++11 STL threads                |
| Atomic operations library | C++11 STL atomic                 |
| Routines to measure time  | ~clock_gettime(CLOCK_MONOTONIC)~ |
|                           | ~/usr/bin/time -f \%e~           |
| Compiler                  | GCC 4.8.2                        |
| Compiler flags            | ~-std=c++11 -O2 -march=native~   |
| Operating system          | Debian 3.2.51-1 x86_64           |
| File system               | ext4                             |
| Processor                 | Intel Core 2 Quad Q9650          |
| Core frequency (GHz)      | 3.00                             |
| No. of cores              | 4                                |
| Amount of RAM (GB)        | 8                                |
| Disk                      | Seagate ST3250318AS              |
| Disk speed (rpm)          | 7200                             |

The experiment consisted of running both implementations on a multi-core machine
varying the size of the surface; the size of CPU thread pool and I/O thread pool
was not changed during the experiment. I/O thread pool consisted of one thread,
and CPU thread pool size was equal the number of physical processor cores.

In the experiment load balancing algorithm showed higher performance than
implementation without it. The more the size of the generated surface is the
more the gap in performance is (fig. [[fig:factory-performance]]) which is a result
of overlap of computation phase and data output phase (fig.
[[fig:factory-overlap]]). In OpenMP implementation data output phase begins only
when computation is over, whereas load balancing algorithm makes both phases end
almost simultaneously. So, /pipelined execution of internally parallel
sequential phases is more efficient than their sequential execution/, and this
allows balancing the load across different devices involved in computation.

#+name: fig:factory-performance
#+begin_src R :results output graphics :exports results :file build/factory-vs-openmp.pdf
source(file.path("R", "common.R"))
arma.plot_factory_vs_openmp(
  xlab="Realisation size",
  ylab="Time, s",
  power=6
)
#+end_src

#+caption: Performance comparison of OpenMP and Factory implementations.
#+RESULTS: fig:factory-performance
[[file:build/factory-vs-openmp.pdf]]

#+name: fig:factory-overlap
#+header: :width 9 :height 4
#+begin_src R :results output graphics :exports results :file build/factory-vs-openmp-overlap.pdf
source(file.path("R", "common.R"))
par(mar=c(5, 6, 0, 1), pty="m")
arma.plot_factory_vs_openmp_overlap(
  xlab="Time, s",
  labels=c("Factory", "OpenMP"),
  scale=10**9
)
#+end_src

#+caption: Overlap of parallel computations on $[G_0,G_1]$ and data output to disk on $[W_0,W_1]$. In OpenMP implementation there is no overlap.
#+RESULTS: fig:factory-overlap
[[file:build/factory-vs-openmp-overlap.pdf]]

Proposed load balancing method for multi-core systems allows increasing
performance of applications that read or write large volumes of data to disk,
but may be used in other cases too. The main idea of the algorithm is to
classify the load and find the suitable device to route the load to. So, any
devices other than disks may be used as well.

** MPP implementation
*** Cluster node discovery algorithm
:PROPERTIES:
:CUSTOM_ID: sec:node-discovery
:END:
**** Introduction.
Many distributed systems are built on the principle of /subordination/: there is
principal node in each cluster which manages job queue, schedules their
execution on subordinate nodes and monitors their state. Principal role is
assigned either /statically/ by an administrator to a particular physical node,
or /dynamically/ by electing one of the cluster nodes as principal. In the
former case fault tolerance is provided by reserving additional spare node which
takes principal role when current principal fails. In the latter case fault
tolerance is provided by electing new principal node from survived nodes.
Despite the fact that dynamic role assignment requires specialised distributed
algorithm, this approach becomes more and more popular as it does not require
spare reserved nodes to recover from principal node failure.

Leader election algorithms (which sometimes referred to as /distributed
consensus/ algorithms are special cases of wave algorithms. In
cite:tel2000introduction Tel defines them as algorithms in which termination
event is preceded by at least one event occurring in /each/ parallel process.
Wave algorithms are not defined for anonymous networks, that is they apply only
to processes that can uniquely define themselves. However, the number of
processes affected by the "wave" can be determined in the course of an
algorithm. For a distributed system this means that wave algorithms work for
computer clusters with dynamically changing number of nodes, and the algorithm
is unaffected by some nodes going on-line and off-line.

The approach in the following work does not use wave algorithms, and hence does
not require communicating with each node of the cluster to determine a leader.
Instead, each node enumerates all nodes in the network it is part of, and
converts this list to a /tree hierarchy/ with a user-defined maximal fan-out
value (maximal number of subordinate nodes). Then the node determines its
hierarchy level and tries to communicate with nodes from higher levels to become
their subordinate. First, it checks the closest ones and then goes all the way
to the top. If there is no top-level nodes or the node cannot connect to them,
then the node itself becomes the principal of the hierarchy.

Tree hierarchy of all hosts in a network defines strict total order on a set of
cluster nodes. Although, technically any function can be chosen to map a node to
a number, in practise this function should be sufficiently smooth along the time
axis and may have infrequent jumps: high-frequency oscillations (which are often
caused by measurement errors) may result in constant passing of principal role
from one node to another, which makes the cluster unmanageable. The simplest
such function is the position of an IP address in network IP address range.

The following key features distinguish this approach with respect to some
existing proposals
cite:brunekreef1996design,aguilera2001stable,romano2014design.
- *Multi-level hierarchy.* The number of principal nodes in a network depends on
  the fan-out value. If it is lesser than the number of IP-addresses in the
  network, then there are multiple principle nodes in the cluster. If it is
  greater or equal to the number of IP-addresses in the network, then there is
  only one principal node. When some node fail, multi-level hierarchy changes
  locally, only nodes adjacent to the failed one communicate.
- *IP-address mapping.* Since hierarchy structure solely depends on the nodes'
  IP addresses, there is no election phase in the algorithm. To change the
  principal each node sends a message to the old principal and to the new one.
- *Completely event-based.* The messages are sent only when some node fails, so
  there is no constant load on the network. Since the algorithm allows
  tolerating failure of sending any message, there is no need in heartbeat
  packets indicating presence of a node in the network; instead, all messages
  play role of heartbeats and packet send time-out is adjusted.
- *No manual configuration.* A node does not require any prior knowledge to find
  the principal: it determines the network it is part of, calculates potential
  principal IP-address and sends the message. If it fails, the process is
  repeated for the next potential principal node. So the algorithm is suitable
  to bootstrap a cluster without manual configuration, the only requirement is
  to start the corresponding service on each node.
To summarise, the advantage of the algorithm is that it
- scales to a large number of nodes by means of hierarchy with multiple
  principals,
- does not constantly load the network with node state updates and heartbeat
  packets,
- does not require manual configuration to bootstrap a cluster.

The disadvantage of the algorithm is that it requires IP-address to change
infrequently. It is not suitable for cloud environments in which node DNS name
is preserved, but IP-address may change over time. When IP-address changes,
current connections may close, thus triggering node "failure" and rebuilding
node hierarchy. So, environments where nodes are not identified by IP-addresses,
are not suitable for the algorithm.

The other disadvantage is that the algorithm creates artificial dependence of
node rank on IP-address: it is difficult to substitute IP-address mapping with a
more sophisticated one (e.g. a mapping which uses current node and network load
to infer node ranks) because measurement errors may result in unstable
hierarchy, and the algorithm cease to be fully event-based.

Node discovery algorithm is designed to balance the load on a cluster of compute
nodes, its use in other applications is not studied here. When distributed or
parallel programme starts on any of cluster nodes, its subtasks are distributed
to all adjacent nodes in the hierarchy (including principal node if applicable).
To distribute the load evenly when the application is run on a subordinate node,
each node maintains weight of each adjacent node in the hierarchy. The weight
equals to the number of nodes in the tree "behind" the adjacent node. For
example, if the weight of the first adjacent node is 2, then round-robin load
balancing algorithm distributes two subtasks to the first node before moving to
the next one.

To summarise, node discovery algorithm is
- designed to ease load balancing on the cluster,
- fully fault-tolerant the state of every node can be recomputed at any time,
- fully event-based which means it does not load the network by periodically
  sending messages.

**** Building a tree hierarchy.
Strict total order on the set $\mathcal{N}$ of cluster nodes connected to a
network is defined as
\begin{equation*}
  \forall n_1 \forall n_2 \in \mathcal{N},
  \forall f \colon \mathcal{N} \rightarrow \mathcal{R}^n
  \Rightarrow (f(n_1) < f(n_2) \Leftrightarrow \neg (f(n_1) \geq f(n_2))),
\end{equation*}
where $f$ maps a node to its rank and operator $<$ defines strict total order on
$\mathcal{R}^n$. Function $f$ defines node's sequential number, and $<$ makes
this number unique.

The simpliest function $f$ maps each node to its Internet address position in
network IP address range. Without conversion to a tree (when only \emph{one}
leader is allowed in the network) a node with the lowest position in this range
becomes the principal. If IP-address of a node occupies the first position in
the range, then there is no principal for it, and it continues to be at the top
of the hierarchy until it fails. Although, IP address mapping is simple to
implement, it introduces artificial dependence of the principal role on the
address of a node. Still, it is useful for initial configuration of a cluster
when more complex mappings are not applicable.

To make discovery algorithm scale to a large number of nodes, IP address range
is mapped to a tree hierarchy. In this hierarchy each node is uniquely
identified by its hierarchy level \(l\), which it occupies, and offset \(o\),
which equals to the sequential number of node on its level. Values of level and
offset are computed from the following optimisation problem.
\begin{align*}
    n = \sum\limits_{i=0}^{l(n)} p^i + o(n), \quad
    l \rightarrow \min, \quad
    o \rightarrow \min, \quad
    l \geq 0, \quad
    o \geq 0
\end{align*}
where $n$ is the position of node's IP address in network IP address range and
$p$ is fan-out value (the maximal number of subordinates, a node can have). The
principal of a node with level $l$ and offset $o$ has level $l-1$ and offset
$\lfloor{o/p}\rfloor$. The distance between any two nodes in the tree with
network positions $i$ and $j$ is computed as
\begin{align*}
    & \langle
        \text{lsub}(l(j), l(i)), \quad
        \left| o(j) - o(i)/p \right|
    \rangle,\\
    & \text{lsub}(l_1, l_2) =
    \begin{cases}
        \infty & \quad \text{if } l_1 \geq l_2, \\
        l_1 - l_2 & \quad \text{if } l_1 < l_2.
    \end{cases}
\end{align*}
The distance is compound to account for level in the first place.

To determine its principal each node ranks all nodes in the network according to
their position $\langle{l(n),o(n)}\rangle$, and using distance formula chooses
the node which is closest to potential principal position and has lower rank.
That way IP addresses of offline nodes are skipped, however, for sparse networks
(in which nodes occupy non-contiguous IP addresses) perfect tree is not
guaranteed.

In order to determine its principal a node is required to communicate to a node
address of which it knows beforehand, so discovery algorithm scales to a large
number of nodes. Communication with other nodes in ranked list occurs only when
the current principal node fails. So, if address of cluster nodes occupy
contiguous addresses network IP address range, each node connects to its
principal only, and inefficient scan of all network by each node does not occur.

**** Evaluation results.
Test platform consisted of several multi-core nodes, on top of which virtual
clusters with varying number of nodes were deployed using Linux network
namespaces. Similar approach is used in
cite:lantz2010network,handigol2012reproducible,heller2013reproducible where the
authors reproduce various real-world experiments using virtual clusters and
compare results to physical ones. The advantage of it is that the tests can be
performed on a large virtual cluster using relatively small number of physical
nodes. This approach was used to evaluate node discovery algorithm, because the
algorithm has low requirement for system resources (processor time and network
throughput).

Performance of the algorithm was evaluated by measuring time needed to all nodes
of the cluster to discover each other. Each change of the hierarchy (as seen by
each node) was written to a file and after 30 seconds all the processes (each of
which models cluster node) were forcibly terminated. Test runs showed that
running more than 100 virtual nodes on one physical node simultaneously warp the
results, thus additional physical nodes, each of which run 100 virtual nodes,
were used for the experiment. The experiment showed that discovery of 100--400
nodes each other takes 1.5 seconds on average, and the value increases only
slightly with increase in the number of nodes (see fig. [[fig:bootstrap-local]]). An
example of tree hierarchy for 11 nodes with fan-out 2 is shown in fig.
[[fig:tree-hierarchy-11]].

#+name: fig:bootstrap-local
#+caption: Time to discover all nodes of the cluster in depending on number of nodes.
[[file:graphics/discovery.eps]]

#+name: fig:tree-hierarchy-11
#+begin_src dot :exports results :file build/tree-hierarchy-11.pdf
digraph {

  node [fontsize=14,margin="0.055,0",shape=box,style=rounded]
  graph [nodesep="0.15",ranksep="0.20",rankdir="BT"]
  edge [arrowsize=0.66]

  m1 [label="127.0.0.1"]
  m2 [label="127.0.0.2"]
  m3 [label="127.0.0.3"]
  m4 [label="127.0.0.4"]
  m5 [label="127.0.0.5"]
  m6 [label="127.0.0.6"]
  m7 [label="127.0.0.7"]
  m8 [label="127.0.0.8"]
  m9 [label="127.0.0.9"]
  m10 [label="127.0.0.10"]
  m11 [label="127.0.0.11"]

  m2->m1
  m3->m1
  m4->m2
  m5->m2
  m6->m3
  m7->m3
  m8->m4
  m9->m4
  m10->m5
  m11->m5
}
#+end_src

#+caption: Tree hierarchy for 11 nodes with fan-out equals 2.
#+RESULTS: fig:tree-hierarchy-11
[[file:build/tree-hierarchy-11.pdf]]

*** Fail over algorithm
**** Introduction.
Fault tolerance of data processing pipelines is one of the top concerns in
development of job schedulers for big data processing, however, most schedulers
provide fault tolerance for subordinate nodes only. These types of failures are
routinely mitigated by restarting the failed job or its part on healthy nodes,
and failure of a master node is often considered either improbable, or too
complicated to handle and configure on the target platform. System
administrators often find alternatives to application level fault tolerance:
they isolate master node from the rest of the cluster by placing it on a
dedicated machine, or use virtualisation technologies instead. All these
alternatives complexify configuration and maintenance, and by decreasing
probability of a machine failure resulting in a whole system failure, they
increase probability of a human error.

From such point of view it seems more practical to implement master node fault
tolerance at application level, however, there is no generic implementation.
Most implementations are too tied to a particular application to become
universally acceptable. We believe that this happens due to people's habit to
think of a cluster as a collection of individual machines each of which can be
either master or slave, rather than to think of a cluster as a whole with master
and slave roles being dynamically assigned to a particular physical machine.

This evolution in thinking allows to implement middleware that manages master
and slave roles automatically and handles node failures in a generic way. This
software provides an API to distribute parallel tasks on the pool of available
nodes and among them. Using this API one can write an application that runs on a
cluster without knowing the exact number of online nodes. The middleware works
as a cluster operating system overlay allowing to write distributed
applications.

**** Related work.
Dynamic role assignment is an emerging trend in design of distributed systems
cite:ostrovsky2015couchbase,divya2013elasticsearch,boyer2012glusterfs,anderson2010couchdb,lakshman2010cassandra,
however, it is still not used in big data job schedulers. For example, in
popular YARN job scheduler cite:vavilapalli2013yarn, which is used by Hadoop and
Spark big data analysis frameworks, master and slave roles are static. Failure
of a slave node is tolerated by restarting a part of a job on a healthy node,
and failure of a master node is tolerated by setting up standby reserved server
cite:murthy2011architecture. Both master servers are coordinated by Zookeeper
service which itself uses dynamic role assignment to ensure its fault-tolerance
cite:okorafor2012zookeeper. So, the whole setup is complicated due to Hadoop
scheduler lacking dynamic roles: if dynamic roles were available, Zookeeper
would be redundant in this setup. Moreover, this setup does not guarantee
continuous operation of master node because standby server needs time to recover
current state after a failure.

The same problem occurs in high-performance computing where master node of a job
scheduler is the single point of failure. In
cite:uhlemann2006joshua,engelmann2006symmetric the authors use replication to
make the master node highly-available, but backup server role is assigned
statically and cannot be delegated to a healthy worker node. This solution is
closer to fully dynamic role assignment than high-availability solution for big
data schedulers, because it does not involve using external service to store
configuration which should also be highly-available, however, it is far from
ideal solution where roles are completely decoupled from physical servers.

Finally, the simplest master node high-availability is implemented in Virtual
Router Redundancy Protocol (VRRP)
cite:knight1998rfc2338,hinden2004virtual,nadas2010rfc5798. Although VRRP
protocol does provide master and backup node roles, which are dynamically
assigned to available routers, this protocol works on top of the IPv4 and IPv6
protocols and is designed to be used by routers and reverse proxy servers. Such
servers lack the state that needs to be restored upon a failure (i.e.~there is
no job queue in web servers), so it is easier for them to provide
high-availability. In Linux it is implemented in Keepalived routing daemon
cite:cassen2002keepalived.

In contrast to web servers and HPC and Big Data job schedulers, some distributed
key-value stores and parallel file systems have symmetric architecture, where
master and slave roles are assigned dynamically, so that any node can act as a
master when the current master node fails
cite:ostrovsky2015couchbase,divya2013elasticsearch,boyer2012glusterfs,anderson2010couchdb,lakshman2010cassandra.
This design decision simplifies management and interaction with a distributed
system. From system administrator point of view it is much simpler to install
the same software stack on each node than to manually configure master and slave
nodes. Additionally, it is much easier to bootstrap new nodes into the cluster
and decommission old ones. From user point of view, it is much simpler to
provide web service high-availability and load-balancing when you have multiple
backup nodes to connect to.

Dynamic role assignment would be beneficial for Big Data job schedulers because
it allows to decouple distributed services from physical nodes, which is the
first step to build highly-available distributed service. The reason that there
is no general solution to this problem is that there is no generic programming
environment to write and execute distributed programmes. The aim of this work is
to propose such an environment and to describe its internal structure.

The programming model used in this work is partly based on well-known actor
model of concurrent computation cite:agha1985actors,hewitt1973universal. Our
model borrows the concept of actor---an object that stores data and methods to
process it; this object can react to external events by either changing its
state or producing more actors. We call this objects /computational kernels/.
Their distinct feature is hierarchical dependence on parent kernel that created
each of them, which allows to implement fault-tolerance based on simple restart
of a failed subordinate kernel.

However, using hierarchical dependence alone is not enough to develop
high-availability of a master kernel---the first kernel in a parallel programme.
To solve the problem the other part of our programming model is based on
bulk-synchronous parallel model cite:valiant1990bridging. It borrows the concept
of superstep---a sequential step of a parallel programme; at any time a
programme executes only one superstep, which allows to implement
high-availability of the first kernel (under assumption that it has only one
subordinate at a time) by sending it along its subordinate to a different
cluster node thus making a distributed copy of it. Since the first kernel has
only one subordinate at a time, its copy is always consistent with the original
kernel. This eliminates the need for complex distributed transactions and
distributed consensus algorithms and guarantees protection from at most one
master node failure per superstep.

To summarise, the framework developed in this paper protects a parallel
programme from failure of any number of subordinate nodes and from one failure
of a master node per superstep. The paper does not answer the question of how to
determine if a node failed, it assumes a failure when the network connection to
a node is prematurely closed.

**** Hierarchy of control flow objects
For load balancing purposes cluster nodes are combined into tree hierarchy (see
section [[#sec:node-discovery]]), and the load is distributed between direct
neighbours: when one runs the kernel on the subordinate node, the principal node
also receive some of its subordinate kernels. This makes the system symmetrical
and easy to maintain: each node have the same set of software that allows
replacing one node with another in case of failure of the former. Similar
architectural solution used in key-value stores
cite:anderson2010couchdb,lakshman2010cassandra to provide fault tolerance, but
author does not know any task schedulers that use this approach.

Unlike ~main~ function in programmes based on message passing library, the first
(the main) kernel is initially run only on one node, and remote nodes are used
on-demand. This design choice allows having arbitrary number of nodes throughout
execution of a programme, and use more nodes for highly parallel parts of the
code. Similar choice is made in the design of big data frameworks
cite:dean2008mapreduce,vavilapalli2013yarn --- a user submitting a job does not
specify the number of hosts to run its job on, and actual hosts are the hosts
where input files are located.

From mathematical point of view kernel $K$ can be described as a vector-valued
functional which recursively maps a kernel to \(n\)-component vector of kernels:
\begin{equation*}
    K(f): \mathbb{K} \rightarrow \mathbb{K}^n
    \qquad
    \mathbb{K}^n = \left\{ f: \mathbb{K} \rightarrow \mathbb{K}^n \right\}.
\end{equation*}
Special kernel $\mathbb{O}: \mathbb{K} \rightarrow \mathbb{K}^0$ is used to stop
the recursion and is passed as an argument to the main kernel. An argument to a
kernel is interpreted as follows.
- If a kernel is a newly created kernel, then its argument is its parent kernel.
- In other cases the argument is an arbitrary kernel (often a child of the
  current kernel).

Kernels are processed in a loop which starts with executing the main kernel,
then inside the main kernel other kernels are created and executed
asynchronously. The loop continues until some kernel returns \(\mathbb{O}\).
Since kernel may return multiple kernels they are executed in parallel, which
quickly fills kernel pool. Since kernels from the pool may be executed in
unspecified order, several concurrent threads retrieve kernels from the pool and
may send the remaining kernels to neighbouring cluster nodes if the pool
overflows.

Kernels are implemented as closures (functors in C++) --- function objects
containing all their arguments, a reference to parent kernel and application
domain data. The data is either processed upon kernel call, or subordinate
kernels are created to process it in parallel. When the processing is complete a
parent kernel closure with its subordinate kernel as an argument is called to
collect the resulting data from it.

**** Fault tolerance.
**** Handling single node failures.
Basic strategy to overcome a failure of a subordinate node is to restart
corresponding kernels on healthy node---a strategy employed in Erlang language
to restart failed subordinate processes cite:armstrong2003thesis. To implement
this we record every kernel that is sent to remote cluster nodes, and in an
event of a node failure these kernels are simply rescheduled to other
subordinate nodes with no special handling from a programmer. If there are no
nodes to sent kernels to, they are scheduled locally. So, in contrast to
heavy-weight checkpoint/restart machinery, tree hierarchy allows automatic and
transparent handling of subordinate node failures without restarting parallel
processes on every node.

A possible way of handling a failure of a node where the first kernel is located
is to replicate this kernel to a backup node, and make all updates to its state
propagate to the backup node by means of a distributed transaction. However,
this approach does not play well with asynchronous nature of computational
kernels. Fortunately, the first kernel usually does not perform operations in
parallel, it is rather sequentially launches execution steps one by one, so it
has only one subordinate at a time. Keeping this in mind, we can simplify
synchronisation of its state: we can send the first kernel along with its
subordinate to the subordinate node. When the node with the first kernel fails,
its copy receives its subordinate, and no execution time is lost. When the node
with its copy fails, its subordinate is rescheduled on some other node, and a
whole step of computation is lost in the worst case.

Described approach works only for kernels that do not have a parent and have
only one subordinate at a time, which means that they act as optimised
checkpoints. The advantage is that they save results after each sequential step,
when memory footprint of a programme is low, they save only relevant data, and
they use memory of a subordinate node instead of stable storage.

**** High availability.
A possible way of handling a failure of a node where the first kernel is located
(a master node) is to replicate this kernel to a backup node, and make all
updates to its state propagate to the backup node by means of a distributed
transaction. This approach requires synchronisation between all nodes that
execute subordinates of the first kernel and the node with the first kernel
itself. When a node with the first kernel goes offline, the nodes with
subordinate kernels must know what node is the backup one. However, if the
backup node also goes offline in the middle of execution of some subordinate
kernel, then it is impossible for this kernel to discover the next backup node
to return to, because this kernel has not discovered the unavailability of the
master node yet. One can think of a consensus-based algorithm to ensure that
subordinate kernels always know where the backup node is, but distributed
consensus algorithms do not scale well to the large number of nodes and they are
not reliable cite:fischer1985impossibility. So, consensus-based approach does
not play well with asynchronous nature of computational kernels as it may
inhibit scalability of a parallel programme.

Fortunately, the first kernel usually does not perform operations in parallel,
it is rather sequentially launches execution steps one by one, so it has only
one subordinate at a time. Such behaviour is described by bulk-synchronous
parallel programming model, in the framework of which a programme consists of
sequential supersteps which are internally parallel cite:valiant1990bridging.
Keeping this in mind, we can simplify synchronisation of its state: we can send
the first kernel along with its subordinate to the subordinate node. When the
node with the first kernel fails, its copy receives its subordinate, and no
execution time is lost. When the node with its copy fails, its subordinate is
rescheduled on some other node, and in the worst case a whole step of
computation is lost.

Described approach works only for kernels that do not have a parent and have
only one subordinate at a time, and act similar to manually triggered
checkpoints. The advantage is that they
- save results after each sequential step when memory footprint of a programme
  is low,
- they save only relevant data,
- and they use memory of a subordinate node instead of stable storage.
**** Evaluation.
Factory framework is evaluated on physical cluster (Table [[tab:cluster]]) on
the example of hydrodynamics HPC application which was developed
in cite:autoreg-stab,autoreg2011csit,autoreg1,autoreg2. This programme
generates wavy ocean surface using ARMA model, its output is a set of files
representing different parts of the realisation. From a computer scientist point
of view the application consists of a series of filters, each applying to the
result of the previous one. Some of the filters are parallel, so the programme
is written as a sequence of big steps and some steps are made internally
parallel to get better performance. In the programme only the most
compute-intensive step (the surface generation) is executed in parallel across
all cluster nodes, and other steps are executed in parallel across all cores of
the master node.

#+name: tab:cluster
#+caption: Test platform configuration.
#+attr_latex: :booktabs t
| CPU                       | Intel Xeon E5440, 2.83GHz |
| RAM                       | 4Gb                       |
| HDD                       | ST3250310NS, 7200rpm      |
| No. of nodes              | 12                        |
| No. of CPU cores per node | 8                         |

The application was rewritten for the new version of the framework which
required only slight modifications to handle failure of a node with the first
kernel: The kernel was flagged so that the framework makes a replica and sends
it to some subordinate node. There were no additional code changes other than
modifying some parts to match the new API. So, the tree hierarchy of kernels is
mostly non-intrusive model for providing fault tolerance which demands explicit
marking of replicated kernels.

In a series of experiments we benchmarked performance of the new version of the
application in the presence of different types of failures (numbers correspond
to the graphs in Figure [[fig:benchmark]]):
- no failures,
- failure of a slave node (a node where a part of wavy surface is generated),
- failure of a master node (a node where the first kernel is run),
- failure of a backup node (a node where a copy of the first kernel is stored).
A tree hierarchy with fan-out value of 64 was chosen to make all cluster nodes
connect directly to the first one. In each run the first kernel was launched on
a different node to make mapping of kernel hierarchy to the tree hierarchy
optimal. A victim node was made offline after a fixed amount of time after the
programme start which is equivalent approximately to $1/3$ of the total run time
without failures on a single node. All relevant parameters are summarised in
Table [[tab:benchmark]] (here ``root'' and ``leaf'' refer to a node in the
tree hierarchy). The results of these runs were compared to the run without node
failures (Figures [[fig:benchmark]]-[[fig:slowdown]]).

There is considerable difference in net performance for different types of
failures. Graphs 2 and 3 in Figure [[fig:benchmark]] show that performance in
case of master or slave node failure is the same. In case of master node failure
a backup node stores a copy of the first kernel and uses this copy when it fails
to connect to the master node. In case of slave node failure, the master node
redistributes the load across remaining slave nodes. In both cases execution
state is not lost and no time is spent to restore it, that is why performance is
the same. Graph 4 in Figure [[fig:benchmark]] shows that performance in case
of a backup node failure is much lower. It happens because master node stores
only the current step of the computation plus some additional fixed amount of
data, whereas a backup node not only stores the copy of this information but
executes this step in parallel with other subordinate nodes. So, when a backup
node fails, the master node executes the whole step once again on arbitrarily
chosen healthy node.

#+name: tab:benchmark
#+caption: Benchmark parameters.
#+attr_latex: :booktabs t
| Experiment no. | Master node | Victim node | Time to offline, s |
|              1 | root        |             |                    |
|              2 | root        | leaf        |                 10 |
|              3 | leaf        | leaf        |                 10 |
|              4 | leaf        | root        |                 10 |

Finally, to measure how much time is lost due to a failure we divide the total
execution time with a failure by the total execution time without the failure
but with the number of nodes minus one. The results for this calculation are
obtained from the same benchmark and are presented in Figure [[fig:slowdown]]. The
difference in performance in case of master and slave node failures lies within
5% margin, and in case of backup node failure within 50% margin for the number
of node less than 6[fn::Measuring this margin for higher number of nodes does
not make sense since time before failure is greater than total execution time
with these numbers of nodes, and programme's execution finishes before a failure
occurs.]]. Increase in execution time of 50% is more than $1/3$ of execution
time after which a failure occurs, but backup node failure need some time to be
discovered: they are detected only when subordinate kernel carrying the copy of
the first kernel finishes its execution and tries to reach its parent. Instant
detection requires abrupt stopping of the subordinate kernel which may be
undesirable for programmes with complicated logic.

#+name: fig:benchmark
#+begin_src R
# TODO
#+end_src

#+caption: Performance of hydrodynamics HPC application in the presence of node failures.
#+RESULTS: fig:benchmark

To summarise, the benchmark showed that /no matter a master or a slave node
fails, the resulting performance roughly equals to the one without failures with
the number of nodes minus one/, however, when a backup node fails performance
penalty is much higher.

#+name: fig:slowdown
#+begin_src R
# TODO
#+end_src

#+caption: Slowdown of the hydrodynamics HPC application in the presence of different types of node failures compared to execution without failures but with the number of nodes minus one.
#+RESULTS: fig:slowdown

**** Discussion.
Described algorithm guarantees to handle one failure per computational step,
more failures can be tolerated if they do not affect the master node. The system
handles simultaneous failure of all subordinate nodes, however, if both master
and backup nodes fail, there is no chance for an application to survive. In this
case the state of the current computation step is lost, and the only way to
restore it is to restart the application.

Computational kernels are means of abstraction that decouple distributed
application from physical hardware: it does not matter how many nodes are online
for an application to run successfully. Computational kernels eliminate the need
to allocate a physical backup node to make master node highly-available, with
computational kernels approach any node can act as a backup one. Finally,
computational kernels can handle subordinate node failures in a way that is
transparent to a programmer.

The disadvantage of this approach is evident: there is no way of making existing
middleware highly-available without rewriting their source code. Although, our
programming framework is lightweight, it is not easy to map architecture of
existing middleware systems to it: most systems are developed keeping in mind
static assignment of server/client roles, which is not easy to make dynamic.
Hopefully, our approach will simplify design of future middleware systems.

The benchmark from the previous section show that it is essential for a
parallel application to have multiple sequential steps to make it resilient to
cluster node failures. Although, the probability of a master node failure is
lower than the probability of failure of any of the slave nodes, it does not
justify loosing all the data when the programme run is near completion. In
general, the more sequential steps one has in an HPC application the less is
performance penalty in an event of master node failure, and the more parallel
parts each step has the less is performance penalty in case of a slave node
failure. In other words, /the more scalable an application is the more
resilient to node failures it becomes/.

In our experiments we specified manually where the programme starts its
execution to make mapping of hierarchy of computational kernels to tree
hierarchy of nodes optimal, however, it does not seem practical for real-world
cluster. The framework may perform such tasks automatically, and distribute the
load efficiently no matter whether the master node of the application is
located in the root or leaf of the tree hierarchy: Allocating the same node for
the first kernel of each application deteriorates fault-tolerance.

Although it may not be clear from the benchmarks, Factory does not only provide
tolerance to node failures: new nodes automatically join the cluster and
receive their portion of the load as soon as it is possible. This is trivial
process as it does not involve restarting failed kernels or managing their
state, so it is not presented in this work.

In theory, hierarchy-based fault-tolerance can be implemented on top of the
message-passing library without loss of generality. Although it would be
complicated to reuse free nodes instead of failed ones, as the number of nodes
is often fixed in such libraries, allocating reasonably large number of nodes
for the application would be enough to make it fault-tolerant. However,
implementing hierarchy-based fault-tolerance ``below'' message-passing
library does not seem beneficial, because it would require saving the state
of a parallel application which equals to the total amount of memory it
ccupies on each host, which would not make it more efficient than
checkpoints.

The weak point of the proposed technology is the length of the period of time
starting from a failure of master node up to the moment when the failure is
detected, the first kernel is restored and new subordinate kernel with the
parent's copy is received by a subordinate node. If during this period of time
backup node fails, execution state of application is completely lost, and there
is no way to recover it other than fully restarting the application. The length
of the dangerous period can be minimised but the possibility of a abrupt
programme stop can not be fully eliminated. This result is consistent with the
scrutiny of ``impossibility theory'', in the framework of which it is proved
the impossibility of the distributed consensus with one faulty
process cite:fischer1985impossibility and impossibility of reliable
communication in the presence of node failures cite:fekete1993impossibility.

* Conclusion
* Acknowledgements
The graphs in this work were prepared using R language for statistical
computing cite:rlang2016,Sarkar2008lattice and Graphviz software
cite:Gansner00anopen. The manuscript was prepared using Org-mode
cite:Schulte2011org2,Schulte2011org1,Dominik2010org for GNU Emacs which provides
computing environment for reproducible research. This means that all graphs can
be reproduced and corresponding statements verified by cloning thesis
repository[fn:repo], installing Emacs and exporting the document.

[fn:repo] [[https://github.com/igankevich/arma-thesis]]

* List of acronyms and symbols
- <<<MPP>>> :: Massively Parallel Processing, computers with distributed memory.
- <<<SMP>>> :: Symmetric Multi-Processing, computers with shared memory.
- <<<ACF>>> :: auto-covariate function.
- <<<FFT>>> :: fast Fourier transform.
- <<<PRNG>>> :: pseudo-random number generator.
- <<<BC>>> :: boundary condition.
- <<<PDE>>> :: partial differential equation.
- <<<NIT>>> :: non-linear inertia-less transform.
- <<<AR>>> :: auto-regressive process.
- <<<ARMA>>> :: auto-regressive moving-average process.
- <<<MA>>> :: moving average process.
- <<<LH>>> :: Longuet---Higgins model.
- <<<LAMP>>> :: Large Amplitude Motion Programme, a programme that simulates
                ship behaviour in ocean waves.
- <<<CLT>>> :: central limit theorem.
- <<<PM>>> :: Pierson---Moskowitz ocean wave spectrum approximation.
- <<<YW>>> :: Yule---Walker equations.
- <<<LS>>> :: least squares.
- <<<PDF>>> :: probability density function.
- <<<CDF>>> :: cumulative distribution function.
- <<<BSP>>> :: Bulk Synchronous Parallel.
- <<<OpenCL>>> :: Open Computing Language.
- <<<OpenMP>>> :: Open Multi-Processing.
- <<<MPI>>> :: Message Passing Interface.
- <<<POSIX>>> :: Portable Operating System.
- <<<FMA>>> :: Fused multiply-add.
- <<<DCMT>>> :: Dynamic creation of Mersenne Twisters.
- <<<GSL>>> :: GNU Scientific Library.
- <<<BLAS>>> :: Basic Linear Algebra Sub-programmes.
- <<<LAPACK>>> :: Linear Algebra Package.
- <<<DNS>>> :: Dynamic name resolution.
- <<<HPC>>> ::  High-performance computing.

#+begin_export latex
\input{postamble}
#+end_export

bibliographystyle:ugost2008
bibliography:bib/refs.bib

* Appendix
