# Local Variables:
# org-ref-default-bibliography ("bib/refs.bib")
# org-latex-image-default-width nil
# org-latex-caption-above nil
# org-latex-hyperref-template "\\hypersetup{\n pdfauthor={%a},\n pdftitle={%t},\n pdfkeywords={%k},\n pdfsubject={%d},\n pdfcreator={%c},\n pdflang={%L},\n unicode={true}\n}\n\\setdefaultlanguage{%l}\n"
# org-export-latex-tables-hline "\\midrule"
# org-export-latex-tables-tstart "\\toprule"
# org-export-latex-tables-tend "\\bottomrule"
# eval: (add-to-list 'org-latex-classes '("gost" "\\documentclass{gost} [DEFAULT-PACKAGES] [PACKAGES] [EXTRA]" ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}") ("\\subsubsection{%s}" . "\\subsubsection*{%s}") ("\\paragraph{%s}" . "\\paragraph*{%s}") ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# End:

#+TITLE: High-performance sea wave simulation model for studying marine object behaviour
#+AUTHOR: Ivan Gankevich
#+DATE: St. Petersburg, 2017
#+LANGUAGE: en
#+LATEX_CLASS: gost
#+LATEX_CLASS_OPTIONS: [hidelinks,fontsize=14pt,paper=a4,pagesize,DIV=calc,noenddot]
#+LATEX_HEADER_EXTRA: \input{preamble}
#+LATEX_HEADER_EXTRA: \organization{Saint Petersburg State University}
#+LATEX_HEADER_EXTRA: \manuscript{}
#+LATEX_HEADER_EXTRA: \degree{thesis for candidate of sciences degree}
#+LATEX_HEADER_EXTRA: \speciality{Speciality 05.13.18\\Mathematical modeling, numerical methods and programme complexes}
#+LATEX_HEADER_EXTRA: \supervisor{Supervisor\\Alexander Degtyarev}
#+LATEX_HEADER_EXTRA: \newcites{published}{Publications on the subject of thesis}
#+OPTIONS: todo:nil title:nil ':t H:5
#+STARTUP: indent
#+PROPERTY: header-args:R :results graphics :exports results

* Config                                                           :noexport:
** Produce data for Q-Q, ACF and velocity potential plots
#+begin_src sh :exports none :results verbatim
root=$(pwd)
for testname in \
    propagating_wave \
    standing_wave \
    plain_wave_linear_solver \
    plain_wave_high_amplitude_solver
do
    wd=$root/build/$testname
    rm -rf $wd
    mkdir -p $wd
    cd $wd
    cp $root/config/mt.dat .
    arma -c $root/config/$testname.arma 2>&1
done
#+end_src

#+RESULTS:
#+begin_example
Input file                     = /home/igankevich/workspace/arma-thesis/config/propagating_wave.arma
Output grid size               = (200,40,40)
Output grid patch size         = (1,1,1)
Model                          = MA
Verification scheme            = manual
MA model                       = order=(20,10,10),acf.shape=(20,10,10),algorithm=fixed_point_iteration
ACF variance = 5
fixed_point_iteration:Iteration=0, var_wn=2.70831
fixed_point_iteration:Iteration=1, var_wn=1.93791
fixed_point_iteration:Iteration=2, var_wn=1.54801
fixed_point_iteration:Iteration=3, var_wn=1.31202
fixed_point_iteration:Iteration=4, var_wn=1.15328
fixed_point_iteration:Iteration=5, var_wn=1.0386
fixed_point_iteration:Iteration=6, var_wn=0.951442
fixed_point_iteration:Iteration=7, var_wn=0.882674
fixed_point_iteration:Iteration=8, var_wn=0.82688
fixed_point_iteration:Iteration=9, var_wn=0.780623
fixed_point_iteration:Iteration=10, var_wn=0.74161
fixed_point_iteration:Iteration=11, var_wn=0.708244
fixed_point_iteration:Iteration=12, var_wn=0.679374
fixed_point_iteration:Iteration=13, var_wn=0.654145
fixed_point_iteration:Iteration=14, var_wn=0.63191
fixed_point_iteration:Iteration=15, var_wn=0.612168
fixed_point_iteration:Iteration=16, var_wn=0.594523
fixed_point_iteration:Iteration=17, var_wn=0.578663
fixed_point_iteration:Iteration=18, var_wn=0.564333
fixed_point_iteration:Iteration=19, var_wn=0.551325
fixed_point_iteration:Iteration=20, var_wn=0.539469
fixed_point_iteration:Iteration=21, var_wn=0.528623
fixed_point_iteration:Iteration=22, var_wn=0.518666
fixed_point_iteration:Iteration=23, var_wn=0.509497
fixed_point_iteration:Iteration=24, var_wn=0.50103
fixed_point_iteration:Iteration=25, var_wn=0.493191
fixed_point_iteration:Iteration=26, var_wn=0.485915
fixed_point_iteration:Iteration=27, var_wn=0.479148
fixed_point_iteration:Iteration=28, var_wn=0.472841
fixed_point_iteration:Iteration=29, var_wn=0.466951
fixed_point_iteration:Iteration=30, var_wn=0.461442
fixed_point_iteration:Iteration=31, var_wn=0.456279
fixed_point_iteration:Iteration=32, var_wn=0.451435
fixed_point_iteration:Iteration=33, var_wn=0.446882
fixed_point_iteration:Iteration=34, var_wn=0.442597
fixed_point_iteration:Iteration=35, var_wn=0.43856
fixed_point_iteration:Iteration=36, var_wn=0.434752
fixed_point_iteration:Iteration=37, var_wn=0.431155
fixed_point_iteration:Iteration=38, var_wn=0.427755
fixed_point_iteration:Iteration=39, var_wn=0.424537
fixed_point_iteration:Iteration=40, var_wn=0.42149
fixed_point_iteration:Iteration=41, var_wn=0.4186
fixed_point_iteration:Iteration=42, var_wn=0.415859
fixed_point_iteration:Iteration=43, var_wn=0.413256
fixed_point_iteration:Iteration=44, var_wn=0.410782
fixed_point_iteration:Iteration=45, var_wn=0.408429
fixed_point_iteration:Iteration=46, var_wn=0.406191
fixed_point_iteration:Iteration=47, var_wn=0.404059
fixed_point_iteration:Iteration=48, var_wn=0.402028
fixed_point_iteration:Iteration=49, var_wn=0.400092
fixed_point_iteration:Iteration=50, var_wn=0.398246
fixed_point_iteration:Iteration=51, var_wn=0.396483
fixed_point_iteration:Iteration=52, var_wn=0.3948
fixed_point_iteration:Iteration=53, var_wn=0.393193
fixed_point_iteration:Iteration=54, var_wn=0.391656
fixed_point_iteration:Iteration=55, var_wn=0.390187
fixed_point_iteration:Iteration=56, var_wn=0.388782
fixed_point_iteration:Iteration=57, var_wn=0.387438
fixed_point_iteration:Iteration=58, var_wn=0.386151
fixed_point_iteration:Iteration=59, var_wn=0.384918
fixed_point_iteration:Iteration=60, var_wn=0.383738
fixed_point_iteration:Iteration=61, var_wn=0.382606
fixed_point_iteration:Iteration=62, var_wn=0.381522
fixed_point_iteration:Iteration=63, var_wn=0.380482
fixed_point_iteration:Iteration=64, var_wn=0.379485
fixed_point_iteration:Iteration=65, var_wn=0.378528
fixed_point_iteration:Iteration=66, var_wn=0.37761
fixed_point_iteration:Iteration=67, var_wn=0.376728
fixed_point_iteration:Iteration=68, var_wn=0.375882
fixed_point_iteration:Iteration=69, var_wn=0.37507
fixed_point_iteration:Iteration=70, var_wn=0.374289
fixed_point_iteration:Iteration=71, var_wn=0.373539
fixed_point_iteration:Iteration=72, var_wn=0.372818
fixed_point_iteration:Iteration=73, var_wn=0.372126
fixed_point_iteration:Iteration=74, var_wn=0.37146
fixed_point_iteration:Iteration=75, var_wn=0.370819
fixed_point_iteration:Iteration=76, var_wn=0.370204
fixed_point_iteration:Iteration=77, var_wn=0.369611
fixed_point_iteration:Iteration=78, var_wn=0.369042
fixed_point_iteration:Iteration=79, var_wn=0.368493
fixed_point_iteration:Iteration=80, var_wn=0.367966
fixed_point_iteration:Iteration=81, var_wn=0.367458
fixed_point_iteration:Iteration=82, var_wn=0.366969
fixed_point_iteration:Iteration=83, var_wn=0.366499
fixed_point_iteration:Iteration=84, var_wn=0.366046
fixed_point_iteration:Iteration=85, var_wn=0.365609
fixed_point_iteration:Iteration=86, var_wn=0.365189
fixed_point_iteration:Iteration=87, var_wn=0.364785
fixed_point_iteration:Iteration=88, var_wn=0.364395
fixed_point_iteration:Iteration=89, var_wn=0.364019
fixed_point_iteration:Iteration=90, var_wn=0.363657
fixed_point_iteration:Iteration=91, var_wn=0.363309
fixed_point_iteration:Iteration=92, var_wn=0.362973
fixed_point_iteration:Iteration=93, var_wn=0.362649
fixed_point_iteration:Iteration=94, var_wn=0.362337
fixed_point_iteration:Iteration=95, var_wn=0.362036
fixed_point_iteration:Iteration=96, var_wn=0.361746
fixed_point_iteration:Iteration=97, var_wn=0.361466
fixed_point_iteration:Iteration=98, var_wn=0.361196
fixed_point_iteration:Iteration=99, var_wn=0.360936
fixed_point_iteration:Iteration=100, var_wn=0.360686
fixed_point_iteration:Iteration=101, var_wn=0.360444
fixed_point_iteration:Iteration=102, var_wn=0.360211
fixed_point_iteration:Iteration=103, var_wn=0.359986
fixed_point_iteration:Iteration=104, var_wn=0.359769
fixed_point_iteration:Iteration=105, var_wn=0.35956
fixed_point_iteration:Iteration=106, var_wn=0.359358
fixed_point_iteration:Iteration=107, var_wn=0.359163
fixed_point_iteration:Iteration=108, var_wn=0.358975
fixed_point_iteration:Iteration=109, var_wn=0.358794
fixed_point_iteration:Iteration=110, var_wn=0.358619
fixed_point_iteration:Iteration=111, var_wn=0.35845
fixed_point_iteration:Iteration=112, var_wn=0.358287
fixed_point_iteration:Iteration=113, var_wn=0.35813
fixed_point_iteration:Iteration=114, var_wn=0.357979
fixed_point_iteration:Iteration=115, var_wn=0.357832
fixed_point_iteration:Iteration=116, var_wn=0.357691
fixed_point_iteration:Iteration=117, var_wn=0.357555
fixed_point_iteration:Iteration=118, var_wn=0.357423
fixed_point_iteration:Iteration=119, var_wn=0.357296
fixed_point_iteration:Iteration=120, var_wn=0.357173
fixed_point_iteration:Iteration=121, var_wn=0.357055
fixed_point_iteration:Iteration=122, var_wn=0.356941
fixed_point_iteration:Iteration=123, var_wn=0.35683
fixed_point_iteration:Iteration=124, var_wn=0.356724
fixed_point_iteration:Iteration=125, var_wn=0.356621
fixed_point_iteration:Iteration=126, var_wn=0.356522
fixed_point_iteration:Iteration=127, var_wn=0.356426
fixed_point_iteration:Iteration=128, var_wn=0.356334
fixed_point_iteration:Iteration=129, var_wn=0.356244
fixed_point_iteration:Iteration=130, var_wn=0.356158
fixed_point_iteration:Iteration=131, var_wn=0.356075
fixed_point_iteration:Iteration=132, var_wn=0.355994
fixed_point_iteration:Iteration=133, var_wn=0.355917
fixed_point_iteration:Iteration=134, var_wn=0.355842
fixed_point_iteration:Iteration=135, var_wn=0.355769
fixed_point_iteration:Iteration=136, var_wn=0.355699
fixed_point_iteration:Iteration=137, var_wn=0.355632
fixed_point_iteration:Iteration=138, var_wn=0.355566
fixed_point_iteration:Iteration=139, var_wn=0.355504
fixed_point_iteration:Iteration=140, var_wn=0.355443
fixed_point_iteration:Iteration=141, var_wn=0.355384
fixed_point_iteration:Iteration=142, var_wn=0.355327
fixed_point_iteration:Iteration=143, var_wn=0.355272
fixed_point_iteration:Iteration=144, var_wn=0.35522
fixed_point_iteration:Iteration=145, var_wn=0.355168
fixed_point_iteration:Iteration=146, var_wn=0.355119
fixed_point_iteration:Iteration=147, var_wn=0.355071
fixed_point_iteration:Iteration=148, var_wn=0.355025
fixed_point_iteration:Iteration=149, var_wn=0.354981
fixed_point_iteration:Iteration=150, var_wn=0.354938
fixed_point_iteration:Iteration=151, var_wn=0.354896
fixed_point_iteration:Iteration=152, var_wn=0.354856
fixed_point_iteration:Iteration=153, var_wn=0.354818
fixed_point_iteration:Iteration=154, var_wn=0.35478
fixed_point_iteration:Iteration=155, var_wn=0.354744
fixed_point_iteration:Iteration=156, var_wn=0.354709
fixed_point_iteration:Iteration=157, var_wn=0.354676
fixed_point_iteration:Iteration=158, var_wn=0.354643
fixed_point_iteration:Iteration=159, var_wn=0.354612
fixed_point_iteration:Iteration=160, var_wn=0.354581
fixed_point_iteration:Iteration=161, var_wn=0.354552
fixed_point_iteration:Iteration=162, var_wn=0.354523
fixed_point_iteration:Iteration=163, var_wn=0.354496
fixed_point_iteration:Iteration=164, var_wn=0.35447
fixed_point_iteration:Iteration=165, var_wn=0.354444
fixed_point_iteration:Iteration=166, var_wn=0.354419
fixed_point_iteration:Iteration=167, var_wn=0.354396
fixed_point_iteration:Iteration=168, var_wn=0.354373
fixed_point_iteration:Iteration=169, var_wn=0.35435
fixed_point_iteration:Iteration=170, var_wn=0.354329
fixed_point_iteration:Iteration=171, var_wn=0.354308
fixed_point_iteration:Iteration=172, var_wn=0.354288
fixed_point_iteration:Iteration=173, var_wn=0.354269
fixed_point_iteration:Iteration=174, var_wn=0.35425
fixed_point_iteration:Iteration=175, var_wn=0.354232
fixed_point_iteration:Iteration=176, var_wn=0.354214
fixed_point_iteration:Iteration=177, var_wn=0.354197
fixed_point_iteration:Iteration=178, var_wn=0.354181
fixed_point_iteration:Iteration=179, var_wn=0.354165
fixed_point_iteration:Iteration=180, var_wn=0.35415
fixed_point_iteration:Iteration=181, var_wn=0.354135
fixed_point_iteration:Iteration=182, var_wn=0.354121
fixed_point_iteration:Iteration=183, var_wn=0.354107
fixed_point_iteration:Iteration=184, var_wn=0.354094
fixed_point_iteration:Iteration=185, var_wn=0.354081
fixed_point_iteration:Iteration=186, var_wn=0.354069
fixed_point_iteration:Iteration=187, var_wn=0.354057
fixed_point_iteration:Iteration=188, var_wn=0.354046
fixed_point_iteration:Iteration=189, var_wn=0.354034
fixed_point_iteration:Iteration=190, var_wn=0.354024
fixed_point_iteration:Iteration=191, var_wn=0.354013
fixed_point_iteration:Iteration=192, var_wn=0.354003
fixed_point_iteration:Iteration=193, var_wn=0.353993
WN variance = 0.353993
Partition size                 = (34,13,13)
Finished part [1/96]
Finished part [2/96]
Finished part [3/96]
Finished part [4/96]
Finished part [5/96]
Finished part [6/96]
Finished part [7/96]
Finished part [8/96]
Finished part [9/96]
Finished part [10/96]
Finished part [11/96]
Finished part [12/96]
Finished part [13/96]
Finished part [14/96]
Finished part [15/96]
Finished part [16/96]
Finished part [17/96]
Finished part [18/96]
Finished part [19/96]
Finished part [20/96]
Finished part [21/96]
Finished part [22/96]
Finished part [23/96]
Finished part [24/96]
Finished part [25/96]
Finished part [26/96]
Finished part [27/96]
Finished part [28/96]
Finished part [29/96]
Finished part [30/96]
Finished part [31/96]
Finished part [32/96]
Finished part [33/96]
Finished part [34/96]
Finished part [35/96]
Finished part [36/96]
Finished part [37/96]
Finished part [38/96]
Finished part [39/96]
Finished part [40/96]
Finished part [41/96]
Finished part [42/96]
Finished part [43/96]
Finished part [44/96]
Finished part [45/96]
Finished part [46/96]
Finished part [47/96]
Finished part [48/96]
Finished part [49/96]
Finished part [50/96]
Finished part [51/96]
Finished part [52/96]
Finished part [53/96]
Finished part [54/96]
Finished part [55/96]
Finished part [56/96]
Finished part [57/96]
Finished part [58/96]
Finished part [59/96]
Finished part [60/96]
Finished part [61/96]
Finished part [62/96]
Finished part [63/96]
Finished part [64/96]
Finished part [65/96]
Finished part [66/96]
Finished part [67/96]
Finished part [68/96]
Finished part [69/96]
Finished part [70/96]
Finished part [71/96]
Finished part [72/96]
Finished part [73/96]
Finished part [74/96]
Finished part [75/96]
Finished part [76/96]
Finished part [77/96]
Finished part [78/96]
Finished part [79/96]
Finished part [80/96]
Finished part [81/96]
Finished part [82/96]
Finished part [83/96]
Finished part [84/96]
Finished part [85/96]
Finished part [86/96]
Finished part [87/96]
Finished part [88/96]
Finished part [89/96]
Finished part [90/96]
Finished part [91/96]
Finished part [92/96]
Finished part [93/96]
Finished part [94/96]
Finished part [95/96]
Finished part [96/96]
Input file                     = /home/igankevich/workspace/arma-thesis/config/standing_wave.arma
Output grid size               = (200,40,40)
Output grid patch size         = (1,1,1)
Model                          = AR
Verification scheme            = manual
AR model                       = order=(7,7,7),acf.shape=(10,10,10)
ACF variance = 5
WN variance = 0.00259511
Partition size                 = (21,10,10)
Finished part [1/160]
Finished part [2/160]
Finished part [3/160]
Finished part [4/160]
Finished part [5/160]
Finished part [6/160]
Finished part [7/160]
Finished part [8/160]
Finished part [9/160]
Finished part [10/160]
Finished part [11/160]
Finished part [12/160]
Finished part [13/160]
Finished part [14/160]
Finished part [15/160]
Finished part [16/160]
Finished part [17/160]
Finished part [18/160]
Finished part [19/160]
Finished part [20/160]
Finished part [21/160]
Finished part [22/160]
Finished part [23/160]
Finished part [24/160]
Finished part [25/160]
Finished part [26/160]
Finished part [27/160]
Finished part [28/160]
Finished part [29/160]
Finished part [30/160]
Finished part [31/160]
Finished part [32/160]
Finished part [33/160]
Finished part [34/160]
Finished part [35/160]
Finished part [36/160]
Finished part [37/160]
Finished part [38/160]
Finished part [39/160]
Finished part [40/160]
Finished part [41/160]
Finished part [42/160]
Finished part [43/160]
Finished part [44/160]
Finished part [45/160]
Finished part [46/160]
Finished part [47/160]
Finished part [48/160]
Finished part [49/160]
Finished part [50/160]
Finished part [51/160]
Finished part [52/160]
Finished part [53/160]
Finished part [54/160]
Finished part [55/160]
Finished part [56/160]
Finished part [57/160]
Finished part [58/160]
Finished part [59/160]
Finished part [60/160]
Finished part [61/160]
Finished part [62/160]
Finished part [63/160]
Finished part [64/160]
Finished part [65/160]
Finished part [66/160]
Finished part [67/160]
Finished part [68/160]
Finished part [69/160]
Finished part [70/160]
Finished part [71/160]
Finished part [72/160]
Finished part [73/160]
Finished part [74/160]
Finished part [75/160]
Finished part [76/160]
Finished part [77/160]
Finished part [78/160]
Finished part [79/160]
Finished part [80/160]
Finished part [81/160]
Finished part [82/160]
Finished part [83/160]
Finished part [84/160]
Finished part [85/160]
Finished part [86/160]
Finished part [87/160]
Finished part [88/160]
Finished part [89/160]
Finished part [90/160]
Finished part [91/160]
Finished part [92/160]
Finished part [93/160]
Finished part [94/160]
Finished part [95/160]
Finished part [96/160]
Finished part [97/160]
Finished part [98/160]
Finished part [99/160]
Finished part [100/160]
Finished part [101/160]
Finished part [102/160]
Finished part [103/160]
Finished part [104/160]
Finished part [105/160]
Finished part [106/160]
Finished part [107/160]
Finished part [108/160]
Finished part [109/160]
Finished part [110/160]
Finished part [111/160]
Finished part [112/160]
Finished part [113/160]
Finished part [114/160]
Finished part [115/160]
Finished part [116/160]
Finished part [117/160]
Finished part [118/160]
Finished part [119/160]
Finished part [120/160]
Finished part [121/160]
Finished part [122/160]
Finished part [123/160]
Finished part [124/160]
Finished part [125/160]
Finished part [126/160]
Finished part [127/160]
Finished part [128/160]
Finished part [129/160]
Finished part [130/160]
Finished part [131/160]
Finished part [132/160]
Finished part [133/160]
Finished part [134/160]
Finished part [135/160]
Finished part [136/160]
Finished part [137/160]
Finished part [138/160]
Finished part [139/160]
Finished part [140/160]
Finished part [141/160]
Finished part [142/160]
Finished part [143/160]
Finished part [144/160]
Finished part [145/160]
Finished part [146/160]
Finished part [147/160]
Finished part [148/160]
Finished part [149/160]
Finished part [150/160]
Finished part [151/160]
Finished part [152/160]
Finished part [153/160]
Finished part [154/160]
Finished part [155/160]
Finished part [156/160]
Finished part [157/160]
Finished part [158/160]
Finished part [159/160]
Finished part [160/160]
Zeta size = (192,32,32)
Input file                     = /home/igankevich/workspace/arma-thesis/config/plain_wave_linear_solver.arma
Output grid size               = (200,128,40)
Output grid patch size         = (1,0.0629921,0.205128)
Model                          = plain_wave
Verification scheme            = manual
Plain wave model               = func=cos,amplitudes=[4],wavenumbers=[0.125],phases=[0],velocities=[0.5]
Velocity potential solver name = N4arma8velocity13Linear_solverIdEE
Velocity potential solver      = wnmax=(0,0.25),depth=12,domain=from (10,-12) to (10,4) npoints (1,128)
Input file                     = /home/igankevich/workspace/arma-thesis/config/plain_wave_high_amplitude_solver.arma
Output grid size               = (200,128,40)
Output grid patch size         = (1,0.0629921,0.205128)
Model                          = plain_wave
Verification scheme            = manual
Plain wave model               = func=cos,amplitudes=[4],wavenumbers=[0.125],phases=[0],velocities=[0.5]
Velocity potential solver name = N4arma8velocity21High_amplitude_solverIdEE
Velocity potential solver      = wnmax=(0,0.25),depth=12,domain=from (10,-12) to (10,4) npoints (1,128)
#+end_example
** Produce NIT plots
#+begin_src sh :exports none :results verbatim
root=$(pwd)

function generate_surface() {
  none=$1
  gcs=$2
  sn=$3
  testcase=$4

  wd=$root/build/$testcase
  rm -rf $wd
  mkdir -p $wd
  cd $wd
  cp $root/config/mt.dat .

  # run linear case
  arma -c $root/config/$none 2>&1
  cp -v zeta.csv zeta-none.csv

  # run Gram---Charlier case
  arma -c $root/config/$gcs 2>&1
  cp -v zeta.csv zeta-gramcharlier.csv

  # run skew normal case
  arma -c $root/config/$sn 2>&1
  cp -v zeta.csv zeta-skewnormal.csv
}

echo "NIT for propagating waves"
generate_surface \
  nit-propagating-none \
  nit-propagating-gramcharlier \
  nit-propagating-skewnormal \
  nit-propagating

echo "NIT for standing waves"
generate_surface \
  nit-standing-none \
  nit-standing-gramcharlier \
  nit-standing-skewnormal \
  nit-standing
#+end_src

#+RESULTS:
#+begin_example
NIT for propagating waves
Input file                     = /home/igankevich/workspace/arma-thesis/config/nit-propagating-none
Output grid size               = (200,40,40)
Output grid patch size         = (1,1,1)
Model                          = MA
Verification scheme            = manual
MA model                       = order=(20,10,10),acf.shape=(20,10,10),algorithm=fixed_point_iteration
Velocity potential solver name = N4arma8velocity21High_amplitude_solverIdEE
Velocity potential solver      = wnmax=from (0,0) to (0,0.25) npoints (2,2),depth=12,domain=from (10,-12) to (10,3) npoints (1,128)
NIT transform                  = none
ACF variance = 1
fixed_point_iteration:Iteration=0, var_wn=0.541662
fixed_point_iteration:Iteration=1, var_wn=0.387581
fixed_point_iteration:Iteration=2, var_wn=0.309602
fixed_point_iteration:Iteration=3, var_wn=0.262404
fixed_point_iteration:Iteration=4, var_wn=0.230656
fixed_point_iteration:Iteration=5, var_wn=0.207721
fixed_point_iteration:Iteration=6, var_wn=0.190288
fixed_point_iteration:Iteration=7, var_wn=0.176535
fixed_point_iteration:Iteration=8, var_wn=0.165376
fixed_point_iteration:Iteration=9, var_wn=0.156125
fixed_point_iteration:Iteration=10, var_wn=0.148322
fixed_point_iteration:Iteration=11, var_wn=0.141649
fixed_point_iteration:Iteration=12, var_wn=0.135875
fixed_point_iteration:Iteration=13, var_wn=0.130829
fixed_point_iteration:Iteration=14, var_wn=0.126382
fixed_point_iteration:Iteration=15, var_wn=0.122434
fixed_point_iteration:Iteration=16, var_wn=0.118905
fixed_point_iteration:Iteration=17, var_wn=0.115733
fixed_point_iteration:Iteration=18, var_wn=0.112867
fixed_point_iteration:Iteration=19, var_wn=0.110265
fixed_point_iteration:Iteration=20, var_wn=0.107894
fixed_point_iteration:Iteration=21, var_wn=0.105725
fixed_point_iteration:Iteration=22, var_wn=0.103733
fixed_point_iteration:Iteration=23, var_wn=0.101899
fixed_point_iteration:Iteration=24, var_wn=0.100206
fixed_point_iteration:Iteration=25, var_wn=0.0986382
fixed_point_iteration:Iteration=26, var_wn=0.0971831
fixed_point_iteration:Iteration=27, var_wn=0.0958297
fixed_point_iteration:Iteration=28, var_wn=0.0945682
fixed_point_iteration:Iteration=29, var_wn=0.0933903
fixed_point_iteration:Iteration=30, var_wn=0.0922883
fixed_point_iteration:Iteration=31, var_wn=0.0912558
fixed_point_iteration:Iteration=32, var_wn=0.0902869
fixed_point_iteration:Iteration=33, var_wn=0.0893763
fixed_point_iteration:Iteration=34, var_wn=0.0885194
fixed_point_iteration:Iteration=35, var_wn=0.087712
fixed_point_iteration:Iteration=36, var_wn=0.0869503
fixed_point_iteration:Iteration=37, var_wn=0.086231
fixed_point_iteration:Iteration=38, var_wn=0.085551
fixed_point_iteration:Iteration=39, var_wn=0.0849075
fixed_point_iteration:Iteration=40, var_wn=0.0842979
fixed_point_iteration:Iteration=41, var_wn=0.0837201
fixed_point_iteration:Iteration=42, var_wn=0.0831718
fixed_point_iteration:Iteration=43, var_wn=0.0826511
fixed_point_iteration:Iteration=44, var_wn=0.0821564
fixed_point_iteration:Iteration=45, var_wn=0.0816859
fixed_point_iteration:Iteration=46, var_wn=0.0812382
fixed_point_iteration:Iteration=47, var_wn=0.0808119
fixed_point_iteration:Iteration=48, var_wn=0.0804057
fixed_point_iteration:Iteration=49, var_wn=0.0800185
fixed_point_iteration:Iteration=50, var_wn=0.0796491
fixed_point_iteration:Iteration=51, var_wn=0.0792966
fixed_point_iteration:Iteration=52, var_wn=0.07896
fixed_point_iteration:Iteration=53, var_wn=0.0786385
fixed_point_iteration:Iteration=54, var_wn=0.0783313
fixed_point_iteration:Iteration=55, var_wn=0.0780375
fixed_point_iteration:Iteration=56, var_wn=0.0777565
fixed_point_iteration:Iteration=57, var_wn=0.0774875
fixed_point_iteration:Iteration=58, var_wn=0.0772301
fixed_point_iteration:Iteration=59, var_wn=0.0769836
fixed_point_iteration:Iteration=60, var_wn=0.0767475
fixed_point_iteration:Iteration=61, var_wn=0.0765213
fixed_point_iteration:Iteration=62, var_wn=0.0763044
fixed_point_iteration:Iteration=63, var_wn=0.0760964
fixed_point_iteration:Iteration=64, var_wn=0.0758969
fixed_point_iteration:Iteration=65, var_wn=0.0757056
fixed_point_iteration:Iteration=66, var_wn=0.075522
fixed_point_iteration:Iteration=67, var_wn=0.0753457
fixed_point_iteration:Iteration=68, var_wn=0.0751764
fixed_point_iteration:Iteration=69, var_wn=0.0750139
fixed_point_iteration:Iteration=70, var_wn=0.0748578
fixed_point_iteration:Iteration=71, var_wn=0.0747078
fixed_point_iteration:Iteration=72, var_wn=0.0745637
fixed_point_iteration:Iteration=73, var_wn=0.0744251
fixed_point_iteration:Iteration=74, var_wn=0.0742919
fixed_point_iteration:Iteration=75, var_wn=0.0741639
fixed_point_iteration:Iteration=76, var_wn=0.0740407
fixed_point_iteration:Iteration=77, var_wn=0.0739223
fixed_point_iteration:Iteration=78, var_wn=0.0738083
fixed_point_iteration:Iteration=79, var_wn=0.0736987
fixed_point_iteration:Iteration=80, var_wn=0.0735932
fixed_point_iteration:Iteration=81, var_wn=0.0734916
fixed_point_iteration:Iteration=82, var_wn=0.0733939
fixed_point_iteration:Iteration=83, var_wn=0.0732998
fixed_point_iteration:Iteration=84, var_wn=0.0732091
fixed_point_iteration:Iteration=85, var_wn=0.0731219
fixed_point_iteration:Iteration=86, var_wn=0.0730379
fixed_point_iteration:Iteration=87, var_wn=0.0729569
fixed_point_iteration:Iteration=88, var_wn=0.072879
fixed_point_iteration:Iteration=89, var_wn=0.0728038
fixed_point_iteration:Iteration=90, var_wn=0.0727315
fixed_point_iteration:Iteration=91, var_wn=0.0726617
fixed_point_iteration:Iteration=92, var_wn=0.0725945
fixed_point_iteration:Iteration=93, var_wn=0.0725298
fixed_point_iteration:Iteration=94, var_wn=0.0724673
fixed_point_iteration:Iteration=95, var_wn=0.0724072
fixed_point_iteration:Iteration=96, var_wn=0.0723491
fixed_point_iteration:Iteration=97, var_wn=0.0722932
fixed_point_iteration:Iteration=98, var_wn=0.0722393
fixed_point_iteration:Iteration=99, var_wn=0.0721873
fixed_point_iteration:Iteration=100, var_wn=0.0721372
fixed_point_iteration:Iteration=101, var_wn=0.0720888
fixed_point_iteration:Iteration=102, var_wn=0.0720422
fixed_point_iteration:Iteration=103, var_wn=0.0719972
fixed_point_iteration:Iteration=104, var_wn=0.0719538
fixed_point_iteration:Iteration=105, var_wn=0.0719119
fixed_point_iteration:Iteration=106, var_wn=0.0718716
fixed_point_iteration:Iteration=107, var_wn=0.0718326
fixed_point_iteration:Iteration=108, var_wn=0.0717951
fixed_point_iteration:Iteration=109, var_wn=0.0717588
fixed_point_iteration:Iteration=110, var_wn=0.0717238
fixed_point_iteration:Iteration=111, var_wn=0.0716901
fixed_point_iteration:Iteration=112, var_wn=0.0716575
fixed_point_iteration:Iteration=113, var_wn=0.0716261
fixed_point_iteration:Iteration=114, var_wn=0.0715957
fixed_point_iteration:Iteration=115, var_wn=0.0715664
fixed_point_iteration:Iteration=116, var_wn=0.0715382
fixed_point_iteration:Iteration=117, var_wn=0.0715109
fixed_point_iteration:Iteration=118, var_wn=0.0714846
fixed_point_iteration:Iteration=119, var_wn=0.0714592
fixed_point_iteration:Iteration=120, var_wn=0.0714347
fixed_point_iteration:Iteration=121, var_wn=0.071411
fixed_point_iteration:Iteration=122, var_wn=0.0713881
fixed_point_iteration:Iteration=123, var_wn=0.0713661
fixed_point_iteration:Iteration=124, var_wn=0.0713448
fixed_point_iteration:Iteration=125, var_wn=0.0713242
fixed_point_iteration:Iteration=126, var_wn=0.0713044
fixed_point_iteration:Iteration=127, var_wn=0.0712852
fixed_point_iteration:Iteration=128, var_wn=0.0712667
fixed_point_iteration:Iteration=129, var_wn=0.0712488
fixed_point_iteration:Iteration=130, var_wn=0.0712316
fixed_point_iteration:Iteration=131, var_wn=0.0712149
fixed_point_iteration:Iteration=132, var_wn=0.0711988
fixed_point_iteration:Iteration=133, var_wn=0.0711833
fixed_point_iteration:Iteration=134, var_wn=0.0711683
fixed_point_iteration:Iteration=135, var_wn=0.0711538
fixed_point_iteration:Iteration=136, var_wn=0.0711398
fixed_point_iteration:Iteration=137, var_wn=0.0711263
fixed_point_iteration:Iteration=138, var_wn=0.0711133
fixed_point_iteration:Iteration=139, var_wn=0.0711007
fixed_point_iteration:Iteration=140, var_wn=0.0710885
fixed_point_iteration:Iteration=141, var_wn=0.0710768
fixed_point_iteration:Iteration=142, var_wn=0.0710654
fixed_point_iteration:Iteration=143, var_wn=0.0710545
fixed_point_iteration:Iteration=144, var_wn=0.0710439
fixed_point_iteration:Iteration=145, var_wn=0.0710337
fixed_point_iteration:Iteration=146, var_wn=0.0710238
WN variance = 0.0710238
Partition size                 = (34,13,13)
Finished part [1/96]Finished part [2/96]Finished part [3/96]Finished part [4/96]Finished part [5/96]Finished part [6/96]Finished part [7/96]Finished part [8/96]Finished part [9/96]Finished part [10/96]Finished part [11/96]Finished part [12/96]Finished part [13/96]Finished part [14/96]Finished part [15/96]Finished part [16/96]Finished part [17/96]Finished part [18/96]Finished part [19/96]Finished part [20/96]Finished part [21/96]Finished part [22/96]Finished part [23/96]Finished part [24/96]Finished part [25/96]Finished part [26/96]Finished part [27/96]Finished part [28/96]Finished part [29/96]Finished part [30/96]Finished part [31/96]Finished part [32/96]Finished part [33/96]Finished part [34/96]Finished part [35/96]Finished part [36/96]Finished part [37/96]Finished part [38/96]Finished part [39/96]Finished part [40/96]Finished part [41/96]Finished part [42/96]Finished part [43/96]Finished part [44/96]Finished part [45/96]Finished part [46/96]Finished part [47/96]Finished part [48/96]Finished part [49/96]Finished part [50/96]Finished part [51/96]Finished part [52/96]Finished part [53/96]Finished part [54/96]Finished part [55/96]Finished part [56/96]Finished part [57/96]Finished part [58/96]Finished part [59/96]Finished part [60/96]Finished part [61/96]Finished part [62/96]Finished part [63/96]Finished part [64/96]Finished part [65/96]Finished part [66/96]Finished part [67/96]Finished part [68/96]Finished part [69/96]Finished part [70/96]Finished part [71/96]Finished part [72/96]Finished part [73/96]Finished part [74/96]Finished part [75/96]Finished part [76/96]Finished part [77/96]Finished part [78/96]Finished part [79/96]Finished part [80/96]Finished part [81/96]Finished part [82/96]Finished part [83/96]Finished part [84/96]Finished part [85/96]Finished part [86/96]Finished part [87/96]Finished part [88/96]Finished part [89/96]Finished part [90/96]Finished part [91/96]Finished part [92/96]Finished part [93/96]Finished part [94/96]Finished part [95/96]Finished part [96/96]
'zeta.csv' -> 'zeta-none.csv'
Input file                     = /home/igankevich/workspace/arma-thesis/config/nit-propagating-gramcharlier
Output grid size               = (200,40,40)
Output grid patch size         = (1,1,1)
Model                          = MA
Verification scheme            = manual
MA model                       = order=(20,10,10),acf.shape=(20,10,10),algorithm=fixed_point_iteration
Velocity potential solver name = N4arma8velocity21High_amplitude_solverIdEE
Velocity potential solver      = wnmax=from (0,0) to (0,0.25) npoints (2,2),depth=12,domain=from (10,-12) to (10,3) npoints (1,128)
NIT transform                  = dist=gram_charlier,skewness=2.25,kurtosis=0.4,interpolation_nodes=100,interpolation_order=12,gram_charlier_order=20
err = 0.999006
err = 0.171001
err = 0.25565
err = 0.275353
err = 6.34477e+26
err = 7.61373e+26
err = 4.86898e+32
err = 4.86898e+32
err = 7.03444e+34
err = 7.03444e+34
err = 7.03444e+34
err = 7.03444e+34
err = 7.03444e+34
err = 7.03444e+34
err = 7.03444e+34
err = 7.03447e+34
err = 7.03447e+34
err = 7.03807e+34
err = 7.03808e+34
err = 7.26744e+34
trim = 2
ACF variance = 1
fixed_point_iteration:Iteration=0, var_wn=0.541662
fixed_point_iteration:Iteration=1, var_wn=0.387581
fixed_point_iteration:Iteration=2, var_wn=0.309602
fixed_point_iteration:Iteration=3, var_wn=0.262404
fixed_point_iteration:Iteration=4, var_wn=0.230656
fixed_point_iteration:Iteration=5, var_wn=0.207721
fixed_point_iteration:Iteration=6, var_wn=0.190288
fixed_point_iteration:Iteration=7, var_wn=0.176535
fixed_point_iteration:Iteration=8, var_wn=0.165376
fixed_point_iteration:Iteration=9, var_wn=0.156125
fixed_point_iteration:Iteration=10, var_wn=0.148322
fixed_point_iteration:Iteration=11, var_wn=0.141649
fixed_point_iteration:Iteration=12, var_wn=0.135875
fixed_point_iteration:Iteration=13, var_wn=0.130829
fixed_point_iteration:Iteration=14, var_wn=0.126382
fixed_point_iteration:Iteration=15, var_wn=0.122434
fixed_point_iteration:Iteration=16, var_wn=0.118905
fixed_point_iteration:Iteration=17, var_wn=0.115733
fixed_point_iteration:Iteration=18, var_wn=0.112867
fixed_point_iteration:Iteration=19, var_wn=0.110265
fixed_point_iteration:Iteration=20, var_wn=0.107894
fixed_point_iteration:Iteration=21, var_wn=0.105725
fixed_point_iteration:Iteration=22, var_wn=0.103733
fixed_point_iteration:Iteration=23, var_wn=0.101899
fixed_point_iteration:Iteration=24, var_wn=0.100206
fixed_point_iteration:Iteration=25, var_wn=0.0986382
fixed_point_iteration:Iteration=26, var_wn=0.0971831
fixed_point_iteration:Iteration=27, var_wn=0.0958297
fixed_point_iteration:Iteration=28, var_wn=0.0945682
fixed_point_iteration:Iteration=29, var_wn=0.0933903
fixed_point_iteration:Iteration=30, var_wn=0.0922883
fixed_point_iteration:Iteration=31, var_wn=0.0912558
fixed_point_iteration:Iteration=32, var_wn=0.0902869
fixed_point_iteration:Iteration=33, var_wn=0.0893763
fixed_point_iteration:Iteration=34, var_wn=0.0885194
fixed_point_iteration:Iteration=35, var_wn=0.087712
fixed_point_iteration:Iteration=36, var_wn=0.0869503
fixed_point_iteration:Iteration=37, var_wn=0.086231
fixed_point_iteration:Iteration=38, var_wn=0.085551
fixed_point_iteration:Iteration=39, var_wn=0.0849075
fixed_point_iteration:Iteration=40, var_wn=0.0842979
fixed_point_iteration:Iteration=41, var_wn=0.0837201
fixed_point_iteration:Iteration=42, var_wn=0.0831718
fixed_point_iteration:Iteration=43, var_wn=0.0826511
fixed_point_iteration:Iteration=44, var_wn=0.0821564
fixed_point_iteration:Iteration=45, var_wn=0.0816859
fixed_point_iteration:Iteration=46, var_wn=0.0812382
fixed_point_iteration:Iteration=47, var_wn=0.0808119
fixed_point_iteration:Iteration=48, var_wn=0.0804057
fixed_point_iteration:Iteration=49, var_wn=0.0800185
fixed_point_iteration:Iteration=50, var_wn=0.0796491
fixed_point_iteration:Iteration=51, var_wn=0.0792966
fixed_point_iteration:Iteration=52, var_wn=0.07896
fixed_point_iteration:Iteration=53, var_wn=0.0786385
fixed_point_iteration:Iteration=54, var_wn=0.0783313
fixed_point_iteration:Iteration=55, var_wn=0.0780375
fixed_point_iteration:Iteration=56, var_wn=0.0777565
fixed_point_iteration:Iteration=57, var_wn=0.0774875
fixed_point_iteration:Iteration=58, var_wn=0.0772301
fixed_point_iteration:Iteration=59, var_wn=0.0769836
fixed_point_iteration:Iteration=60, var_wn=0.0767475
fixed_point_iteration:Iteration=61, var_wn=0.0765213
fixed_point_iteration:Iteration=62, var_wn=0.0763044
fixed_point_iteration:Iteration=63, var_wn=0.0760964
fixed_point_iteration:Iteration=64, var_wn=0.0758969
fixed_point_iteration:Iteration=65, var_wn=0.0757056
fixed_point_iteration:Iteration=66, var_wn=0.075522
fixed_point_iteration:Iteration=67, var_wn=0.0753457
fixed_point_iteration:Iteration=68, var_wn=0.0751764
fixed_point_iteration:Iteration=69, var_wn=0.0750139
fixed_point_iteration:Iteration=70, var_wn=0.0748578
fixed_point_iteration:Iteration=71, var_wn=0.0747078
fixed_point_iteration:Iteration=72, var_wn=0.0745637
fixed_point_iteration:Iteration=73, var_wn=0.0744251
fixed_point_iteration:Iteration=74, var_wn=0.0742919
fixed_point_iteration:Iteration=75, var_wn=0.0741639
fixed_point_iteration:Iteration=76, var_wn=0.0740407
fixed_point_iteration:Iteration=77, var_wn=0.0739223
fixed_point_iteration:Iteration=78, var_wn=0.0738083
fixed_point_iteration:Iteration=79, var_wn=0.0736987
fixed_point_iteration:Iteration=80, var_wn=0.0735932
fixed_point_iteration:Iteration=81, var_wn=0.0734916
fixed_point_iteration:Iteration=82, var_wn=0.0733939
fixed_point_iteration:Iteration=83, var_wn=0.0732998
fixed_point_iteration:Iteration=84, var_wn=0.0732091
fixed_point_iteration:Iteration=85, var_wn=0.0731219
fixed_point_iteration:Iteration=86, var_wn=0.0730379
fixed_point_iteration:Iteration=87, var_wn=0.0729569
fixed_point_iteration:Iteration=88, var_wn=0.072879
fixed_point_iteration:Iteration=89, var_wn=0.0728038
fixed_point_iteration:Iteration=90, var_wn=0.0727315
fixed_point_iteration:Iteration=91, var_wn=0.0726617
fixed_point_iteration:Iteration=92, var_wn=0.0725945
fixed_point_iteration:Iteration=93, var_wn=0.0725298
fixed_point_iteration:Iteration=94, var_wn=0.0724673
fixed_point_iteration:Iteration=95, var_wn=0.0724072
fixed_point_iteration:Iteration=96, var_wn=0.0723491
fixed_point_iteration:Iteration=97, var_wn=0.0722932
fixed_point_iteration:Iteration=98, var_wn=0.0722393
fixed_point_iteration:Iteration=99, var_wn=0.0721873
fixed_point_iteration:Iteration=100, var_wn=0.0721372
fixed_point_iteration:Iteration=101, var_wn=0.0720888
fixed_point_iteration:Iteration=102, var_wn=0.0720422
fixed_point_iteration:Iteration=103, var_wn=0.0719972
fixed_point_iteration:Iteration=104, var_wn=0.0719538
fixed_point_iteration:Iteration=105, var_wn=0.0719119
fixed_point_iteration:Iteration=106, var_wn=0.0718716
fixed_point_iteration:Iteration=107, var_wn=0.0718326
fixed_point_iteration:Iteration=108, var_wn=0.0717951
fixed_point_iteration:Iteration=109, var_wn=0.0717588
fixed_point_iteration:Iteration=110, var_wn=0.0717238
fixed_point_iteration:Iteration=111, var_wn=0.0716901
fixed_point_iteration:Iteration=112, var_wn=0.0716575
fixed_point_iteration:Iteration=113, var_wn=0.0716261
fixed_point_iteration:Iteration=114, var_wn=0.0715957
fixed_point_iteration:Iteration=115, var_wn=0.0715664
fixed_point_iteration:Iteration=116, var_wn=0.0715382
fixed_point_iteration:Iteration=117, var_wn=0.0715109
fixed_point_iteration:Iteration=118, var_wn=0.0714846
fixed_point_iteration:Iteration=119, var_wn=0.0714592
fixed_point_iteration:Iteration=120, var_wn=0.0714347
fixed_point_iteration:Iteration=121, var_wn=0.071411
fixed_point_iteration:Iteration=122, var_wn=0.0713881
fixed_point_iteration:Iteration=123, var_wn=0.0713661
fixed_point_iteration:Iteration=124, var_wn=0.0713448
fixed_point_iteration:Iteration=125, var_wn=0.0713242
fixed_point_iteration:Iteration=126, var_wn=0.0713044
fixed_point_iteration:Iteration=127, var_wn=0.0712852
fixed_point_iteration:Iteration=128, var_wn=0.0712667
fixed_point_iteration:Iteration=129, var_wn=0.0712488
fixed_point_iteration:Iteration=130, var_wn=0.0712316
fixed_point_iteration:Iteration=131, var_wn=0.0712149
fixed_point_iteration:Iteration=132, var_wn=0.0711988
fixed_point_iteration:Iteration=133, var_wn=0.0711833
fixed_point_iteration:Iteration=134, var_wn=0.0711683
fixed_point_iteration:Iteration=135, var_wn=0.0711538
fixed_point_iteration:Iteration=136, var_wn=0.0711398
fixed_point_iteration:Iteration=137, var_wn=0.0711263
fixed_point_iteration:Iteration=138, var_wn=0.0711133
fixed_point_iteration:Iteration=139, var_wn=0.0711007
fixed_point_iteration:Iteration=140, var_wn=0.0710885
fixed_point_iteration:Iteration=141, var_wn=0.0710768
fixed_point_iteration:Iteration=142, var_wn=0.0710654
fixed_point_iteration:Iteration=143, var_wn=0.0710545
fixed_point_iteration:Iteration=144, var_wn=0.0710439
fixed_point_iteration:Iteration=145, var_wn=0.0710337
fixed_point_iteration:Iteration=146, var_wn=0.0710238
WN variance = 0.0710238
Partition size                 = (34,13,13)
Finished part [1/96]Finished part [2/96]Finished part [3/96]Finished part [4/96]Finished part [5/96]Finished part [6/96]Finished part [7/96]Finished part [8/96]Finished part [9/96]Finished part [10/96]Finished part [11/96]Finished part [12/96]Finished part [13/96]Finished part [14/96]Finished part [15/96]Finished part [16/96]Finished part [17/96]Finished part [18/96]Finished part [19/96]Finished part [20/96]Finished part [21/96]Finished part [22/96]Finished part [23/96]Finished part [24/96]Finished part [25/96]Finished part [26/96]Finished part [27/96]Finished part [28/96]Finished part [29/96]Finished part [30/96]Finished part [31/96]Finished part [32/96]Finished part [33/96]Finished part [34/96]Finished part [35/96]Finished part [36/96]Finished part [37/96]Finished part [38/96]Finished part [39/96]Finished part [40/96]Finished part [41/96]Finished part [42/96]Finished part [43/96]Finished part [44/96]Finished part [45/96]Finished part [46/96]Finished part [47/96]Finished part [48/96]Finished part [49/96]Finished part [50/96]Finished part [51/96]Finished part [52/96]Finished part [53/96]Finished part [54/96]Finished part [55/96]Finished part [56/96]Finished part [57/96]Finished part [58/96]Finished part [59/96]Finished part [60/96]Finished part [61/96]Finished part [62/96]Finished part [63/96]Finished part [64/96]Finished part [65/96]Finished part [66/96]Finished part [67/96]Finished part [68/96]Finished part [69/96]Finished part [70/96]Finished part [71/96]Finished part [72/96]Finished part [73/96]Finished part [74/96]Finished part [75/96]Finished part [76/96]Finished part [77/96]Finished part [78/96]Finished part [79/96]Finished part [80/96]Finished part [81/96]Finished part [82/96]Finished part [83/96]Finished part [84/96]Finished part [85/96]Finished part [86/96]Finished part [87/96]Finished part [88/96]Finished part [89/96]Finished part [90/96]Finished part [91/96]Finished part [92/96]Finished part [93/96]Finished part [94/96]Finished part [95/96]Finished part [96/96]
'zeta.csv' -> 'zeta-gramcharlier.csv'
Input file                     = /home/igankevich/workspace/arma-thesis/config/nit-propagating-skewnormal
Output grid size               = (200,40,40)
Output grid patch size         = (1,1,1)
Model                          = MA
Verification scheme            = manual
MA model                       = order=(20,10,10),acf.shape=(20,10,10),algorithm=fixed_point_iteration
Velocity potential solver name = N4arma8velocity21High_amplitude_solverIdEE
Velocity potential solver      = wnmax=from (0,0) to (0,0.25) npoints (2,2),depth=12,domain=from (10,-12) to (10,3) npoints (1,128)
NIT transform                  = dist=skew_normal,mean=0,stdev=1,alpha=1,interpolation_nodes=100,interpolation_order=12,gram_charlier_order=20
err = inf
err = inf
err = inf
err = inf
err = inf
err = inf
err = inf
err = inf
err = inf
err = inf
err = inf
err = inf
err = inf
err = inf
err = inf
err = inf
err = inf
err = inf
err = inf
err = inf
trim = 0
ACF variance = 1
fixed_point_iteration:Iteration=0, var_wn=0.541662
fixed_point_iteration:Iteration=1, var_wn=0.387581
fixed_point_iteration:Iteration=2, var_wn=0.309602
fixed_point_iteration:Iteration=3, var_wn=0.262404
fixed_point_iteration:Iteration=4, var_wn=0.230656
fixed_point_iteration:Iteration=5, var_wn=0.207721
fixed_point_iteration:Iteration=6, var_wn=0.190288
fixed_point_iteration:Iteration=7, var_wn=0.176535
fixed_point_iteration:Iteration=8, var_wn=0.165376
fixed_point_iteration:Iteration=9, var_wn=0.156125
fixed_point_iteration:Iteration=10, var_wn=0.148322
fixed_point_iteration:Iteration=11, var_wn=0.141649
fixed_point_iteration:Iteration=12, var_wn=0.135875
fixed_point_iteration:Iteration=13, var_wn=0.130829
fixed_point_iteration:Iteration=14, var_wn=0.126382
fixed_point_iteration:Iteration=15, var_wn=0.122434
fixed_point_iteration:Iteration=16, var_wn=0.118905
fixed_point_iteration:Iteration=17, var_wn=0.115733
fixed_point_iteration:Iteration=18, var_wn=0.112867
fixed_point_iteration:Iteration=19, var_wn=0.110265
fixed_point_iteration:Iteration=20, var_wn=0.107894
fixed_point_iteration:Iteration=21, var_wn=0.105725
fixed_point_iteration:Iteration=22, var_wn=0.103733
fixed_point_iteration:Iteration=23, var_wn=0.101899
fixed_point_iteration:Iteration=24, var_wn=0.100206
fixed_point_iteration:Iteration=25, var_wn=0.0986382
fixed_point_iteration:Iteration=26, var_wn=0.0971831
fixed_point_iteration:Iteration=27, var_wn=0.0958297
fixed_point_iteration:Iteration=28, var_wn=0.0945682
fixed_point_iteration:Iteration=29, var_wn=0.0933903
fixed_point_iteration:Iteration=30, var_wn=0.0922883
fixed_point_iteration:Iteration=31, var_wn=0.0912558
fixed_point_iteration:Iteration=32, var_wn=0.0902869
fixed_point_iteration:Iteration=33, var_wn=0.0893763
fixed_point_iteration:Iteration=34, var_wn=0.0885194
fixed_point_iteration:Iteration=35, var_wn=0.087712
fixed_point_iteration:Iteration=36, var_wn=0.0869503
fixed_point_iteration:Iteration=37, var_wn=0.086231
fixed_point_iteration:Iteration=38, var_wn=0.085551
fixed_point_iteration:Iteration=39, var_wn=0.0849075
fixed_point_iteration:Iteration=40, var_wn=0.0842979
fixed_point_iteration:Iteration=41, var_wn=0.0837201
fixed_point_iteration:Iteration=42, var_wn=0.0831718
fixed_point_iteration:Iteration=43, var_wn=0.0826511
fixed_point_iteration:Iteration=44, var_wn=0.0821564
fixed_point_iteration:Iteration=45, var_wn=0.0816859
fixed_point_iteration:Iteration=46, var_wn=0.0812382
fixed_point_iteration:Iteration=47, var_wn=0.0808119
fixed_point_iteration:Iteration=48, var_wn=0.0804057
fixed_point_iteration:Iteration=49, var_wn=0.0800185
fixed_point_iteration:Iteration=50, var_wn=0.0796491
fixed_point_iteration:Iteration=51, var_wn=0.0792966
fixed_point_iteration:Iteration=52, var_wn=0.07896
fixed_point_iteration:Iteration=53, var_wn=0.0786385
fixed_point_iteration:Iteration=54, var_wn=0.0783313
fixed_point_iteration:Iteration=55, var_wn=0.0780375
fixed_point_iteration:Iteration=56, var_wn=0.0777565
fixed_point_iteration:Iteration=57, var_wn=0.0774875
fixed_point_iteration:Iteration=58, var_wn=0.0772301
fixed_point_iteration:Iteration=59, var_wn=0.0769836
fixed_point_iteration:Iteration=60, var_wn=0.0767475
fixed_point_iteration:Iteration=61, var_wn=0.0765213
fixed_point_iteration:Iteration=62, var_wn=0.0763044
fixed_point_iteration:Iteration=63, var_wn=0.0760964
fixed_point_iteration:Iteration=64, var_wn=0.0758969
fixed_point_iteration:Iteration=65, var_wn=0.0757056
fixed_point_iteration:Iteration=66, var_wn=0.075522
fixed_point_iteration:Iteration=67, var_wn=0.0753457
fixed_point_iteration:Iteration=68, var_wn=0.0751764
fixed_point_iteration:Iteration=69, var_wn=0.0750139
fixed_point_iteration:Iteration=70, var_wn=0.0748578
fixed_point_iteration:Iteration=71, var_wn=0.0747078
fixed_point_iteration:Iteration=72, var_wn=0.0745637
fixed_point_iteration:Iteration=73, var_wn=0.0744251
fixed_point_iteration:Iteration=74, var_wn=0.0742919
fixed_point_iteration:Iteration=75, var_wn=0.0741639
fixed_point_iteration:Iteration=76, var_wn=0.0740407
fixed_point_iteration:Iteration=77, var_wn=0.0739223
fixed_point_iteration:Iteration=78, var_wn=0.0738083
fixed_point_iteration:Iteration=79, var_wn=0.0736987
fixed_point_iteration:Iteration=80, var_wn=0.0735932
fixed_point_iteration:Iteration=81, var_wn=0.0734916
fixed_point_iteration:Iteration=82, var_wn=0.0733939
fixed_point_iteration:Iteration=83, var_wn=0.0732998
fixed_point_iteration:Iteration=84, var_wn=0.0732091
fixed_point_iteration:Iteration=85, var_wn=0.0731219
fixed_point_iteration:Iteration=86, var_wn=0.0730379
fixed_point_iteration:Iteration=87, var_wn=0.0729569
fixed_point_iteration:Iteration=88, var_wn=0.072879
fixed_point_iteration:Iteration=89, var_wn=0.0728038
fixed_point_iteration:Iteration=90, var_wn=0.0727315
fixed_point_iteration:Iteration=91, var_wn=0.0726617
fixed_point_iteration:Iteration=92, var_wn=0.0725945
fixed_point_iteration:Iteration=93, var_wn=0.0725298
fixed_point_iteration:Iteration=94, var_wn=0.0724673
fixed_point_iteration:Iteration=95, var_wn=0.0724072
fixed_point_iteration:Iteration=96, var_wn=0.0723491
fixed_point_iteration:Iteration=97, var_wn=0.0722932
fixed_point_iteration:Iteration=98, var_wn=0.0722393
fixed_point_iteration:Iteration=99, var_wn=0.0721873
fixed_point_iteration:Iteration=100, var_wn=0.0721372
fixed_point_iteration:Iteration=101, var_wn=0.0720888
fixed_point_iteration:Iteration=102, var_wn=0.0720422
fixed_point_iteration:Iteration=103, var_wn=0.0719972
fixed_point_iteration:Iteration=104, var_wn=0.0719538
fixed_point_iteration:Iteration=105, var_wn=0.0719119
fixed_point_iteration:Iteration=106, var_wn=0.0718716
fixed_point_iteration:Iteration=107, var_wn=0.0718326
fixed_point_iteration:Iteration=108, var_wn=0.0717951
fixed_point_iteration:Iteration=109, var_wn=0.0717588
fixed_point_iteration:Iteration=110, var_wn=0.0717238
fixed_point_iteration:Iteration=111, var_wn=0.0716901
fixed_point_iteration:Iteration=112, var_wn=0.0716575
fixed_point_iteration:Iteration=113, var_wn=0.0716261
fixed_point_iteration:Iteration=114, var_wn=0.0715957
fixed_point_iteration:Iteration=115, var_wn=0.0715664
fixed_point_iteration:Iteration=116, var_wn=0.0715382
fixed_point_iteration:Iteration=117, var_wn=0.0715109
fixed_point_iteration:Iteration=118, var_wn=0.0714846
fixed_point_iteration:Iteration=119, var_wn=0.0714592
fixed_point_iteration:Iteration=120, var_wn=0.0714347
fixed_point_iteration:Iteration=121, var_wn=0.071411
fixed_point_iteration:Iteration=122, var_wn=0.0713881
fixed_point_iteration:Iteration=123, var_wn=0.0713661
fixed_point_iteration:Iteration=124, var_wn=0.0713448
fixed_point_iteration:Iteration=125, var_wn=0.0713242
fixed_point_iteration:Iteration=126, var_wn=0.0713044
fixed_point_iteration:Iteration=127, var_wn=0.0712852
fixed_point_iteration:Iteration=128, var_wn=0.0712667
fixed_point_iteration:Iteration=129, var_wn=0.0712488
fixed_point_iteration:Iteration=130, var_wn=0.0712316
fixed_point_iteration:Iteration=131, var_wn=0.0712149
fixed_point_iteration:Iteration=132, var_wn=0.0711988
fixed_point_iteration:Iteration=133, var_wn=0.0711833
fixed_point_iteration:Iteration=134, var_wn=0.0711683
fixed_point_iteration:Iteration=135, var_wn=0.0711538
fixed_point_iteration:Iteration=136, var_wn=0.0711398
fixed_point_iteration:Iteration=137, var_wn=0.0711263
fixed_point_iteration:Iteration=138, var_wn=0.0711133
fixed_point_iteration:Iteration=139, var_wn=0.0711007
fixed_point_iteration:Iteration=140, var_wn=0.0710885
fixed_point_iteration:Iteration=141, var_wn=0.0710768
fixed_point_iteration:Iteration=142, var_wn=0.0710654
fixed_point_iteration:Iteration=143, var_wn=0.0710545
fixed_point_iteration:Iteration=144, var_wn=0.0710439
fixed_point_iteration:Iteration=145, var_wn=0.0710337
fixed_point_iteration:Iteration=146, var_wn=0.0710238
WN variance = 0.0710238
Partition size                 = (34,13,13)
Finished part [1/96]Finished part [2/96]Finished part [3/96]Finished part [4/96]Finished part [5/96]Finished part [6/96]Finished part [7/96]Finished part [8/96]Finished part [9/96]Finished part [10/96]Finished part [11/96]Finished part [12/96]Finished part [13/96]Finished part [14/96]Finished part [15/96]Finished part [16/96]Finished part [17/96]Finished part [18/96]Finished part [19/96]Finished part [20/96]Finished part [21/96]Finished part [22/96]Finished part [23/96]Finished part [24/96]Finished part [25/96]Finished part [26/96]Finished part [27/96]Finished part [28/96]Finished part [29/96]Finished part [30/96]Finished part [31/96]Finished part [32/96]Finished part [33/96]Finished part [34/96]Finished part [35/96]Finished part [36/96]Finished part [37/96]Finished part [38/96]Finished part [39/96]Finished part [40/96]Finished part [41/96]Finished part [42/96]Finished part [43/96]Finished part [44/96]Finished part [45/96]Finished part [46/96]Finished part [47/96]Finished part [48/96]Finished part [49/96]Finished part [50/96]Finished part [51/96]Finished part [52/96]Finished part [53/96]Finished part [54/96]Finished part [55/96]Finished part [56/96]Finished part [57/96]Finished part [58/96]Finished part [59/96]Finished part [60/96]Finished part [61/96]Finished part [62/96]Finished part [63/96]Finished part [64/96]Finished part [65/96]Finished part [66/96]Finished part [67/96]Finished part [68/96]Finished part [69/96]Finished part [70/96]Finished part [71/96]Finished part [72/96]Finished part [73/96]Finished part [74/96]Finished part [75/96]Finished part [76/96]Finished part [77/96]Finished part [78/96]Finished part [79/96]Finished part [80/96]Finished part [81/96]Finished part [82/96]Finished part [83/96]Finished part [84/96]Finished part [85/96]Finished part [86/96]Finished part [87/96]Finished part [88/96]Finished part [89/96]Finished part [90/96]Finished part [91/96]Finished part [92/96]Finished part [93/96]Finished part [94/96]Finished part [95/96]Finished part [96/96]
'zeta.csv' -> 'zeta-skewnormal.csv'
NIT for standing waves
Input file                     = /home/igankevich/workspace/arma-thesis/config/nit-standing-none
Output grid size               = (200,40,40)
Output grid patch size         = (1,1,1)
Model                          = AR
Verification scheme            = manual
AR model                       = order=(7,7,7),acf.shape=(10,10,10)
Velocity potential solver name = N4arma8velocity21High_amplitude_solverIdEE
Velocity potential solver      = wnmax=from (0,0) to (0,0.25) npoints (2,2),depth=12,domain=from (10,-12) to (10,3) npoints (1,128)
NIT transform                  = none
ACF variance = 1
WN variance = 0.000519022
Partition size                 = (21,10,10)
Finished part [1/160]Finished part [2/160]Finished part [3/160]Finished part [4/160]Finished part [5/160]Finished part [6/160]Finished part [7/160]Finished part [8/160]Finished part [9/160]Finished part [10/160]Finished part [11/160]Finished part [12/160]Finished part [13/160]Finished part [14/160]Finished part [15/160]Finished part [16/160]Finished part [17/160]Finished part [18/160]Finished part [19/160]Finished part [20/160]Finished part [21/160]Finished part [22/160]Finished part [23/160]Finished part [24/160]Finished part [25/160]Finished part [26/160]Finished part [27/160]Finished part [28/160]Finished part [29/160]Finished part [30/160]Finished part [31/160]Finished part [32/160]Finished part [33/160]Finished part [34/160]Finished part [35/160]Finished part [36/160]Finished part [37/160]Finished part [38/160]Finished part [39/160]Finished part [40/160]Finished part [41/160]Finished part [42/160]Finished part [43/160]Finished part [44/160]Finished part [45/160]Finished part [46/160]Finished part [47/160]Finished part [48/160]Finished part [49/160]Finished part [50/160]Finished part [51/160]Finished part [52/160]Finished part [53/160]Finished part [54/160]Finished part [55/160]Finished part [56/160]Finished part [57/160]Finished part [58/160]Finished part [59/160]Finished part [60/160]Finished part [61/160]Finished part [62/160]Finished part [63/160]Finished part [64/160]Finished part [65/160]Finished part [66/160]Finished part [67/160]Finished part [68/160]Finished part [69/160]Finished part [70/160]Finished part [71/160]Finished part [72/160]Finished part [73/160]Finished part [74/160]Finished part [75/160]Finished part [76/160]Finished part [77/160]Finished part [78/160]Finished part [79/160]Finished part [80/160]Finished part [81/160]Finished part [82/160]Finished part [83/160]Finished part [84/160]Finished part [85/160]Finished part [86/160]Finished part [87/160]Finished part [88/160]Finished part [89/160]Finished part [90/160]Finished part [91/160]Finished part [92/160]Finished part [93/160]Finished part [94/160]Finished part [95/160]Finished part [96/160]Finished part [97/160]Finished part [98/160]Finished part [99/160]Finished part [100/160]Finished part [101/160]Finished part [102/160]Finished part [103/160]Finished part [104/160]Finished part [105/160]Finished part [106/160]Finished part [107/160]Finished part [108/160]Finished part [109/160]Finished part [110/160]Finished part [111/160]Finished part [112/160]Finished part [113/160]Finished part [114/160]Finished part [115/160]Finished part [116/160]Finished part [117/160]Finished part [118/160]Finished part [119/160]Finished part [120/160]Finished part [121/160]Finished part [122/160]Finished part [123/160]Finished part [124/160]Finished part [125/160]Finished part [126/160]Finished part [127/160]Finished part [128/160]Finished part [129/160]Finished part [130/160]Finished part [131/160]Finished part [132/160]Finished part [133/160]Finished part [134/160]Finished part [135/160]Finished part [136/160]Finished part [137/160]Finished part [138/160]Finished part [139/160]Finished part [140/160]Finished part [141/160]Finished part [142/160]Finished part [143/160]Finished part [144/160]Finished part [145/160]Finished part [146/160]Finished part [147/160]Finished part [148/160]Finished part [149/160]Finished part [150/160]Finished part [151/160]Finished part [152/160]Finished part [153/160]Finished part [154/160]Finished part [155/160]Finished part [156/160]Finished part [157/160]Finished part [158/160]Finished part [159/160]Finished part [160/160]
Zeta size = (192,32,32)
'zeta.csv' -> 'zeta-none.csv'
Input file                     = /home/igankevich/workspace/arma-thesis/config/nit-standing-gramcharlier
Output grid size               = (200,40,40)
Output grid patch size         = (1,1,1)
Model                          = AR
Verification scheme            = manual
AR model                       = order=(7,7,7),acf.shape=(10,10,10)
Velocity potential solver name = N4arma8velocity21High_amplitude_solverIdEE
Velocity potential solver      = wnmax=from (0,0) to (0,0.25) npoints (2,2),depth=12,domain=from (10,-12) to (10,3) npoints (1,128)
NIT transform                  = dist=gram_charlier,skewness=3.25,kurtosis=2.4,interpolation_nodes=100,interpolation_order=12,gram_charlier_order=20
err = 0.995366
err = 0.472576
err = 0.608796
err = 0.61584
err = 0.639818
err = 12.4501
err = 9.45159e+10
err = 1.08018e+11
err = 3.05905e+13
err = 3.39774e+13
err = 1.70414e+14
err = 1.82818e+14
err = 1.83166e+14
err = 2.11267e+25
err = 9.15985e+25
err = 2.75794e+29
err = 2.79745e+29
err = 3.63216e+31
err = 3.64474e+31
err = 2.32998e+33
trim = 2
ACF variance = 1
WN variance = 0.000519022
Partition size                 = (21,10,10)
Finished part [1/160]Finished part [2/160]Finished part [3/160]Finished part [4/160]Finished part [5/160]Finished part [6/160]Finished part [7/160]Finished part [8/160]Finished part [9/160]Finished part [10/160]Finished part [11/160]Finished part [12/160]Finished part [13/160]Finished part [14/160]Finished part [15/160]Finished part [16/160]Finished part [17/160]Finished part [18/160]Finished part [19/160]Finished part [20/160]Finished part [21/160]Finished part [22/160]Finished part [23/160]Finished part [24/160]Finished part [25/160]Finished part [26/160]Finished part [27/160]Finished part [28/160]Finished part [29/160]Finished part [30/160]Finished part [31/160]Finished part [32/160]Finished part [33/160]Finished part [34/160]Finished part [35/160]Finished part [36/160]Finished part [37/160]Finished part [38/160]Finished part [39/160]Finished part [40/160]Finished part [41/160]Finished part [42/160]Finished part [43/160]Finished part [44/160]Finished part [45/160]Finished part [46/160]Finished part [47/160]Finished part [48/160]Finished part [49/160]Finished part [50/160]Finished part [51/160]Finished part [52/160]Finished part [53/160]Finished part [54/160]Finished part [55/160]Finished part [56/160]Finished part [57/160]Finished part [58/160]Finished part [59/160]Finished part [60/160]Finished part [61/160]Finished part [62/160]Finished part [63/160]Finished part [64/160]Finished part [65/160]Finished part [66/160]Finished part [67/160]Finished part [68/160]Finished part [69/160]Finished part [70/160]Finished part [71/160]Finished part [72/160]Finished part [73/160]Finished part [74/160]Finished part [75/160]Finished part [76/160]Finished part [77/160]Finished part [78/160]Finished part [79/160]Finished part [80/160]Finished part [81/160]Finished part [82/160]Finished part [83/160]Finished part [84/160]Finished part [85/160]Finished part [86/160]Finished part [87/160]Finished part [88/160]Finished part [89/160]Finished part [90/160]Finished part [91/160]Finished part [92/160]Finished part [93/160]Finished part [94/160]Finished part [95/160]Finished part [96/160]Finished part [97/160]Finished part [98/160]Finished part [99/160]Finished part [100/160]Finished part [101/160]Finished part [102/160]Finished part [103/160]Finished part [104/160]Finished part [105/160]Finished part [106/160]Finished part [107/160]Finished part [108/160]Finished part [109/160]Finished part [110/160]Finished part [111/160]Finished part [112/160]Finished part [113/160]Finished part [114/160]Finished part [115/160]Finished part [116/160]Finished part [117/160]Finished part [118/160]Finished part [119/160]Finished part [120/160]Finished part [121/160]Finished part [122/160]Finished part [123/160]Finished part [124/160]Finished part [125/160]Finished part [126/160]Finished part [127/160]Finished part [128/160]Finished part [129/160]Finished part [130/160]Finished part [131/160]Finished part [132/160]Finished part [133/160]Finished part [134/160]Finished part [135/160]Finished part [136/160]Finished part [137/160]Finished part [138/160]Finished part [139/160]Finished part [140/160]Finished part [141/160]Finished part [142/160]Finished part [143/160]Finished part [144/160]Finished part [145/160]Finished part [146/160]Finished part [147/160]Finished part [148/160]Finished part [149/160]Finished part [150/160]Finished part [151/160]Finished part [152/160]Finished part [153/160]Finished part [154/160]Finished part [155/160]Finished part [156/160]Finished part [157/160]Finished part [158/160]Finished part [159/160]Finished part [160/160]
Zeta size = (192,32,32)
'zeta.csv' -> 'zeta-gramcharlier.csv'
Input file                     = /home/igankevich/workspace/arma-thesis/config/nit-standing-skewnormal
Output grid size               = (200,40,40)
Output grid patch size         = (1,1,1)
Model                          = AR
Verification scheme            = manual
AR model                       = order=(7,7,7),acf.shape=(10,10,10)
Velocity potential solver name = N4arma8velocity21High_amplitude_solverIdEE
Velocity potential solver      = wnmax=from (0,0) to (0,0.25) npoints (2,2),depth=12,domain=from (10,-12) to (10,3) npoints (1,128)
NIT transform                  = dist=skew_normal,mean=0,stdev=1,alpha=1,interpolation_nodes=100,interpolation_order=12,gram_charlier_order=20
err = 0.893011
err = 0.269503
err = 0.173329
err = 0.0040605
err = 2.82799e+09
err = 4.3525e+25
err = 4.3525e+25
err = 7.2978e+33
err = 2.7618e+44
err = 3.06867e+44
err = 3.06867e+44
err = 3.06867e+44
err = 3.06867e+44
err = 3.06867e+44
err = 3.06867e+44
err = 3.06867e+44
err = 3.06867e+44
err = 3.06867e+44
err = 3.06867e+44
err = 3.06867e+44
trim = 4
ACF variance = 1
WN variance = 0.000519022
Partition size                 = (21,10,10)
Finished part [1/160]Finished part [2/160]Finished part [3/160]Finished part [4/160]Finished part [5/160]Finished part [6/160]Finished part [7/160]Finished part [8/160]Finished part [9/160]Finished part [10/160]Finished part [11/160]Finished part [12/160]Finished part [13/160]Finished part [14/160]Finished part [15/160]Finished part [16/160]Finished part [17/160]Finished part [18/160]Finished part [19/160]Finished part [20/160]Finished part [21/160]Finished part [22/160]Finished part [23/160]Finished part [24/160]Finished part [25/160]Finished part [26/160]Finished part [27/160]Finished part [28/160]Finished part [29/160]Finished part [30/160]Finished part [31/160]Finished part [32/160]Finished part [33/160]Finished part [34/160]Finished part [35/160]Finished part [36/160]Finished part [37/160]Finished part [38/160]Finished part [39/160]Finished part [40/160]Finished part [41/160]Finished part [42/160]Finished part [43/160]Finished part [44/160]Finished part [45/160]Finished part [46/160]Finished part [47/160]Finished part [48/160]Finished part [49/160]Finished part [50/160]Finished part [51/160]Finished part [52/160]Finished part [53/160]Finished part [54/160]Finished part [55/160]Finished part [56/160]Finished part [57/160]Finished part [58/160]Finished part [59/160]Finished part [60/160]Finished part [61/160]Finished part [62/160]Finished part [63/160]Finished part [64/160]Finished part [65/160]Finished part [66/160]Finished part [67/160]Finished part [68/160]Finished part [69/160]Finished part [70/160]Finished part [71/160]Finished part [72/160]Finished part [73/160]Finished part [74/160]Finished part [75/160]Finished part [76/160]Finished part [77/160]Finished part [78/160]Finished part [79/160]Finished part [80/160]Finished part [81/160]Finished part [82/160]Finished part [83/160]Finished part [84/160]Finished part [85/160]Finished part [86/160]Finished part [87/160]Finished part [88/160]Finished part [89/160]Finished part [90/160]Finished part [91/160]Finished part [92/160]Finished part [93/160]Finished part [94/160]Finished part [95/160]Finished part [96/160]Finished part [97/160]Finished part [98/160]Finished part [99/160]Finished part [100/160]Finished part [101/160]Finished part [102/160]Finished part [103/160]Finished part [104/160]Finished part [105/160]Finished part [106/160]Finished part [107/160]Finished part [108/160]Finished part [109/160]Finished part [110/160]Finished part [111/160]Finished part [112/160]Finished part [113/160]Finished part [114/160]Finished part [115/160]Finished part [116/160]Finished part [117/160]Finished part [118/160]Finished part [119/160]Finished part [120/160]Finished part [121/160]Finished part [122/160]Finished part [123/160]Finished part [124/160]Finished part [125/160]Finished part [126/160]Finished part [127/160]Finished part [128/160]Finished part [129/160]Finished part [130/160]Finished part [131/160]Finished part [132/160]Finished part [133/160]Finished part [134/160]Finished part [135/160]Finished part [136/160]Finished part [137/160]Finished part [138/160]Finished part [139/160]Finished part [140/160]Finished part [141/160]Finished part [142/160]Finished part [143/160]Finished part [144/160]Finished part [145/160]Finished part [146/160]Finished part [147/160]Finished part [148/160]Finished part [149/160]Finished part [150/160]Finished part [151/160]Finished part [152/160]Finished part [153/160]Finished part [154/160]Finished part [155/160]Finished part [156/160]Finished part [157/160]Finished part [158/160]Finished part [159/160]Finished part [160/160]
Zeta size = (192,32,32)
'zeta.csv' -> 'zeta-skewnormal.csv'
#+end_example
** Download arma-benchmarks data from repository
#+begin_src sh :exports none :results verbatim
set -e
dir=build/arma-benchmarks
mkdir -p $dir
if ! test -d "$dir/.git"
then
    git clone https://github.com/igankevich/arma-benchmarks $dir
fi
cd $dir
git checkout master
git pull
git checkout 5c07dd4b6a334cc869e81588b2a2a772d6b85e43
#+end_src

#+RESULTS:
:       origin/master.
: Already up-to-date.

* Introduction
**** Topic relevance.
Software programmes, which simulates vessel behaviour in sea waves, are widely
used to model ship motion, estimate impact of external forces on floating
platform or other marine object, and estimate capsize probability under given
weather conditions; however, to model sea waves most of the simulation codes
use linear wave theory\nbsp{}cite:shin2003nonlinear,van2007forensic,kat2001prediction,van2002development, in
the framework of which it is difficult to reproduce certain peculiarities of
wind wave climate. Among them are transition between normal and storm weather,
and sea composed of multiple wave systems\nbsp{}--- both wind waves and swell\nbsp{}---
heading from multiple directions. Another shortcoming of linear wave theory is
an assumption, that wave amplitude is small compared to wave length. This makes
calculations imprecise when modelling ship motion in irregular waves, for which
the assumption does not hold. So, studying new and more advanced models and
methods for sea simulation software may increase number of its application
scenarios and foster a study of ship motion in extreme conditions in particular.

**** State-of-the-art.
Autoregressive moving average (ARMA) model emerged in response to difficulties
encountered by practitioners who used wave simulation models developed in the
framework of linear wave theory. The problems they have encountered with
Longuet---Higgins model (a model which is entirely based on linear wave theory)
can be summarised as the following.
1. /Periodicity/. Linear wave theory approximates waves by a sum of harmonics,
   so period of the whole wavy surface realisation depends on the number of
   harmonics in the model. The more realisation size is, the more coefficients
   are required to eliminate periodicity, therefore, generation time grows
   non-linearly with realisation size. This in turn results in overall low
   efficiency of any model based on this theory, no matter how optimised the
   software implementation is.
2. /Linearity/. Linear wave theory gives mathematical definition for sea waves
   which have small amplitudes compared to their lengths. Waves of this type
   occur mostly in open ocean, so near-shore waves as well as storm waves, for
   which this assumption does not hold, are not perfectly captured by linear
   theory.
3. /Probabilistic convergence/. Phase of a wave, which is often generated by
   pseudo random number generator (PRNG), has uniform distribution, and this
   makes wavy surface characteristics (average wave height, wave period, wave
   length etc.) sometimes converge slowly to the desired values. Convergence
   rate depends on the values generated by PRNG, so high convergence rate is not
   guaranteed.

These difficulties became a starting point in search for a new model which is
not based on linear wave theory. ARMA process studies were found to have all the
required mathematical apparatus.
1. ARMA process takes auto-covariate function (ACF) as an input parameter, and
   this function can be directly obtained from wave energy or
   frequency-directional spectrum (which is the input for Longuet---Higgins
   model). So, inputs for one model can easily be converted to each other.
2. There is no small-amplitude waves assumption. Wave may have any amplitude,
   and can be generated as steep as it is possible with real sea wave ACF.
3. Period of the realisation equals the period of PRNG, so generation time grows
   linearly with the realisation size.
4. White noise\nbsp{}--- the only probabilistic term in ARMA process\nbsp{}--- has
   Gaussian distribution; so, convergence rate is not probabilistic.

**** Goals and objectives.
ARMA process is the basis for ARMA sea simulation model, however, there
is still much work to be done to make it useful in practice.
1. Investigate how different ACF shapes affect the choice of ARMA parameters
   (the number of moving average and autoregressive processes coefficients).
2. Investigate a possibility to generate waves of arbitrary profile, not only
   cosines (which means taking into account asymmetric distribution of wavy
   surface elevation).
3. Develop a method to determine pressure field under discretely given wavy
   surface. Usually, such formulae are derived for a particular model by
   substituting wave profile formula into the eq. eqref:eq-problem, however,
   ARMA process does not provide explicit wave profile formula, so this problem
   has to be solved for general wavy surface (which is not defined by an
   analytic formula), without linearisation of boundaries and assumption of
   small-amplitude waves.
4. Verify wavy surface integral characteristics to match the ones of real sea
   waves.
5. Develop software programme that implements ARMA model and pressure
   calculation method, and allows to run simulations on both shared memory (SMP)
   and distributed memory (MPP) computer systems.

**** Scientific novelty.
ARMA model, as opposed to other sea simulation models, does not use linear
wave theory. This makes it capable of
- generating waves with arbitrary amplitudes by adjusting wave steepness via
  ACF;
- generating waves with arbitrary profiles by adjusting asymmetry of wave
  elevation distribution via non-linear inertia-less transform (NIT).
This makes it possible to use ARMA process to model transition between normal
and storm weather taking into account climate spectra and assimilation data of a
particular ocean region, which is not possible with models based on linear wave
theory.

The distinct feature of this work is the use of /three-dimensional/ AR and MA
models in all experiments and the development of software programme that
implements sea wavy surface generation and pressure field computation on both
shared memory (SMP) and distributed memory (MPP) computer systems, and also on
hybrid (GPGPU) systems using graphical coprocessors to accelerate computations.

**** Theoretical and practical significance.
Implementing ARMA model, that does not use assumptions of linear wave theory,
will increase quality of ship motion and marine object behaviour simulation
software.

1. Since pressure field method is developed for discrete wavy surface and
   without assumptions about wave amplitudes, it is applicable to any wavy
   surface of incompressible inviscid fluid (in particular, it is applicable to
   wavy surface generated by LH model). This allows to use this method without
   being tied to ARMA model.
2. From computational point of view this method is more efficient than the
   corresponding method for LH model, because integrals in its formula are
   reduced to Fourier transforms, for which there is fast Fourier transform
   (FFT) family of algorithms, optimised for different processor architectures.
3. Since the formula is explicit, there is no need in data exchange between
   parallel processes, which allows to achieve high scalability on computer
   clusters.
4. Finally, ARMA model is itself more efficient than LH model due to vicinity of
   trigonometric functions in its formula: In fact, wavy surface is computed as
   a sum of large number of polynomials, for which there is low-level assembly
   instruction (Fused Multiply-Add) giving native performance on CPUs.

**** Methodology and research methods.
Software implementation of ARMA model and pressure field calculation method was
created incrementally: a prototype written in high-level engineering
language\nbsp{}cite:mathematica10,octave2015 was rewritten in lower level
language (C++). Implementation of the same algorithms and methods in languages
of varying levels (which involves usage of different abstractions and language
primitives) allows to correct errors, which would left unnoticed otherwise. Wavy
surface, generated by ARMA model, as well as all input parameters (ACF,
distribution of wave elevation etc.) were inspected via graphical means built
into the programming language allowing visual control of programme correctness.

**** Theses for the defence.
- Wind wave model which allows to generate wavy surface realisations with large
  period and consisting of wave of arbitrary amplitudes;
- Pressure field calculation method derived for this model without assumptions
  of linear wave theory;
- Software implementation of the model and the method for shared memory (SMP)
  and distributed memory (MPP) systems.

**** Results verification and approbation.
ARMA model is verified by comparing generated wavy surface integral
characteristics (distribution of wave elevation, wave heights and lengths etc.)
to the ones of real sea waves. Pressure field calculation method was developed
using Mathematica language, where resulting formulae are verified by built-in
graphical means.

ARMA model and pressure field calculation method were incorporated into Large
Amplitude Motion Programme (LAMP)\nbsp{}--- an ship motion simulation software
programme\nbsp{}--- where they were compared to previously used LH model.
Preliminary numerical experiments showed higher computational efficiency of ARMA
model.

* Problem statement
The aim of the study reported here is to investigate possibilities of applying
ARMA process mathematical apparatus to sea wave modelling and to derive formula
for pressure field under generated wavy surface without assumptions of linear
wave theory.
- In case of small-amplitude waves resulting formula must correspond to the
  one from linear wave theory; in all other cases the formula must not diverge.
- Integral characteristics of generated wavy surface must match the ones of real
  sea waves.
- Software implementation of ARMA model and pressure field formula must work on
  shared memory (SMP) and distributed memory (MPP) systems.

**** Pressure field formula.
The problem of finding pressure field under wavy sea surface represents inverse
problem of hydrodynamics for incompressible inviscid fluid. System of equations
for it in general case is written as\nbsp{}cite:kochin1966theoretical
\begin{align}
    & \nabla^2\phi = 0,\nonumber\\
    & \phi_t+\frac{1}{2} |\vec{\upsilon}|^2 + g\zeta=-\frac{p}{\rho}, & \text{at }z=\zeta(x,y,t),\label{eq-problem}\\
    & D\zeta = \nabla \phi \cdot \vec{n}, & \text{at }z=\zeta(x,y,t),\nonumber
\end{align}
where \(\phi\)\nbsp{}--- velocity potential, \(\zeta\)\nbsp{}--- elevation
(\(z\) coordinate) of wavy surface, \(p\)\nbsp{}--- wave pressure,
\(\rho\)\nbsp{}--- fluid density,
\(\vec{\upsilon}=(\phi_x,\phi_y,\phi_z)\)\nbsp{}--- velocity vector,
\(g\)\nbsp{}--- acceleration of gravity, and \(D\)\nbsp{}--- substantial
(Lagrange) derivative. The first equation is called continuity (Laplace)
equation, the second one is the conservation of momentum law (the so called
dynamic boundary condition); the third one is kinematic boundary condition for
free wavy surface, which states that rate of change of wavy surface elevation
(\(D\zeta\)) equals to the change of velocity potential derivative along the
wavy surface normal (\(\nabla\phi\cdot\vec{n}\), see
section\nbsp{}[[#directional-derivative]]).

Inverse problem of hydrodynamics consists in solving this system of equations
for \(\phi\). In this formulation dynamic boundary condition becomes explicit
formula to determine pressure field using velocity potential derivatives
obtained from the remaining equations. So, from mathematical point of view
inverse problem of hydrodynamics reduces to Laplace equation with mixed boundary
condition\nbsp{}--- Robin problem.

Inverse problem is feasible because ARMA model generate hydrodynamically
adequate sea wavy surface: distributions of integral characteristics and
dispersion relation match the ones of real
waves\nbsp{}cite:boukhanovsky1997thesis,degtyarev2011modelling.

* ARMA model for sea wave simulation
** Sea wave models analysis
Pressure computation is only possible when the shape of wavy surface is known.
It is defined either at discrete grid points, or continuously via some analytic
formula. As will be shown in section [[#linearisation]], such formula may simplify
pressure computation by effectively reducing the task to pressure field
generation, instead of wavy surface generation.

**** Longuet---Higgins model.
The simplest model, formula of which is derived in the framework of linear wave
theory (see\nbsp{}section\nbsp{}[[#longuet-higgins-derivation]]), is
Longuet---Higgins (LH) model\nbsp{}cite:longuet1957statistical. In-depth
comparative analysis of this model and ARMA model is done
in\nbsp{}cite:degtyarev2011modelling,boukhanovsky1997thesis.

LH model represents sea wavy surface as a superposition of
sine waves with random amplitudes \(c_n\) and phases \(\epsilon_n\), continuously
distributed on interval \([0,2\pi]\). Wavy surface elevation (\(z\) coordinate) is
defined by
#+name: eq-longuet-higgins
\begin{equation}
    \zeta(x,y,t) = \sum\limits_n c_n \cos(u_n x + v_n y - \omega_n t + \epsilon_n).
\end{equation}
Here wave numbers \((u_n,v_n)\) are continuously distributed on plane \((u,v)\),
i.e. area \(du \times dv\) contains infinite quantity of wave numbers. Frequency
is related to wave numbers via dispersion relation \(\omega_n=\omega(u_n,v_n)\).
Function \(\zeta(x,y,t)\) is a three-dimensional ergodic stationary homogeneous
Gaussian process defined by
\begin{equation*}
    2E_\zeta(u,v)\, du\,  dv = \sum\limits_n c_n^2,
\end{equation*}
where \(E_\zeta(u,v)\)\nbsp{}--- two-dimensional wave energy spectral density.
Coefficients \(c_n\) are derived from wave energy spectrum \(S(\omega)\) via
\begin{equation*}
    c_n = \sqrt{ \textstyle\int\limits_{\omega_n}^{\omega_{n+1}} S(\omega) d\omega}.
\end{equation*}

**** Disadvantages of Longuet-Higgins model.
Although LH model is simple and easy to understand, there are shortcomings that
appear in practice.

1. The model simulates only stationary Gaussian process. This is consequence of
   central limit theorem (CLT): sum of large number of sines with random
   amplitudes and phases has normal distribution, no matter what spectrum is
   used as the model input. Using lower number of coefficients may solve the
   problem, but also make realisation period smaller. So, using LH model to
   simulate waves with non-Gaussian distribution of elevation\nbsp{}--- a
   distribution which real sea waves
   have\nbsp{}cite:huang1980experimental,1996 \nbsp{}--- is
   impractical.
2. From computational point of view, the deficiency of the model is non-linear
   increase of wavy surface generation time with the increase of realisation
   size. The larger the size of the realisation, the higher number of
   coefficients (discrete points of frequency-directional spectrum) is needed to
   eliminate periodicity. This makes LH model inefficient for long-time
   simulations.
3. Finally, there are peculiarities which make LH model unsuitable base for
   building more advanced simulation models.
   - In software implementation convergence rate of eq.\nbsp{}[[eq-longuet-higgins]]
     may be low due to randomness of phases \(\epsilon_n\).
   - It is difficult to generalise LH model for non-Gaussian processes as it
     involves incorporating non-linear terms in eq.\nbsp{}[[eq-longuet-higgins]] for
     which there is no known formula to determine
     coefficients\nbsp{}cite:1990.

To summarise, LH model is applicable to generating sea wavy surface in the
framework of linear wave theory, inefficient for long-time simulations, and
difficult to use as a base for more advanced models.

**** ARMA model
In\nbsp{}cite:spanos1982arma ARMA model is used to generate time series spectrum of
which is compatible with Pierson---Moskowitz (PM) approximation of sea wave
spectrum. The authors carry out experiments for one-dimensional AR, MA and ARMA
models. They mention excellent agreement between target and initial spectra and
higher performance of ARMA model compared to models based on summing large
number of harmonic components with random phases. The also mention that in order
to reach agreement between target and initial spectrum MA model require lesser
number of coefficients than AR model. In\nbsp{}cite:spanos1996efficient the authors
generalise ARMA model coefficients determination formulae for multi-variate
(vector) case.

One thing that distinguishes present work with respect to afore-mentioned ones
is the study of three-dimensional (2D in space and 1D in time) ARMA model, which
is mostly a different problem.
1. Yule---Walker system of equations, which are used to determine AR
   coefficients, has complex block-block structure.
2. Optimal model order (in a sense that target spectrum agrees with initial) is
   determined manually.
3. Instead of PM spectrum, analytic formulae for standing and propagating
   waves ACF are used as the model input.
4. Three-dimensional wavy surface should be compatible with real sea surface
   not only in terms of spectral characteristics, but also in the shape of wave
   profiles. So, model verification includes distributions of various parameters
   of generated waves (lengths, heights, periods etc.).
Multi-dimensionality of investigated model not only complexifies the task, but
also allows to carry out visual validation of generated wavy surface. It is the
opportunity to visualise output of the programme that allowed to ensure that
generated surface is compatible with real sea surface, and is not abstract
multi-dimensional stochastic process that is real only statistically.

In\nbsp{}cite:fusco2010short AR model is used to predict swell waves to control
wave-energy converters (WEC) in real-time. In order to make WEC more efficient
its internal oscillator frequency should match the one of sea waves. The
authors treat wave elevation as time series and compare performance of AR model,
neural networks and cyclical models in forecasting time series future values. AR
model gives the most accurate prediction of low-frequency swell waves for up to
two typical wave periods. It is an example of successful application of AR
process to sea wave modelling.

** Governing equations for 3-dimensional ARMA process
ARMA sea simulation model defines sea wavy surface as three-dimensional (two
dimensions in space and one in time) autoregressive moving average process:
every surface point is represented as a weighted sum of previous in time and
space points plus weighted sum of previous in time and space normally
distributed random impulses. The governing equation for 3-D ARMA process is
\begin{equation}
    \zeta_{\vec i}
    =
    \sum\limits_{\vec j = \vec 0}^{\vec N}
    \Phi_{\vec j} \zeta_{\vec i - \vec j}
    +
    \sum\limits_{\vec j = \vec 0}^{\vec M}
    \Theta_{\vec j} \epsilon_{\vec i - \vec j}
    ,
    \label{eq-arma-process}
\end{equation}
where \(\zeta\)\nbsp{}--- wave elevation, \(\Phi\)\nbsp{}--- AR process
coefficients, \(\Theta\)\nbsp{}--- MA process coefficients,
\(\epsilon\)\nbsp{}--- white noise with Gaussian distribution,
\(\vec{N}\)\nbsp{}--- AR process order, \(\vec{M}\)\nbsp{}--- MA process order,
and \(\Phi_{\vec{0}}\equiv{0}\), \(\Theta_{\vec{0}}\equiv{0}\). Here arrows
denote multi-component indices with a component for each dimension. In general,
any scalar quantity can be a component (temperature, salinity, concentration of
some substance in water etc.). Equation parameters are AR and MA process
coefficients and order.

Sea simulation model is derived to simulate realistic realisations of wind
induced wave field and is suitable to use in ship dynamics calculations.
Stationarity and invertibility properties are the main criteria for choosing one
or another process to simulate waves with different profiles, which are
discussed in sec.\nbsp{}[[#sec-process-selection]].

**** Autoregressive (AR) process.
AR process is ARMA process with only one random impulse instead of theirs
weighted sum:
\begin{equation}
    \zeta_{\vec i}
    =
    \sum\limits_{\vec j = \vec 0}^{\vec N}
    \Phi_{\vec j} \zeta_{\vec i - \vec j}
    +
    \epsilon_{i,j,k}
    .
    \label{eq-ar-process}
\end{equation}
The coefficients \(\Phi\) are calculated from ACF via three-dimensional
Yule---Walker equations, which are obtained after multiplying both parts of the
previous equation by \(\zeta_{\vec{i}-\vec{k}}\) and computing the expected value.
Generic form of YW equations is
\begin{equation}
    \label{eq-yule-walker}
    \gamma_{\vec k}
    =
    \sum\limits_{\vec j = \vec 0}^{\vec N}
    \Phi_{\vec j}
    \text{ }\gamma_{\vec{k}-\vec{j}}
    +
    \Var{\epsilon} \delta_{\vec{k}},
    \qquad
    \delta_{\vec{k}} =
    \begin{cases}
        1, \quad \text{if } \vec{k}=0 \\
        0, \quad \text{if } \vec{k}\neq0,
    \end{cases}
\end{equation}
where \(\gamma\)\nbsp{}--- ACF of process \(\zeta\),
\(\Var{\epsilon}\)\nbsp{}--- white noise variance. Matrix form of
three-dimensional YW equations, which is used in the present work, is
\begin{equation*}
    \Gamma
    \left[
        \begin{array}{l}
            \Phi_{\vec 0}\\
            \Phi_{0,0,1}\\
            \vdotswithin{\Phi_{\vec 0}}\\
            \Phi_{\vec N}
        \end{array}
    \right]
    =
    \left[
        \begin{array}{l}
            \gamma_{0,0,0}-\Var{\epsilon}\\
            \gamma_{0,0,1}\\
            \vdotswithin{\gamma_{\vec 0}}\\
            \gamma_{\vec N}
        \end{array}
    \right],
    \qquad
    \Gamma=
    \left[
        \begin{array}{llll}
            \Gamma_0 & \Gamma_1 & \cdots & \Gamma_{N_1} \\
            \Gamma_1 & \Gamma_0 & \ddots & \vdotswithin{\Gamma_0} \\
            \vdotswithin{\Gamma_0} & \ddots & \ddots & \Gamma_1 \\
            \Gamma_{N_1} & \cdots & \Gamma_1 & \Gamma_0
        \end{array}
    \right],
\end{equation*}
where \(\vec N = \left( p_1, p_2, p_3 \right)\) and
\begin{equation*}
    \Gamma_i =
    \left[
    \begin{array}{llll}
        \Gamma^0_i & \Gamma^1_i & \cdots & \Gamma^{N_2}_i \\
        \Gamma^1_i & \Gamma^0_i & \ddots & \vdotswithin{\Gamma^0_i} \\
        \vdotswithin{\Gamma^0_i} & \ddots & \ddots & \Gamma^1_i \\
        \Gamma^{N_2}_i & \cdots & \Gamma^1_i & \Gamma^0_i
    \end{array}
    \right]
    \qquad
    \Gamma_i^j=
    \left[
    \begin{array}{llll}
        \gamma_{i,j,0} & \gamma_{i,j,1} & \cdots & \gamma_{i,j,N_3} \\
        \gamma_{i,j,1} & \gamma_{i,j,0} & \ddots &x \vdotswithin{\gamma_{i,j,0}} \\
        \vdotswithin{\gamma_{i,j,0}} & \ddots & \ddots & \gamma_{i,j,1} \\
        \gamma_{i,j,N_3} & \cdots & \gamma_{i,j,1} & \gamma_{i,j,0}
    \end{array}
    \right],
\end{equation*}
Since \(\Phi_{\vec 0}\equiv0\), the first row and column of \(\Gamma\) can be
eliminated. Matrix \(\Gamma\) is block-toeplitz, positive definite and symmetric,
hence the system is efficiently solved by Cholesky decomposition, which is
particularly suitable for these types of matrices.

After solving this system of equations white noise variance is estimated from
eqref:eq-yule-walker by plugging \(\vec k = \vec 0\):
\begin{equation*}
    \Var{\epsilon} =
    \Var{\zeta}
    -
    \sum\limits_{\vec j = \vec 0}^{\vec N}
    \Phi_{\vec j}
    \text{ }\gamma_{\vec{j}}.
\end{equation*}

**** Moving average (MA) process.
MA process is ARMA process with \(\Phi\equiv0\):
\begin{equation}
    \zeta_{\vec i}
    =
    \sum\limits_{\vec j = \vec 0}^{\vec M}
    \Theta_{\vec j} \epsilon_{\vec i - \vec j}
    .
    \label{eq-ma-process}
\end{equation}
MA coefficients \(\Theta\) are defined implicitly via the following non-linear
system of equations:
\begin{equation*}
  \gamma_{\vec i} =
	\left[
		\displaystyle
    \sum\limits_{\vec j = \vec i}^{\vec M}
    \Theta_{\vec j}\Theta_{\vec j - \vec i}
	\right]
  \Var{\epsilon}.
\end{equation*}
The system is solved numerically by fixed-point iteration method via the
following formulae
\begin{equation*}
  \Theta_{\vec i} =
    -\frac{\gamma_{\vec 0}}{\Var{\epsilon}}
		+
    \sum\limits_{\vec j = \vec i}^{\vec M}
    \Theta_{\vec j} \Theta_{\vec j - \vec i}.
\end{equation*}
Here coefficients \(\Theta\) are calculated from back to front: from
\(\vec{i}=\vec{M}\) to \(\vec{i}=\vec{0}\). White noise variance is estimated by
\begin{equation*}
    \Var{\epsilon} = \frac{\gamma_{\vec 0}}{
		1
		+
    \sum\limits_{\vec j = \vec 0}^{\vec M}
    \Theta_{\vec j}^2
    }.
\end{equation*}
Authors of\nbsp{}cite:box1976time suggest using Newton---Raphson method to solve this
equation with higher precision, however, this method does not work in three
dimensions. Using slower method does not have dramatic effect on the overall
programme performance, because the number of coefficients is small and most of
the time is spent generating wavy surface.

**** Stationarity and invertibility of AR and MA processes
In order for modelled wavy surface to represent physical phenomena, the
corresponding process must be stationary and invertible. If the process is
invertible, then there is a reasonable connection of current events with the
events in the past, and if the process is stationary, the modelled physical
signal amplitude does not increase infinitely in time and space.

AR process is always invertible, and for stationarity it is necessary for roots
of characteristic equation
\begin{equation*}
1 - \Phi_{0,0,1} z - \Phi_{0,0,2} z^2
- \cdots
- \Phi_{\vec N} z^{N_0 N_1 N_2} = 0,
\end{equation*}
to lie \emph{outside} the unit circle. Here \(\vec{N}\) is AR process order
and \(\Phi\) are coefficients.

MA process is always stationary, and for invertibility it is necessary for roots
of characteristic equation
\begin{equation*}
1 - \Theta_{0,0,1} z - \Theta_{0,0,2} z^2
- \cdots
- \Theta_{\vec M} z^{M_0 M_1 M_2} = 0,
\end{equation*}
to lie \emph{outside} the unit circle. Here \(\vec{M}\) is
three-dimensional MA process order and \(\Theta\) are coefficients.

Stationarity and invertibility properties are the main criteria in selection of
the process to model different wave profiles, which are discussed in
section\nbsp{}[[#sec-process-selection]].

**** Mixed autoregressive moving average (ARMA) process.
:PROPERTIES:
:CUSTOM_ID: sec:how-to-mix-ARMA
:END:
Generally speaking, ARMA process is obtained by plugging MA generated wavy
surface as random impulse to AR process, however, in order to get the process
with desired ACF one should re-compute AR coefficients before plugging. There
are several approaches to "mix" AR and MA processes.
- The approach proposed in\nbsp{}cite:box1976time which involves dividing ACF into MA
  and AR part along each dimension is not applicable here, because in three
  dimensions such division is not possible: there always be parts of the ACF
  that are not taken into account by AR and MA process.
- The alternative approach is to use the same (undivided) ACF for both AR and MA
  processes but use different process order, however, then realisation
  characteristics (mean, variance etc.) become skewed: these are characteristics
  of the two overlapped processes.
For the first approach there is a formula to re-compute ACF for AR process, but
there is no such formula for the second approach. So, the best solution for now
is to simply use AR and MA process exclusively.

**** Process selection criteria for different wave profiles.
:PROPERTIES:
:CUSTOM_ID: sec-process-selection
:END:

One problem of ARMA model application to sea wave generation is that for
different types of wave profiles different processes /must/ be used: standing
waves are modelled by AR process, and propagating waves by MA process. This
statement comes from practice: if one tries to use the processes the other way
round, the resulting realisation either diverges or does not correspond to real
sea waves. (The latter happens for non-invertible MA process, as it is always
stationary.) So, the best way to apply ARMA model to sea wave generation is to
use AR process for standing waves and MA process for progressive waves.

The other problem is inability to automatically determine optimal number of
coefficients for three-dimensional AR and MA processes. For one-dimensional
processes this can be achieved via iterative methods\nbsp{}cite:box1976time, but they
diverge in three-dimensional case.

The final problem, which is discussed in [[#sec:how-to-mix-ARMA]], is inability to
"mix" AR and MA process in three dimensions.

In practice some statements made for AR and MA processes in\nbsp{}cite:box1976time
should be flipped for three-dimensional case. For example, the authors say that
ACF of MA process cuts at \(q\) and ACF of AR process decays to nought infinitely,
but in practice making ACF of 3-dimensional MA process not decay results in it
being non-invertible and producing realisation that does not look like real
sea waves, whereas doing the same for ACF of AR process results in stationary
process and adequate realisation. Also, the authors say that one
should allocate the first \(q\) points of ACF to MA process (as it often needed to
describe the peaks in ACF) and leave the rest points to AR process, but in
practice in case of ACF of a propagating wave AR process is stationary only for
the first time slice of the ACF, and the rest is left to MA process.

To summarise, the only established scenario of applying ARMA model to sea wave
generation is to use AR process for standing waves and MA process for
propagating waves. With new formulae for 3 dimensions a single mixed ARMA
process might increase model precision, which is one of the objectives of the
future research.

** Known pressure field determination formulae
**** Small amplitude waves theory.
In\nbsp{}cite:stab2012,1998,degtyarev1997analysis the
authors propose a solution for inverse problem of hydrodynamics of potential
flow in the framework of small-amplitude wave theory (under assumption that wave
length is much larger than height: \(\lambda \gg h\)). In that case inverse
problem is linear and reduces to Laplace equation with mixed boundary
conditions, and equation of motion is solely used to determine pressures for
calculated velocity potential derivatives. The assumption of small amplitudes
means the slow decay of wind wave coherence function, i.e. small change of local
wave number in time and space compared to the wavy surface elevation (\(z\)
coordinate). This assumption allows to calculate elevation \(z\) derivative as
\(\zeta_z=k\zeta\), where \(k\) is wave number. In two-dimensional case the
solution is written explicitly as
\begin{align}
    \left.\frac{\partial\phi}{\partial x}\right|_{x,t}= &
        -\frac{1}{\sqrt{1+\alpha^{2}}}e^{-I(x)}
            \int\limits_{0}^x\frac{\partial\dot{\zeta}/\partial
                z+\alpha\dot{\alpha}}{\sqrt{1+\alpha^{2}}}e^{I(x)}dx,\label{eq-old-sol-2d}\\
    I(x)= & \int\limits_{0}^x\frac{\partial\alpha/\partial z}{1+\alpha^{2}}dx,\nonumber
\end{align}
where \(\alpha\) is wave slope. In three-dimensional case solution is written in
the form of elliptic partial differential equation (PDE):
\begin{align*}
    & \frac{\partial^2 \phi}{\partial x^2} \left( 1 + \alpha_x^2 \right) +
    \frac{\partial^2 \phi}{\partial y^2} \left( 1 + \alpha_y^2 \right) +
    2\alpha_x\alpha_y \frac{\partial^2 \phi}{\partial x \partial y} + \\
    & \left(
        \frac{\partial \alpha_x}{\partial z} +
        \alpha_x \frac{\partial \alpha_x}{\partial x} +
        \alpha_y \frac{\partial \alpha_x}{\partial y}
    \right) \frac{\partial \phi}{\partial x} + \\
    & \left(
        \frac{\partial \alpha_y}{\partial z} +
        \alpha_x \frac{\partial \alpha_y}{\partial x} +
        \alpha_y \frac{\partial \alpha_y}{\partial y}
    \right) \frac{\partial \phi}{\partial y} + \\
    & \frac{\partial \dot{\zeta}}{\partial z} +
    \alpha_x \dot{\alpha_x} + \alpha_y \dot{\alpha_y} = 0.
\end{align*}
The authors suggest transforming this equation to finite differences and solve
it numerically.

As will be shown in [[#sec:compare-formulae]] that eqref:eq-old-sol-2d diverges when
attempted to calculate velocity field for large-amplitude waves, and this is the
reason that it can not be used together with ARMA model, that generates
arbitrary-amplitude waves.

**** Linearisation of boundary condition.
:PROPERTIES:
:CUSTOM_ID: linearisation
:END:

LH model allows to derive an explicit formula for velocity field by linearising
kinematic boundary condition. Velocity potential formula is written as
\begin{equation*}
\phi(x,y,z,t) = \sum_n \frac{c_n g}{\omega_n}
     e^{\sqrt{u_n^2+v_n^2} z}
     \sin(u_n x + v_n y - \omega_n t + \epsilon_n).
\end{equation*}
This formula is differentiated to obtain velocity potential derivatives, which
are plugged to dynamic boundary condition to obtain pressures.

** Determining wave pressures for discretely given wavy surface
Analytic solutions to boundary problems in classical equations are often used to
study different properties of the solution, and for that purpose general
solution formula is too difficult to study, as it contains integrals of unknown
functions. Fourier method is one of the methods to find analytic solutions to
PDE. It is based on application of Fourier transform to each part of PDE, which
reduces the equation to algebraic, and the solution is written as inverse
Fourier transform of some function (which may contain Fourier transforms of
other functions). Since, it is not possible to write analytic forms of these
Fourier transforms in all cases, unique solutions are found and their behaviour
is studied in different domains instead. At the same time, computing discrete
Fourier transforms on the computer is possible for any discretely defined
function and efficient when using FFT algorithms. These algorithms use symmetry
of complex exponentials to decrease asymptotic complexity from
\(\mathcal{O}(n^2)\) to \(\mathcal{O}(n\log_{2}n)\). So, even if general solution
contains Fourier transforms of unknown functions, they still can be computed
numerically, and FFT family of algorithms makes this approach efficient.

Alternative approach to solve PDE is to reduce it to difference equations, which
are solved by constructing various numerical schemes. This approach leads to
approximate solution, and asymptotic complexity of corresponding algorithms is
comparable to that of FFT. For example, stationary elliptic PDE transforms to
implicit numerical scheme which is solved by iterative method on each step of
which a tridiagonal of five-diagonal system of algebraic equations is solved by
Thomas algorithm. Asymptotic complexity of this approach is
\(\mathcal{O}({n}{m})\), where \(n\)\nbsp{}--- number of wavy surface grid points, \(m\)\nbsp{}---
number of iterations. Despite their wide spread, iterative algorithms are
inefficient on parallel computer architectures; in particular, their mapping to
co-processors may involve copying data in and out of the co-processor in each
iteration, which negatively affects their performance. At the same time, high
number of Fourier transforms in the solution is an advantage, rather than a
disadvantage. First, solutions obtained by Fourier method are explicit, hence
their implementations scales with the large number of parallel computer cores.
Second, there are implementations of FFT optimised for different processor
architectures as well as co-processors (GPU, MIC) which makes it easy to get
high performance on any computing platform. These advantages substantiate the
choice of Fourier method to obtain explicit analytic solution to the problem of
determining pressures under wavy sea surface.

*** Two-dimensional velocity field
:PROPERTIES:
:CUSTOM_ID: sec:pressure-2d
:END:
**** Formula for infinite depth fluid.
Two-dimensional Laplace equation with Robin boundary condition is written as
\begin{align}
    \label{eq-problem-2d}
    & \phi_{xx}+\phi_{zz}=0,\\
    & \zeta_t + \zeta_x\phi_x = \frac{\zeta_x}{\sqrt{1 + \zeta_x^2}} \phi_x - \phi_z, & \text{ }z=\zeta(x,t).\nonumber
\end{align}
Use Fourier method to solve this problem. Applying Fourier transform to both
sides of the equation yields
\begin{equation*}
    -4 \pi^2 \left( u^2 + v^2 \right)
    \FourierY{\phi(x,z)}{u,v} = 0,
\end{equation*}
hence \(v = \pm i u\). Hereinafter we use the following symmetric form of Fourier
transform:
\begin{equation*}
    \FourierY{f(x,y)}{u,v} =
    \iint\limits_{-\infty}^{\phantom{--}\infty}
    f(x,y)
    e^{-2\pi i (x u + y v)}
    dx dy.
\end{equation*}
We seek solution in the form of inverse Fourier transform
\(\phi(x,z)=\InverseFourierY{E(u,v)}{x,z}\). Plugging[fn::\(v={-i}{u}\) is not
applicable because velocity potential must go to nought when depth goes to
infinity.] \(v={i}{u}\) into the formula yields
\begin{equation}
    \label{eq-guessed-sol-2d}
    \phi(x,z) = \InverseFourierY{e^{2\pi u z}E(u)}{x}.
\end{equation}
In order to make substitution \(z=\zeta(x,t)\) not interfere with Fourier
transforms, we rewrite eqref:eq-guessed-sol-2d as a convolution:
\begin{equation*}
    \phi(x,z)
    =
    \Fun{z}
    \ast
    \InverseFourierY{E(u)}{x},
\end{equation*}
where \(\Fun{z}\)\nbsp{}--- a function, form of which is defined in section
[[#sec:compute-delta]] and which satisfies equation
\(\FourierY{\Fun{z}}{u}=e^{2\pi{u}{z}}\). Plugging formula \(\phi\) into the boundary
condition yields
\begin{equation*}
    \zeta_t
    =
    \left( i f(x) - 1 \right)
    \left[
        \Fun{z}
        \ast
        \InverseFourierY{2\pi u E(u)}{x}
    \right],
\end{equation*}
where \(f(x)={\zeta_x}/{\sqrt{1+\zeta_x^2}}-\zeta_x\). Applying Fourier transform
to both sides of this equation yields formula for coefficients \(E\):
\begin{equation*}
    E(u) =
    \frac{1}{2\pi u}
    \frac{
    \FourierY{\zeta_t / \left(i f(x) - 1\right)}{u}
    }{
    \FourierY{\Fun{z}}{u}
    }
\end{equation*}
Finally, substituting \(z\) for \(\zeta(x,t)\) and plugging resulting equation into
eqref:eq-guessed-sol-2d yields formula for \(\phi(x,z)\):
\begin{equation}
    \label{eq-solution-2d}
    \boxed{
        \phi(x,z)
        =
        \InverseFourierY{
            \frac{e^{2\pi u z}}{2\pi u}
            \frac{
            \FourierY{ \zeta_t / \left(i f(x) - 1\right) }{u}
            }{
            \FourierY{ \Fun{\zeta(x,t)} }{u}
            }
        }{x}.
    }
\end{equation}

Multiplier \(e^{2\pi{u}{z}}/(2\pi{u})\) makes graph of a function to which Fourier
transform of which is applied asymmetric with respect to \(OY\) axis. This makes
it difficult to apply FFT which expects periodic function with nought on both
ends of the interval. Using numerical integration instead of FFT is not faster
than solving the initial system of equations with numerical schemes. This
problem is alleviated by using formula eqref:eq-solution-2d-full for finite
depth fluid with wittingly large depth \(h\). This formula is derived in the
following section.

**** Formula for finite depth fluid.
On the sea bottom vertical fluid velocity component equals nought: \(\phi_z=0\) on
\(z=-h\), where \(h\)\nbsp{}--- water depth. In this case equation \(v=-{i}{u}\), which came
from Laplace equation, can not be neglected, hence the solution is sought in the
following form:
\begin{equation}
    \phi(x,z)
    =
    \InverseFourierY{
        \left( C_1 e^{2\pi u z} + C_2 e^{-2\pi u z} \right)
        E(u)
    }{x}.
    \label{eq-guessed-sol-2d-full}
\end{equation}
Plugging \(\phi\) into the boundary condition on the sea bottom yields
\begin{equation*}
    C_1 e^{-2\pi u h} - C_2 e^{2\pi u h} = 0,
\end{equation*}
hence \(C_1=\frac{1}{2}C{e}^{2\pi{u}{h}}\) and
\(C_2=-\frac{1}{2}C{e}^{-2\pi{u}{h}}\). Constant \(C\) may take arbitrary value
here, because after plugging it becomes part of unknown coefficients \(E(u)\).
Plugging formulae for \(C_1\) and \(C_2\) into eqref:eq-guessed-sol-2d-full yields
\begin{equation*}
    \phi(x,z) = \InverseFourierY{ \Sinh{2\pi u (z+h)} E(u) }{x}.
\end{equation*}
Plugging \(\phi\) into the boundary condition on the free surface yields
\begin{equation*}
    \zeta_t = f(x) \InverseFourierY{ 2\pi i u \Sinh{2\pi u (z+h)} E(u) }{x}
            - \InverseFourierY{ 2\pi u \SinhX{2\pi u (z+h)} E(u) }{x}.
\end{equation*}
Here \(\sinh\) and \(\cosh\) give similar results near free surface, and since this
is the main area of interest in practical applications, we assume that
\(\Sinh{2\pi{u}(z+h)}\approx\SinhX{2\pi{u}(z+h)}\). Performing analogous to the
previous section transformations yields final formula for \(\phi(x,z)\):
\begin{equation}
\boxed{
    \phi(x,z,t)
    =
  \InverseFourierY{
        \frac{\Sinh{2\pi u (z+h)}}{2\pi u}
        \frac{
            \FourierY{ \zeta_t / \left(i f(x) - 1\right) }{u}
        }{
            \FourierY{ \FunSecond{\zeta(x,t)} }{u}
        }
    }{x},
}
    \label{eq-solution-2d-full}
\end{equation}
where \(\FunSecond{z}\)\nbsp{}--- a function, form of which is defined in section
[[#sec:compute-delta]] and which satisfies equation
\(\FourierY{\FunSecond{z}}{u}=\Sinh{2\pi{u}{z}}\).

**** Reducing to the formulae from linear wave theory.
Check the validity of derived formulae by substituting \(\zeta(x,t)\) with known
analytic formula for plain waves. Symbolic computation of Fourier transforms in
this section were performed in Mathematica\nbsp{}cite:mathematica10. In the framework
of linear wave theory assume that waves have small amplitude compared to their
lengths, which allows us to simplify initial system of equations
eqref:eq-problem-2d to
\begin{align*}
    & \phi_{xx}+\phi_{zz}=0,\\
    & \zeta_t = -\phi_z & \text{ }z=\zeta(x,t),
\end{align*}
solution to which is written as
\begin{equation*}
    \phi(x,z,t)
    =
    -\InverseFourierY{
        \frac{e^{2\pi u z}}{2\pi u}
        \FourierY{\zeta_t}{u}
    }{x}
    .
\end{equation*}
Propagating wave profile is defined as \(\zeta(x,t)=A\cos(2\pi(kx-t))\). Plugging
this formula into eqref:eq-solution-2d yields
\(\phi(x,z,t)=-\frac{A}{k}\sin(2\pi(kx-t))\Sinh{2\pi{k}{z}}\). In order to reduce
it to the formula from linear wave theory, rewrite hyperbolic sine in
exponential form, discard the term containing \(e^{-2\pi{k}{z}}\) as contradicting
condition \(\phi\underset{z\rightarrow-\infty}{\longrightarrow}0\). Taking real
part of the resulting formula yields
\(\phi(x,z,t)=\frac{A}{k}e^{2\pi{k}{z}}\sin(2\pi(kx-t))\), which corresponds to
the known formula from linear wave theory. Similarly, under small-amplitude
waves assumption the formula for finite depth fluid eqref:eq-solution-2d-full is
reduced to
\begin{equation*}
    \phi(x,z,t)
    =
    -\InverseFourierY{
        \frac{\Sinh{2\pi u (z+h)}}{2\pi u \Sinh{2\pi u h}}
        \FourierY{\zeta_t}{u}
    }{x}.
\end{equation*}
Substituting \(\zeta(x,t)\) with propagating plain wave profile formula yields
\begin{equation}
    \label{eq-solution-2d-linear}
    \phi(x,z,t)=\frac{A}{k}
    \frac{\Sinh{2 \pi k (z+h)}}{ \Sinh{2 \pi k h} }
    \sin(2 \pi (k x-t)),
\end{equation}
which corresponds to the formula from linear wave theory for finite depth fluid.

Different forms of Laplace equation solutions, in which decaying exponent is
written with either "+" or "-" signs, may cause incompatibilities between
formulae from linear wave theory and formulae derived in this work, where
\(\sinh\) is used instead of \(\cosh\). Equality
\(\frac{\Sinh{2\pi{k}(z+h)}}{\Sinh{2\pi{k}{h}}}\approx\frac{\sinh(2\pi{k}(z+h))}{\sinh(2\pi{k}{h})}\)
becomes strict on the free surface, and difference between left-hand and
right-hand sides increases when approaching sea bottom (for sufficiently large
depth difference near free surface is negligible). So, for sufficiently large
depth any function (\(\cosh\) or \(\sinh\)) may be used for velocity potential
computation near free surface.

Reducing eqref:eq-solution-2d  eqref:eq-solution-2d-full to the known formulae
from linear wave theory shows, that formula for infinite depth
eqref:eq-solution-2d is not suitable to compute velocity potentials with Fourier
method, because it does not have symmetry, which is required for Fourier
transform. However, formula for finite depth can be used instead by setting \(h\)
to some characteristic water depth. For standing wave reducing to linear wave
theory formulae is made under the same assumptions.

*** Three-dimensional velocity field
Three-dimensional version of eqref:eq-problem is written as
\begin{align}
    \label{eq-problem-3d}
    & \phi_{xx} + \phi_{yy} + \phi_{zz} = 0,\\
    & \zeta_t + \zeta_x\phi_x + \zeta_y\phi_y
    =
    \frac{\zeta_x}{\SqrtZeta{1 + \zeta_x^2 + \zeta_y^2}} \phi_x
    +\frac{\zeta_y}{\SqrtZeta{1 + \zeta_x^2 + \zeta_y^2}} \phi_y
    - \phi_z, & \text{ }z=\zeta(x,y,t).\nonumber
\end{align}
Again, use Fourier method to solve it. Applying Fourier transform to both sides
of Laplace equation yields
\begin{equation*}
    -4 \pi^2 \left( u^2 + v^2 + w^2 \right)
    \FourierY{\phi(x,y,z)}{u,v,w} = 0,
\end{equation*}
hence \(w=\pm{i}\sqrt{u^2+v^2}\). We seek solution in the form of inverse Fourier
transform \(\phi(x,y,z)=\InverseFourierY{E(u,v,w)}{x,y,z}\). Plugging
\(w=i\sqrt{u^2+v^2}=i\Kveclen\) into the formula yields
\begin{equation*}
    \phi(x,y,z) = \InverseFourierY{
        \left(
            C_1 e^{2\pi \Kveclen z}
            -C_2 e^{-2\pi \Kveclen z}
        \right)
        E(u,v)
    }{x,y}.
\end{equation*}
Plugging \(\phi\) into the boundary condition on the sea bottom (analogous to
two-dimensional case) yields
\begin{equation}
    \label{eq-guessed-sol-3d}
    \phi(x,y,z) = \InverseFourierY{
        \Sinh{2\pi \Kveclen (z+h)} E(u,v)
    }{x,y}.
\end{equation}
Plugging \(\phi\) into the boundary condition on the free surface yields
\begin{equation*}
    \arraycolsep=1.4pt
    \begin{array}{rl}
        \zeta_t = & i f_1(x,y) \InverseFourierY{2 \pi u \Sinh{2\pi \Kveclen (z+h)}E(u,v)}{x,y} \\
        + & i f_2(x,y) \InverseFourierY{2 \pi v \Sinh{2\pi \Kveclen (z+h)}E(u,v)}{x,y} \\
        - & \InverseFourierY{2 \pi \Kveclen \SinhX{2\pi \Kveclen (z+h)}E(u,v)}{x,y}
    \end{array}
\end{equation*}
where \(f_1(x,y)={\zeta_x}/{\SqrtZeta{1+\zeta_x^2+\zeta_y^2}}-\zeta_x\) and
\(f_2(x,y)={\zeta_y}/{\SqrtZeta{1+\zeta_x^2+\zeta_y^2}}-\zeta_y\).

Like in Section\nbsp{}[[#sec:pressure-2d]] we assume that
\(\Sinh{2\pi{u}(z+h)}\approx\SinhX{2\pi{u}(z+h)}\) near free surface, but in
three-dimensional case this is not enough to solve the problem. In order to get
analytic formula for coefficients \(E\) we need to assume, that all Fourier
transforms in the equation have radially symmetric kernels, i.e. replace \(u\)
and \(v\) with \(\Kveclen\). There are two points supporting this assumption.
First, in numerical implementation integration is done over positive wave
numbers, so the sign of \(u\) and \(v\) does not affect the solution. Second,
the rate growth of \(\cosh\) term of the integral kernel is much higher than the
one of \(u\) or \(\Kveclen\), so the substitution has small effect on the
magnitude of the solution. Despite these two points, a use of more
mathematically rigorous approach would be preferable.

Making the replacement, applying Fourier transform to both sides of the equation
and plugging the result into eqref:eq-guessed-sol-3d yields formula for
\(\phi\):
\begin{equation*}
    \phi(x,y,z,t) = \InverseFourierY{
        \frac{ \Sinh{\smash{2\pi \Kveclen (z+h)}} }{ 2\pi\Kveclen }
        \frac{ \FourierY{ \zeta_t / \left( i f_1(x,y) + i f_2(x,y) - 1 \right)}{u,v} }
        { \FourierY{\mathcal{D}_3\left( x,y,\zeta\left(x,y\right) \right)}{u,v} }
    }{x,y},
\end{equation*}
where \(\FourierY{\mathcal{D}_3\left(x,y,z\right)}{u,v}=\Sinh{\smash{2\pi\Kveclen{}z}}\).

** Modelling non-linearity of sea waves
ARMA model allows to model asymmetry of wave elevation distribution, i.e.\nbsp{}
generate sea waves, distribution of \(z\)-coordinate of which has non-nought
kurtosis and asymmetry. Such distribution is inherent to real sea
waves\nbsp{}cite:longuet1963nonlinear.

Wave asymmetry is modelled by non-linear inertia-less transform (NIT) of
stochastic process, however, transforming resulting wavy surface means
transforming initial ACF. In order to alleviate this, ACF must be preliminary
transformed as shown in\nbsp{}cite:boukhanovsky1997thesis.

**** Wavy surface transformation.
Explicit formula \(z=f(y)\) that transforms wavy surface to desired
one-dimensional distribution \(F(z)\) is the solution of non-linear
transcendental equation \(F(z)=\Phi(y)\), where \(\Phi(y)\)\nbsp{}---
one-dimensional Gaussian distribution. Since distribution of wave elevation is
often given by some approximation based on field data, this equation is solved
numerically with respect to \(z_k\) in each grid point \(y_k|_{k=0}^N\) of
generated wavy surface. In this case equation is rewritten as
\begin{equation}
    \label{eq-distribution-transformation}
    F(z_k)
    =
    \frac{1}{\sqrt{2\pi}}
    \int\limits_0^{y_k} \exp\left[ -\frac{t^2}{2} \right] dt
    .
\end{equation}
Since, distribution functions are monotonic, the simplest interval halving
(bisection) numerical method is used to solve this equation.

**** Preliminary ACF transformation.
In order to transform ACF \(\gamma_z\) of the process, it is expanded in series
of Hermite polynomials (Gram---Charlier series)
\begin{equation*}
    \gamma_z \left( \vec u \right)
    =
    \sum\limits_{m=0}^{\infty}
    C_m^2 \frac{\gamma_y^m \left( \vec u \right)}{m!},
\end{equation*}
where
\begin{equation*}
    C_m = \frac{1}{\sqrt{2\pi}}
  \int\limits_{0}^\infty
    f(y) H_m(y) \exp\left[ -\frac{y^2}{2} \right],
\end{equation*}
\(H_m\)\nbsp{}--- Hermite polynomial, and \(f(y)\)\nbsp{}--- solution to equation
eqref:eq-distribution-transformation. Plugging polynomial approximation
\(f(y)\approx\sum\limits_{i}d_{i}y^i\) and analytic formulae for Hermite
polynomial yields
\begin{equation*}
    \frac{1}{\sqrt{2\pi}}
    \int\limits_\infty^\infty
    y^k \exp\left[ -\frac{y^2}{2} \right]
    =
    \begin{cases}
        (k-1)!! & \text{if }k\text{ is even},\\
        0       & \text{if }k\text{ is odd},
    \end{cases}
\end{equation*}
which simplifies the former equation. Optimal number of coefficients \(C_m\) is
determined by computing them sequentially and stopping when variances of both
fields become equal with desired accuracy \(\epsilon\):
\begin{equation}
    \label{eq-nit-error}
    \left| \Var{z} - \sum\limits_{k=0}^m
    \frac{C_k^2}{k!} \right| \leq \epsilon.
\end{equation}

In\nbsp{}cite:boukhanovsky1997thesis the author suggests using polynomial
approximation \(f(y)\) also for wavy surface transformation, however, in
practice sea surface realisation often contains points, where \(z\)-coordinate
is beyond the limits of the approximation, which makes solution invalid. In
these points it is more efficient to solve equation
eqref:eq-distribution-transformation by bisection method. Using the same
approximation in Gram---Charlier series does not lead to such errors.

* Numerical methods and experimental results
** The shape of ACF for different types of waves
:PROPERTIES:
:CUSTOM_ID: sec-wave-acfs
:END:
**** Analytic method of finding the ACF.
The straightforward way to find ACF for a given sea wave profile is to apply
Wiener---Khinchin theorem. According to this theorem the autocorrelation \(K\) of
a function \(\zeta\) is given by the Fourier transform of the absolute square of
the function:
\begin{equation}
  K(t) = \Fourier{\left| \zeta(t) \right|^2}.
  \label{eq-wiener-khinchin}
\end{equation}
When \(\zeta\) is replaced with actual wave profile, this formula gives you
analytic formula for the corresponding ACF.

For three-dimensional wave profile (2D in space and 1D in time) analytic formula
is a polynomial of high order and is best obtained via symbolic computation
programme. Then for practical usage it can be approximated by superposition of
exponentially decaying cosines (which is how ACF of a stationary ARMA process
looks like\nbsp{}cite:box1976time).

**** Empirical method of finding the ACF.
However, for three-dimensional case there exists simpler empirical method which
does not require sophisticated software to determine shape of the ACF. It is
known that ACF represented by exponentially decaying cosines satisfies first
order Stokes' equations for gravity waves\nbsp{}cite:boccotti1983wind. So, if the
shape of the wave profile is the only concern in the simulation, then one can
simply multiply it by a decaying exponent to get appropriate ACF. This ACF does
not reflect other wave profile parameters, such as wave height and period, but
opens possibility to simulate waves of a particular non-analytic shape by
"drawing" their profile, then multiplying it by an exponent and using the
resulting function as ACF. So, this empirical method is imprecise but offers
simpler alternative to Wiener---Khinchin theorem approach; it is mainly useful
to test ARMA model.

**** Standing wave ACF.
For three-dimensional plain standing wave the profile is given by
\begin{equation}
  \zeta(t, x, y) = A \sin (k_x x + k_y y) \sin (\sigma t).
  \label{eq-standing-wave}
\end{equation}
Find ACF via analytic method. Multiplying the formula by a decaying exponent
(because Fourier transform is defined for a function \(f\) that
\(f\underset{x\rightarrow\pm\infty}{\longrightarrow}0\)) yields
\begin{equation}
  \zeta(t, x, y) =
  A
  \exp\left[-\alpha (|t|+|x|+|y|) \right]
  \sin (k_x x + k_y y) \sin (\sigma t).
  \label{eq-decaying-standing-wave}
\end{equation}
Then, apply 3D Fourier transform to both sides of the equation via symbolic
computation programme, fit the resulting polynomial to the following
approximation:
\begin{equation}
  K(t,x,y) =
  \gamma
  \exp\left[-\alpha (|t|+|x|+|y|) \right]
  \cos \beta t
  \cos \left[ \beta x + \beta y \right].
  \label{eq-standing-wave-acf}
\end{equation}
So, after applying Wiener---Khinchin theorem we get initial formula but with
cosines instead of sines. This difference is important because the value of ACF
at \((0,0,0)\) equals to the ARMA process variance, and if one used sines the
value would be wrong.

If one tries to replicate the same formula via empirical method, the usual way
is to adapt eqref:eq-decaying-standing-wave to match eqref:eq-standing-wave-acf.
This can be done either by changing the phase of the sine, or by substituting
sine with cosine to move the maximum of the function to the origin of
coordinates.

**** Propagating wave ACF.
Three-dimensional profile of plain propagating wave is given by
\begin{equation}
  \zeta(t, x, y) = A \cos (\sigma t + k_x x + k_y y).
  \label{eq-propagating-wave}
\end{equation}
For the analytic method repeating steps from the previous two paragraphs yields
\begin{equation}
  K(t,x,y) =
  \gamma
  \exp\left[-\alpha (|t|+|x|+|y|) \right]
  \cos\left[\beta (t+x+y) \right].
  \label{eq-propagating-wave-acf}
\end{equation}
For the empirical method the wave profile is simply multiplied by a decaying
exponent without need to adapt the maximum value of ACF (as it is required for
standing wave).

**** Comparison of studied methods.
To summarise, the analytic method of finding sea wave's ACF reduces to the
following steps.
- Make wave profile decay when approaching \(\pm\infty\) by multiplying it by
  a decaying exponent.
- Apply Fourier transform to the absolute square of the resulting equation using
  symbolic computation programme.
- Fit the resulting polynomial to the appropriate ACF approximation.

Two examples in this section showed that in case of standing and propagating
waves their decaying profiles resemble the corresponding ACFs with the exception
that the ACF's maximum should be moved to the origin to preserve simulated
process variance. Empirical method of finding ACF reduces to the following
steps.
- Make wave profile decay when approaching \(\pm\infty\) by multiplying it by
  a decaying exponent.
- Move maximum value of the resulting function to the origin by using
  trigonometric identities to shift the phase.

** Additional formulae, methods and algorithms for ARMA model
:PROPERTIES:
:CUSTOM_ID: sec:arma-algorithms
:END:
*** Wave elevation distribution approximation
One of the parameters of sea wavy surface generator is probability density
function (PDF) of the surface elevation. This distribution is given by either
polynomial approximation of /in situ/ data or analytic formula.

**** Gram---Charlier series expansion.
In\nbsp{}cite:huang1980experimental the authors experimentally show, that PDF of sea
surface elevation is distinguished from normal distribution by non-nought
kurtosis and skewness. In\nbsp{}cite:1996 the authors show, that this type
of PDF expands in Gram---Charlier series:
\begin{align}
    \label{eq-skew-normal-1}
    & F(z; \mu=0, \sigma=1, \gamma_1, \gamma_2) \approx
    \Phi(z; \mu, \sigma) \frac{2 + \gamma_2}{2}
    - \frac{2}{3} \phi(z; \mu, \sigma)
    \left(\gamma_2 z^3+\gamma_1
    \left(2 z^2+1\right)\right) \nonumber
    \\
    & f(z; \mu, \sigma, \gamma_1, \gamma_2) \approx
    \phi(z; \mu, \sigma)
    \left[
        1+
        \frac{1}{3!} \gamma_1 H_3 \left(\frac{z-\mu}{\sigma}\right)
        + \frac{1}{4!} \gamma_2 H_4 \left(\frac{z-\mu}{\sigma}\right)
    \right],
\end{align}
where \(\Phi(z)\)\nbsp{}--- CDF of normal distribution, \(\phi\)\nbsp{}--- PDF
of normal distribution, \(\gamma_1\)\nbsp{}--- skewness, \(\gamma_2\)\nbsp{}---
kurtosis, \(f\)\nbsp{}--- PDF, \(F\)\nbsp{}--- cumulative distribution function
(CDF). According to\nbsp{}cite:1990 for sea waves skewness
is selected from interval \(0.1\leq\gamma_1\leq{0.52}]\) and kurtosis from
interval \(0.1\leq\gamma_2\leq{0.7}\). Family of probability density functions
for different parameters is shown in fig.\nbsp{}[[fig-skew-normal-1]].

#+NAME: fig-skew-normal-1
#+begin_src R :file build/skew-normal-1.pdf
source(file.path("R", "common.R"))
x <- seq(-3, 3, length.out=100)
params <- data.frame(
  skewness = c(0.00, 0.52, 0.00, 0.52),
  kurtosis = c(0.00, 0.00, 0.70, 0.70),
  linetypes = c("solid", "dashed", "dotdash", "dotted")
)
arma.skew_normal_1_plot(x, params)
legend(
  "topleft",
  mapply(
    function (s, k) {
      as.expression(bquote(list(
        gamma[1] == .(arma.fmt(s, 2)),
        gamma[2] == .(arma.fmt(k, 2))
      )))
    },
    params$skewness,
    params$kurtosis
  ),
  lty = paste(params$linetypes)
)
#+end_src

#+caption: Probability density function eqref:eq-skew-normal-1 of sea wavy surface elevation for different values of skewness \(\gamma_1\) and kurtosis \(\gamma_2\).
#+label: fig-skew-normal-1
#+RESULTS: fig-skew-normal-1
[[file:build/skew-normal-1.pdf]]

**** Skew-normal distribution.
Alternative approach is to approximate distribution of sea wavy surface
elevation by skew-normal distribution:
\begin{align}
    \label{eq-skew-normal-2}
    F(z; \alpha) & = \frac{1}{2}
   \mathrm{erfc}\left[-\frac{z}{\sqrt{2}}\right]-2 T(z,\alpha ), \nonumber \\
    f(z; \alpha) & = \frac{e^{-\frac{z^2}{2}}}{\sqrt{2 \pi }}
   \mathrm{erfc}\left[-\frac{\alpha z}{\sqrt{2}}\right],
\end{align}
where \(T\)\nbsp{}--- Owen \(T\)-function\nbsp{}cite:owen1956tables. Using this formula it is
impossible to specify skewness and kurtosis separately\nbsp{}--- both values are
adjusted via \(\alpha\) parameter. The only advantage of the formula is its
relative computational simplicity: this function is available in some programmes
and mathematical libraries. Its graph for different values of \(\alpha\) is shown
in fig.\nbsp{}[[fig-skew-normal-2]].

#+name: fig-skew-normal-2
#+begin_src R :file build/skew-normal-2.pdf
source(file.path("R", "common.R"))
x <- seq(-3, 3, length.out=100)
alpha <- c(0.00, 0.87, 2.25, 4.90)
params <- data.frame(
  alpha = alpha,
  skewness = arma.bits.skewness_2(alpha),
  kurtosis = arma.bits.kurtosis_2(alpha),
  linetypes = c("solid", "dashed", "dotdash", "dotted")
)
arma.skew_normal_2_plot(x, params)
legend(
  "topleft",
  mapply(
    function (a, s, k) {
      as.expression(bquote(list(
        alpha == .(arma.fmt(a, 2)),
        gamma[1] == .(arma.fmt(s, 2)),
        gamma[2] == .(arma.fmt(k, 2))
      )))
    },
    params$alpha,
    params$skewness,
    params$kurtosis
  ),
  lty = paste(params$linetypes)
)
#+end_src

#+caption: Probability density function eqref:eq-skew-normal-2 of sea wavy surface for different values of skewness coefficient \(\alpha\).
#+label: fig-skew-normal-2
#+RESULTS: fig-skew-normal-2
[[file:build/skew-normal-2.pdf]]

**** Evaluation.
Equation eqref:eq-distribution-transformation with selected wave elevation
distribution may be solved either in every point of generated wavy surface,
which gives the most accurate results, or in every fixed grid point
interpolating result via least-squares (LS) polynomial. In the second case
precision is lower. For example, interpolating 12^th order polynomial on a fixed
grid of 500 points on interval \(-5\sigma_z\leq{z}\leq{5}\sigma_z\) gives error of
\(\approx{0.43}\cdot10^{-3}\). Increasing polynomial order leads to either numeric
overflows during LS interpolation, or more coefficient close to nought;
increasing the size of the grid has insignificant effect on the result. In the
majority of cases three Gram---Charlier series coefficients is enough to
transform ACF; relative error without interpolation is \(10^{-5}\).

*** White noise generation algorithm
In order to eliminate periodicity from generated wavy surface, it is imperative
to use PRNG with sufficiently large period to generate white noise. Parallel
Mersenne Twister\nbsp{}cite:matsumoto1998mersenne with a period of \(2^{19937}-1\) is
used as a generator in this work. It allows to produce aperiodic sea wavy
surface realisations in any practical usage scenarios.

There is no guarantee that multiple Mersenne Twisters executed in parallel
threads with distinct initial states produce uncorrelated pseudo-random number
sequences, however, algorithm of dynamic creation of Mersenne Twisters\nbsp{}cite:matsumoto1998dynamic may be used to provide such guarantee. The essence of
the algorithm is to find matrices of initial generator states, that give
maximally uncorrelated pseudo-random number sequences when Mersenne Twisters are
executed in parallel with these initial states. Since finding such initial
states consumes considerable amount of processor time, vector of initial states
is created preliminary with knowingly larger number of parallel threads and
saved to a file, which is then read before starting white noise generation.

*** Wavy surface generation algorithm
In ARMA model value of wavy surface elevation at a particular point depends on
previous in space and time points, as a result the so called /ramp-up interval/
(see fig.\nbsp{}[[fig-ramp-up-interval]]), in which realisation does not correspond to
specified ACF, forms in the beginning of the realisation. There are several
solutions to this problem which depend on the simulation context.

If realisation is used in the context of ship stability simulation without
manoeuvring, ramp-up interval will not affect results of the simulation, because
it is located on the border (too far away from the studied marine object). If
ship stability with manoeuvring is studied, then the interval may be simply
discarded from the realisation (the size of the interval approximately equals
the number of AR coefficients in each dimension). However, this may lead to loss
of a very large number of points, because discarding occurs for each dimension.
Alternative approach is to generate sea wavy surface on ramp-up interval with
LH model and generate the rest of the realisation with ARMA model.

Algorithm of wavy surface generation is data-parallel: realisation is divided
into equal parts each of which is generated independently, however, in the
beginning of each realisation there is ramp-up interval. To eliminate it
/overlap-add/ method\nbsp{}cite:oppenheim1989discrete,svoboda2011efficient,pavel2013algorithms (a popular
method in signal processing) is used. The essence of the method is to add
another interval, size of which is equal to the ramp-up interval size, to the
end of each part. Then wavy surface is generated in each point of each part
(including points from the added interval), the interval at the end of part \(N\)
is superimposed on the ramp-up interval at the beginning of the part \(N+1\), and
values in corresponding points are added.

#+name: fig-ramp-up-interval
#+begin_src R :file build/ramp-up-interval.pdf
source(file.path("R", "common.R"))
arma.plot_ramp_up_interval()
#+end_src

#+caption: Ramp-up interval at the beginning of the \(OX\) axis of the realisation.
#+label: fig-ramp-up-interval
#+RESULTS: fig-ramp-up-interval
[[file:build/ramp-up-interval.pdf]]

*** Velocity potential normalisation formulae
:PROPERTIES:
:CUSTOM_ID: sec:compute-delta
:END:

In solutions eqref:eq-solution-2d and eqref:eq-solution-2d-full to
two-dimensional pressure determination problem there are functions
\(\Fun{z}=\InverseFourierY{e^{2\pi{u}{z}}}{x}\) and
\(\FunSecond{z}=\InverseFourierY{\Sinh{2\pi{u}{z}}}{x}\) which has multiple
analytic representations and are difficult to compute. Each function is a
Fourier transform of linear combination of exponents which reduces to poorly
defined Dirac delta function of a complex argument (see
table\nbsp{}[[tab-delta-functions]]). The usual way of handling this type of
functions is to write them as multiplication of Dirac delta functions of real
and imaginary part, however, this approach does not work here, because applying
inverse Fourier transform to this representation does not produce exponent,
which severely warp resulting velocity field. In order to get unique analytic
definition normalisation factor \(1/\Sinh{2\pi{u}{h}}\) (which is also included
in formula for \(E(u)\)) may be used. Despite the fact that normalisation allows
to obtain adequate velocity potential field, numerical experiments show that
there is little difference between this field and the one produced by formulae
from linear wave theory, in which terms with \(\zeta\) are omitted. As a result,
the formula for three-dimensional case was not derived.

#+name: tab-delta-functions
#+caption: Formulae for computing \(\Fun{z}\) and \(\FunSecond{z}\) from [[#sec:pressure-2d]], that use normalisation to eliminate uncertainty from definition of Dirac delta function of complex argument.
#+attr_latex: :booktabs t
| Function          | Without normalisation                                        | Normalised                                                                                                                             |
|-------------------+--------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------|
| \(\Fun{z}\)       | \(\delta (x+i z)\)                                           | \(\frac{1}{2 h}\mathrm{sech}\left(\frac{\pi  (x-i (h+z))}{2 h}\right)\)                                                                |
| \(\FunSecond{z}\) | \(\frac{1}{2}\left[\delta (x-i z) + \delta (x+i z) \right]\) | \(\frac{1}{4 h}\left[\text{sech}\left(\frac{\pi  (x-i (h+z))}{2 h}\right)+\text{sech}\left(\frac{\pi  (x+i(h+z))}{2 h}\right)\right]\) |

** ARMA model verification
:PROPERTIES:
:CUSTOM_ID: sec:verification
:END:

In\nbsp{}cite:degtyarev2011modelling,degtyarev2013synoptic,boukhanovsky1997thesis
AR model the following items are verified experimentally:
- probability distributions of different wave characteristics (wave heights,
  lengths, crests, periods, slopes, three-dimensionality),
- dispersion relation,
- retention of integral characteristics for mixed wave sea state.
In this work both AR and MA model are verified by comparing probability
distributions of different wave characteristics.

*** Verification of wavy surface integral characteristics
In\nbsp{}cite:1990 the authors show that several sea wave
characteristics (listed in table\nbsp{}[[tab-weibull-shape]]) have Weibull distribution,
and wavy surface elevation has Gaussian distribution. In order to verify that
distributions corresponding to generated realisation are correct,
quantile-quantile plots are used (plots where analytic quantile values are used
for \(OX\) axis and estimated quantile values for \(OY\) axis). If the estimated
distribution matches analytic then the graph has the form of the straight line.
Tails of the graph may diverge from the straight line, because they can not be
reliably estimated from the finite-size realisation. Different methods of
extracting waves from realisation produce variations in quantile function tails,
it is probably impractical to extract every possible wave from realisation since
they may (and often) overlap.

#+name: tab-weibull-shape
#+caption: Values of Weibull shape parameter for different wave characteristics.
#+attr_latex: :booktabs t
| Characteristic       | Weibull shape (\(k\)) |
|----------------------+-----------------------|
| Wave height          |                     2 |
| Wave length          |                   2.3 |
| Crest length         |                   2.3 |
| Wave period          |                     3 |
| Wave slope           |                   2.5 |
| Three-dimensionality |                   2.5 |

Verification was performed for standing and propagating waves. The corresponding
ACFs and quantile-quantile plots of wave characteristics distributions are shown
in
fig.\nbsp{}[[propagating-wave-distributions]],\nbsp{}[[standing-wave-distributions]],\nbsp{}[[acf-slices]].

#+name: propagating-wave-distributions
#+begin_src R :file build/propagating-wave-qqplots.pdf
source(file.path("R", "common.R"))
par(pty="s", mfrow=c(2, 2))
arma.qqplot_grid(
  file.path("build", "propagating_wave"),
  c("elevation", "heights_y", "lengths_y", "periods"),
  c("elevation", "height Y", "length Y", "period"),
  xlab="x",
  ylab="y"
)
#+end_src

#+caption: Quantile-quantile plots for propagating waves.
#+label: propagating-wave-distributions
#+RESULTS: propagating-wave-distributions
[[file:build/propagating-wave-qqplots.pdf]]

#+name: standing-wave-distributions
#+begin_src R :file build/standing-wave-qqplots.pdf
source(file.path("R", "common.R"))
par(pty="s", mfrow=c(2, 2))
arma.qqplot_grid(
  file.path("build", "standing_wave"),
  c("elevation", "heights_y", "lengths_y", "periods"),
  c("elevation", "height Y", "length Y", "period"),
  xlab="x",
  ylab="y"
)
#+end_src

#+caption: Quantile-quantile plots for standing waves.
#+label: standing-wave-distributions
#+RESULTS: standing-wave-distributions
[[file:build/standing-wave-qqplots.pdf]]

#+name: acf-slices
#+header: :width 6 :height 9
#+begin_src R :file build/acf-slices.pdf
source(file.path("R", "common.R"))
propagating_acf <- read.csv(file.path("build", "propagating_wave", "acf.csv"))
standing_acf <- read.csv(file.path("build", "standing_wave", "acf.csv"))
par(mfrow=c(5, 2), mar=c(0,0,0,0))
for (i in seq(0, 4)) {
  arma.wavy_plot(standing_acf, i, zlim=c(-5,5))
  arma.wavy_plot(propagating_acf, i, zlim=c(-5,5))
}
#+end_src

#+caption: Time slices of ACF for standing (left column) and propagating waves (right column).
#+label: acf-slices
#+RESULTS: acf-slices
[[file:build/acf-slices.pdf]]

Graph tails in fig.\nbsp{}[[propagating-wave-distributions]] deviate from original
distribution for individual wave characteristics, because every wave have to be
extracted from the resulting wavy surface to measure its length, period and
height. There is no algorithm that guarantees correct extraction of all waves,
because they may and often overlap each other. Weibull distribution right tail
represents infrequently occurring waves, so it deviates more than left tail.

Correspondence rate for standing waves (fig.\nbsp{}[[standing-wave-distributions]])
is lower for height and length, roughly the same for surface
elevation and higher for wave period distribution tails. Lower correspondence
degree for length and height may be attributed to the fact that Weibull
distributions were obtained empirically for sea waves which are typically
propagating, and distributions may be different for standings waves. Higher
correspondence degree for wave periods is attributed to the fact that wave
periods of standing waves are extracted more precisely as the waves do not move
outside simulated wavy surface region. The same correspondence degree for wave elevation
is obtained, because this is the characteristic of the wavy surface (and
corresponding AR or MA process) and is not affected by the type of waves.
*** Verification of velocity potential fields
:PROPERTIES:
:CUSTOM_ID: sec:compare-formulae
:END:

Comparing obtained generic formulae eqref:eq-solution-2d and
eqref:eq-solution-2d-full to the known formulae from linear wave theory allows
to see the difference between velocity fields for both large and small amplitude
waves. In general analytic formula for velocity potential in not known, even for
plain waves, so comparison is done numerically. Taking into account conclusions
of [[#sec:pressure-2d]], only finite depth formulae are compared.

**** The difference with linear wave theory formulae.
In order to obtain velocity potential fields, sea wavy surface was generated
by AR model with varying wave amplitude. In numerical implementation wave
numbers in Fourier transforms were chosen on the interval from \(0\) to the
maximal wave number determined numerically from the obtained wavy surface.
Experiments were conducted for waves of both small and large amplitudes.

The experiment showed that velocity potential fields produced by formula
eqref:eq-solution-2d-full for finite depth fluid and formula
eqref:eq-solution-2d-linear from linear wave theory are qualitatively different
(fig.\nbsp{}[[fig-potential-field-nonlinear]]). First, velocity potential contours
have sinusoidal shape, which is different from oval shape described by linear
wave theory. Second, velocity potential decays more rapidly than in linear wave
theory as getting closer to the bottom, and the region where the majority of
wave energy is concentrated is closer to the wave crest. Similar numerical
experiment, in which all terms of eqref:eq-solution-2d-full that are neglected
in the framework of linear wave theory are eliminated, shows no difference (as
much as machine precision allows) in resulting velocity potential fields.

#+name: fig-potential-field-nonlinear
#+header: :width 8 :height 11
#+begin_src R :file build/plain-wave-velocity-field-comparison.pdf
source(file.path("R", "velocity-potentials.R"))
par(pty="s")
nlevels <- 41
levels <- pretty(c(-200,200), nlevels)
palette <- colorRampPalette(c("blue", "lightyellow", "red"))
col <- palette(nlevels-1)

# linear solver
par(fig=c(0,0.95,0,0.5),new=TRUE)
arma.plot_velocity_potential_field(
  file.path("build", "plain_wave_linear_solver"),
  levels=levels,
  col=col
)

# high-amplitude solver
par(fig=c(0,0.95,0.5,1),new=TRUE)
arma.plot_velocity_potential_field(
  file.path("build", "plain_wave_high_amplitude_solver"),
  levels=levels,
  col=col
)

# legend 1
par(pty="m",fig=c(0.80,1,0.5,1), new=TRUE)
arma.plot_velocity_potential_field_legend(
  levels=levels,
  col=col
)

# legend 2
par(pty="m",fig=c(0.80,1,0,0.5), new=TRUE)
arma.plot_velocity_potential_field_legend(
  levels=levels,
  col=col
)
#+end_src

#+caption: Velocity potential field of propagating wave \(\zeta(x,y,t) = \cos(2\pi x - t/2)\). Field produced by formula eqref:eq-solution-2d-full (top) and linear wave theory formula (bottom).
#+label: fig-potential-field-nonlinear
#+attr_latex: :width \textwidth
#+RESULTS: fig-potential-field-nonlinear
[[file:build/plain-wave-velocity-field-comparison.pdf]]

**** The difference with small-amplitude wave theory.
The experiment, in which velocity fields produced numerically by different
formulae were compared, shows that velocity fields produced by formula
eqref:eq-solution-2d-full and eqref:eq-old-sol-2d correspond to each other for
small-amplitude waves. Two sea wavy surface realisations were made by AR
model: one containing small-amplitude waves, other containing large-amplitude
waves. Integration in formula eqref:eq-solution-2d-full was done over wave
numbers range extracted from the generated wavy surface. For small-amplitude
waves both formulae showed comparable results (the difference in the velocity is
attributed to the stochastic nature of AR model), whereas for large-amplitude
waves stable velocity field was produced only by formula
eqref:eq-solution-2d-full (fig.\nbsp{}[[fig-velocity-field-2d]]). So, generic
formula eqref:eq-solution-2d-full gives satisfactory results without restriction
on wave amplitudes.

#+name: fig-velocity-field-2d
#+header: :width 8 :height 11
#+begin_src R :file build/large-and-small-amplitude-velocity-field-comparison.pdf
source(file.path("R", "velocity-potentials.R"))
linetypes = c("solid", "dashed")
par(mfrow=c(2, 1))
arma.plot_velocity(
  file.path("data", "velocity", "low-amp"),
  file.path("data", "velocity", "low-amp-0"),
  linetypes=linetypes,
  ylim=c(-2,2)
)
arma.plot_velocity(
  file.path("data", "velocity", "high-amp"),
  file.path("data", "velocity", "high-amp-0"),
  linetypes=linetypes,
  ylim=c(-2,2)
)
#+end_src

#+label: fig-velocity-field-2d
#+caption: Comparison of velocity field on the sea wavy surface obtained by generic formula (\(u_1\)) and formula for small-amplitude waves (\(u_2\)). Velocity field for realisations containing small-amplitude (top) and large-amplitude (bottom) waves.
#+attr_latex: :width \textwidth
#+RESULTS: fig-velocity-field-2d
[[file:build/large-and-small-amplitude-velocity-field-comparison.pdf]]

*** Verification of nonlinear inertialess transformation
In order to measure the effect of NIT on the shape of the resulting wavy
surface, three realisations were generated:
- realisation with Gaussian distribution (without NIT),
- realisation with Gram---Charlier series (GCS) based distribution, and
- realisation with skew normal distribution.
The seed of PRNG was set to be the same for all progrmme executions to make ARMA
model produce the same values for each realisation. There we two experiments:
for standing and propagating waves with ACFs given by formulae from
section\nbsp{}[[#sec-wave-acfs]].

The results of the experiments are twofold: while the experiment showed that
applying NIT with GCS-based distribution increases wave steepness, the same is
not true for skew normal distribution (fig.\nbsp{}[[fig-nit]]). Using this
distribution results in wavy surface each \(z\)-coordinate of which is always
greater or equal to nought. So, skew normal distribution is unsuitable for NIT.
NIT increases the wave height and steepness of both standing and propagating
waves. Increasing either skewness or kurtosis parameter of GCS-based
distribution increases both wave height and steepness. The error of ACF
approximation (eq.\nbsp{}eqref:eq-nit-error) ranges from 0.20 for GCS-based
distribution to 0.70 for skew normal distribution (table\nbsp{}[[tab-nit-error]]).

#+name: fig-nit
#+header: :width 7 :height 7
#+begin_src R :file build/nit.pdf
source(file.path("R", "nonlinear.R"))
par(mfrow=c(2, 1), mar=c(4,4,4,0.5), family='serif')
args <- list(
  graphs=c('Gaussian', 'GramCharlier', 'Skew normal'),
  linetypes=c('solid', 'dashed', 'dotted')
)
args$title <- 'Propagating waves'
arma.plot_nonlinear(file.path("build", "nit-propagating"), args)
args$title <- 'Standing waves'
arma.plot_nonlinear(file.path("build", "nit-standing"), args)
#+end_src

#+label: fig-nit
#+caption: Wavy surface slices with different distributions of wave elevation (Gaussian, Gram---Charlier series based and skew normal).
#+RESULTS: fig-nit
[[file:build/nit.pdf]]

#+name: tab-nit-error
#+caption: Errors of ACF approximations (the difference of variances) for different wave elevation distributions. \(N\)\nbsp{}--- number of coefficients of ACF approximation.
#+attr_latex: :booktabs t
| Wave type   | Distribution | \(\gamma_1\) | \(\gamma_2\) | \(\alpha\) | Error | \(N\) | Wave height |
|-------------+--------------+--------------+--------------+------------+-------+-------+-------------|
| propagating | Gaussian     |              |              |            |       |       |        2.41 |
| propagating | GCS-based    |         2.25 |          0.4 |            |  0.20 |     2 |        2.75 |
| propagating | skew normal  |              |              |          1 |  0.70 |     3 |        1.37 |
| standing    | Gaussian     |              |              |            |       |       |        1.73 |
| standing    | GCS-based    |         2.25 |          0.4 |            |  0.26 |     2 |        1.96 |
| standing    | skew normal  |              |              |          1 |  0.70 |     3 |        0.94 |

To summarise, the only test case that showed acceptable results is realisation
with GCS-based distribution for both standing and propagating waves. Skew normal
distribution warps wavy surface for both types of waves. GCS-based realisations
have large error of ACF approximation, which results in increase of wave height.
The reason for the large error is that approximations Gram---Charlier series are
not accurate as they do not converge for all possible
functions\nbsp{}cite:wallace1958asymptotic. Despite the large error, the change
in wave height is small (table\nbsp{}[[tab-nit-error]]).

**** Wave height                                                :noexport:
:PROPERTIES:
:header-args:R: :results output org
:END:

#+header: 
#+begin_src R :results output org
source(file.path("R", "nonlinear.R"))
propagating <- arma.print_wave_height(file.path("build", "nit-propagating"))
standing <- arma.print_wave_height(file.path("build", "nit-standing"))
result <- data.frame(
  h1=c(propagating$h1, standing$h1),
  h2=c(propagating$h2, standing$h2),
  h3=c(propagating$h3, standing$h3)
)
rownames(result) <- c('propagating', 'standing')
colnames(result) <- c('none', 'gcs', 'sn')
ascii(result)
#+end_src

#+RESULTS:
#+BEGIN_SRC org
|             | none |  gcs |   sn |
|-------------+------+------+------|
| propagating | 2.41 | 2.75 | 1.37 |
| standing    | 1.73 | 1.96 | 0.94 |
#+END_SRC

*** Non-physical nature of ARMA model
ARMA model, owing to its non-physical nature, does not have the notion of sea
wave; it simulates wavy surface as a whole instead. Motions of individual waves
and their shape are often rough, and the total number of waves can not be
determined precisely. However, integral characteristics of wavy surface match
the ones of real sea waves.

Theoretically, sea waves themselves can be chosen as ACFs, the only
pre-processing step is to make them decay exponentially. This may allow
to generate waves of arbitrary profiles, and is one of the directions of future
work.

* High-performance software implementation of sea wave simulation
** Computational model
**** Mapping wavy surface generation algorithm on computational model.
Software implementation of ARMA model works as a computational pipeline, in
which each joint applies some function to the output coming from the pipe of the
previous joint. Joints are distributed across computer cluster nodes to enable
function parallelism, and then data flowing through the joints is distributed
across processor cores to enable data parallelism. Figure\nbsp{}[[fig-pipeline]]
shows a diagram of data processing pipeline in which rectangles with rounded
corners denote joints, regular rectangles denote arrays of problem domain
objects flowing from one joint to another, and arrows show flow direction. Some
joints are divided into /sections/ each of which process a separate part of the
array. If joints are connected without a /barrier/ (horizontal or vertical bar),
then transfer of separate objects between them is done in parallel to
computations, as they become available. Sections work in parallel on each
processor core (or node of the cluster). There is surjective mapping between a
set of processor cores, a set of pipeline joint sections and objects, i.e. each
processor core may run several sections, each of which may sequentially process
several objects, but a section can not work simultaneously on several processor
cores, and an object can not be processed simultaneously by several sections.
So, the programme is a pipeline through which control objects flow.

#+name: fig-pipeline
#+begin_src dot :exports results :file build/pipeline.pdf
digraph {

  node [fontsize=14,margin="0.055,0"]
  graph [nodesep="0.25",ranksep="0.25",rankdir="TB"]
  edge [arrowsize=0.66]

  # data
  subgraph xcluster_linear {
    label="Linear model"

    start [label="",shape=circle,style=filled,fillcolor=black,width=0.23]
    spectrum [label="S(,)",shape=box]
    acf [label="K(i,j,k)",shape=box]
    phi [label="(i,j,k)",shape=box]

    # transformations
    fourier_transform [label="Fourier transform",shape=box,style=rounded]
    solve_yule_walker [label="Solve YuleWalker\nequations",shape=box,style=rounded]

    subgraph cluster_nonlinear_1 {
      label="Simulate non-linearity\l"
      labeljust=left
      style=filled
      color=lightgrey
      acf2 [label="K*(i,j,k)",shape=box]
      transform_acf [label="Transform ACF",shape=box,style=rounded]
    }
  }

  subgraph xcluster_linear2 {

    eps_parts [label="<e1> |<e2> |<e3> |<e4> |<e> (t,x,y)",shape=record]
    end [label="",shape=doublecircle,style=filled,fillcolor=black,width=0.23]

    generate_white_noise [label="<g1> g|<g2> g|<g3> |<g4> g|<gen> Generate\lwhitenoise",shape=record,style=rounded]
    generate_zeta [label="<g1> g|<g2> g|<g3> |<g4> g|<gen> Generatesea\lwavy surface parts\l",shape=record,style=rounded]

    zeta_parts [label="<g1> |<g2> |<g3> |<g4> |<gen> Non-crosslinked\lrealisationparts",shape=record]
    overlap_add [label="<g1> |<g2> |<g3> |<g4> |<gen> Crosslinkrealisation\lparts\l",shape=record,style=rounded]

    zeta_parts:g1->overlap_add:g1
    zeta_parts:g2->overlap_add:g2
    zeta_parts:g3->overlap_add:g3
    zeta_parts:g4->overlap_add:g4

    zeta_parts:g2->overlap_add:g1 [constraint=false]
    zeta_parts:g3->overlap_add:g2 [constraint=false]
    zeta_parts:g4->overlap_add:g3 [constraint=false]

    overlap_add:g1->zeta2_parts:g1
    overlap_add:g2->zeta2_parts:g2
    overlap_add:g3->zeta2_parts:g3
    overlap_add:g4->zeta2_parts:g4

    zeta2_parts:g1->transform_zeta:g1->zeta3_parts:g1->write_zeta:g1->eps_end
    zeta2_parts:g2->transform_zeta:g2->zeta3_parts:g2->write_zeta:g2->eps_end
    zeta2_parts:g3->transform_zeta:g3->zeta3_parts:g3->write_zeta:g3->eps_end
    zeta2_parts:g4->transform_zeta:g4->zeta3_parts:g4->write_zeta:g4->eps_end

  }

  subgraph part3 {

    zeta2_parts [label="<g1> |<g2> |<g3> |<g4> |<gen> Wavysurfacewith\lGaussiandistribution\l",shape=record]

    subgraph cluster_nonlinear_2 {
      label="Simulate non-linearity\r"
      labeljust=right
      style=filled
      color=lightgrey
      zeta3_parts [label="<g1> |<g2> |<g3> |<g4> |<gen> (t,x,y)",shape=record]
      transform_zeta [label="<g1> g|<g2> g|<g3> |<g4> g|<gen> Transformwavy\lsurface elevation\lprobability distribution\l",shape=record,style=rounded]
    }

    # barriers
    eps_start [label="",shape=box,style=filled,fillcolor=black,height=0.05]
    eps_end [label="",shape=box,style=filled,fillcolor=black,height=0.05]

    write_zeta [label="<g1> g|<g2> g|<g3> |<g4> g|<gen> Writefinished\lpartstoafile\l",shape=record,style=rounded]
  }

  # edges
  start->spectrum->fourier_transform->acf->transform_acf
  transform_acf->acf2
  acf2->solve_yule_walker
  solve_yule_walker->phi
  phi->eps_start [constraint=false]
  eps_start->generate_white_noise:g1
  eps_start->generate_white_noise:g2
  eps_start->generate_white_noise:g3
  eps_start->generate_white_noise:g4
  generate_white_noise:g1->eps_parts:e1->generate_zeta:g1->zeta_parts:g1
  generate_white_noise:g2->eps_parts:e2->generate_zeta:g2->zeta_parts:g2
  generate_white_noise:g3->eps_parts:e3->generate_zeta:g3->zeta_parts:g3
  generate_white_noise:g4->eps_parts:e4->generate_zeta:g4->zeta_parts:g4

  eps_end->end
}
#+end_src

#+caption: Diagram of data processing pipeline, that implements sea wavy surface generation via AR model.
#+label: fig-pipeline
#+RESULTS: fig-pipeline
[[file:build/pipeline.pdf]]

Object pipeline may be seen as an improvement of BSP (Bulk Synchronous Parallel)
model\nbsp{}cite:valiant1990bridging, which is used in graph
processing\nbsp{}cite:malewicz2010pregel,seo2010hama. Pipeline eliminates global
synchronisation (where it is possible) after each sequential computation step by
doing data transfer between joints in parallel to computations, whereas in BSP
model global synchronisation occurs after each step.

Object pipeline speeds up the programme by parallel execution of code blocks
that work with different compute devices: while the current part of wavy surface
is generated by a processor, the previous part is written to a disk. This
approach allows to get speed-up because compute devices operate asynchronously,
and their parallel usage increases the whole programme performance.

Since data transfer between pipeline joints is done in parallel to computations,
the same pipeline may be used to run several copies of the application but with
different parameters (generate several sea wavy surfaces having different
characteristics). In practise, high-performance applications do not always
consume 100% of processor time spending a portion of time on synchronisation of
parallel processes and writing data to disk. Using pipeline in this case allows
to run several computations on the same set of processes, and use all of the
computer devices at maximal efficiency. For example, when one object writes data
to a file, the other do computations on the processor in parallel. This
minimises downtime of the processor and other computer devices and increases
throughput of the computer cluster.

Pipelining of otherwise sequential steps is beneficial not only for code working
with different devices, but for code different branches of which are suitable
for execution by multiple hardware threads of the same processor core,
i.e.\nbsp{}branches accessing different memory blocks or performing mixed
arithmetic (integer and floating point). Code branches which use different
modules of processor are good candidates to run in parallel on a processor core
with multiple hardware threads.

So, computational model with a pipeline can be seen as /bulk-asynchronous
model/, because of the parallel nature of programme steps. This model is the
basis of the fault-tolerance model which will be described later.

**** Software implementation.
For efficiency reasons object pipeline and fault tolerance techniques (which
will be described later) are implemented in the C++ framework: From the author's
perspective C language is deemed low-level for distributed programmes, and Java
incurs too much overhead and is not popular in HPC community. As of now, the
framework runs in the same process as an parallel application that uses it. The
framework is called Factory, it is now in proof-of-concept development stage.

**** Computational model overview.
The key feature that is missing in the current parallel programming technologies
is a possibility to specify hierarchical dependencies between parallel tasks.
When one has such dependency, it is trivial to determine which task should be
responsible for re-executing a failed task on one of the survived nodes. To
re-execute the task on the top of the hierarchy, a backup task is created and
executed on a different node. There exists a number of systems that are capable
of executing directed acyclic graphs of tasks in parallel\nbsp{}cite:acun2014charmpp,islam2012oozie, but graphs are not suitable to infer
principal-subordinate relationship between tasks, because a node in the graph
may have multiple parent nodes.

The main purpose of the model is to simplify development of distributed batch
processing applications and middleware. The main focus is to make application
resilient to failures, i.e. make it fault tolerant and highly available, and do
it transparently to a programmer. The implementation is divided into two layers:
the lower layer consists of routines and classes for single node applications
(with no network interactions), and the upper layer for applications that run on
an arbitrary number of nodes. There are two kinds of tightly coupled entities in
the model\nbsp{}--- /control flow objects/ (or /kernels/ for short) and
/pipelines/\nbsp{}--- which are used together to compose a programme.

Kernels implement control flow logic in theirs ~act~ and ~react~ methods and
store the state of the current control flow branch. Both logic and state are
implemented by a programmer. In ~act~ method some function is either directly
computed or decomposed into nested functions (represented by a set of
subordinate kernels) which are subsequently sent to a pipeline. In ~react~
method subordinate kernels that returned from the pipeline are processed by
their parent. Calls to ~act~ and ~react~ methods are asynchronous and are made
within threads attached to a pipeline. For each kernel ~act~ is called only
once, and for multiple kernels the calls are done in parallel to each other,
whereas ~react~ method is called once for each subordinate kernel, and all the
calls are made in the same thread to prevent race conditions (for different
parent kernels different threads may be used).

Pipelines implement asynchronous calls to ~act~ and ~react~, and try to make as
many parallel calls as possible considering concurrency of the platform (no. of
cores per node and no. of nodes in a cluster). A pipeline consists of a kernel
pool, which contains all the subordinate kernels sent by their parents, and a
thread pool that processes kernels in accordance with rules outlined in the
previous paragraph. A separate pipeline is used for each device: There are
pipelines for parallel processing, schedule-based processing (periodic and
delayed tasks), and a proxy pipeline for processing of kernels on other cluster
nodes (see fig.\nbsp{}[[fig-subord-ppl]]).

In principle, kernels and pipelines machinery reflect the one of procedures and
call stacks, with the advantage that kernel methods are called asynchronously
and in parallel to each other (as much as programme logic allows). Kernel field
is the stack, ~act~ method is a sequence of processor instructions before nested
procedure call, and ~react~ method is a sequence of processor instructions after
the call. Constructing and sending subordinate kernels to the pipeline is nested
procedure call. Two methods are necessary to make calls asynchronous, and
replace active wait for completion of subordinate kernels with passive one.
Pipelines, in turn, allow to implement passive wait, and call correct kernel
methods by analysing their internal state.

#+name: fig-subord-ppl
#+begin_src dot :exports results :file build/subord-ppl.pdf
graph G {

  node [fontname="Old Standard",fontsize=14,margin="0.055,0",shape=box]
  graph [nodesep="0.25",ranksep="0.25",rankdir="LR"]
  edge [arrowsize=0.66]

  subgraph cluster_daemon {
    label="Daemon process"
    style=filled
    color=lightgrey

    factory [label="Factory"]
    parallel_ppl [label="Parallel\npipeline"]
    io_ppl [label="I/O\npipeline"]
    sched_ppl [label="Schedule-based\npipeline"]
    net_ppl [label="Network\npipeline"]
    proc_ppl [label="Process\npipeline"]

    upstream [label="Upstream\nthread pool"]
    downstream [label="Downstream\nthread pool"]
  }

  factory--parallel_ppl
  factory--io_ppl
  factory--sched_ppl
  factory--net_ppl
  factory--proc_ppl

  subgraph cluster_hardware {
    label="Compute devices"
    style=filled
    color=lightgrey

    cpu [label="CPU"]
    core0 [label="Core 0"]
    core1 [label="Core 1"]
    core2 [label="Core 2"]
    core3 [label="Core 3"]

    storage [label="Storage"]
    disk0 [label="Disk 0"]

    network [label="Network"]
    nic0 [label="NIC 0"]

    timer [label="Timer"]

  }

  core0--cpu
  core1--cpu
  core2--cpu
  core3--cpu

  disk0--storage
  nic0--network

  parallel_ppl--upstream
  parallel_ppl--downstream

  upstream--{core0,core1,core2,core3} [style="dashed"]
  downstream--core0 [style="dashed"]

  io_ppl--core0 [style="dashed"]
  io_ppl--disk0 [style="dashed"]
  sched_ppl--core0 [style="dashed"]
  sched_ppl--timer [style="dashed"]
  net_ppl--core0 [style="dashed"]
  net_ppl--nic0 [style="dashed"]
  proc_ppl--core0 [style="dashed"]

  subgraph cluster_children {
    style=filled
    color=white

    subgraph cluster_child0 {
      label="Child process 0"
      style=filled
      color=lightgrey
      labeljust=right

      app0_factory [label="Factory"]
      app0 [label="Child process\rpipeline"]
    }

#    subgraph cluster_child1 {
#      label="Child process 1"
#      style=filled
#      color=lightgrey
#      labeljust=right
#
#      app1_factory [label="Factory"]
#      app1 [label="Child process\rpipeline"]
#    }
  }

  proc_ppl--app0
#  proc_ppl--app1

  app0_factory--app0 [constraint=false]
#  app1_factory--app1 [constraint=false]

}
#+end_src

#+caption: Mapping of parent and child process pipelines to compute devices. Solid lines denote aggregation, dashed lines denote mapping between logical and physical entities.
#+attr_latex: :width \textwidth
#+label: fig-subord-ppl
#+RESULTS: fig-subord-ppl
[[file:build/subord-ppl.pdf]]
**** Governing principles.
Data processing pipeline model is based on the following principles, following
which maximises efficiency of a programme.
- There is no notion of a message in the model, a kernel is itself a message
  that can be sent over network to another node and directly access any kernel
  on the local node. Only programme logic may guarantee the existence of the
  kernel.
- A kernel is a /cooperative routine/, which is submitted to kernel pool upon the
  call and is executed asynchronously by a scheduler. There can be any number of
  calls to other subroutines inside routine body. Every call submits
  corresponding subroutine to kernel pool and returns immediately. Kernels in the
  pool can be executed in any order; this fact is used by a scheduler to exploit
  parallelism offered by the computer by distributing kernels from the pool
  across available cluster nodes and processor cores.
- Asynchronous execution prevents the use of explicit synchronisation after the
  call to subroutine is made; system scheduler returns control flow to the
  routine each time one of its subroutine returns. Such cooperation transforms
  each routine which calls subroutines into event handler, where each event is a
  subroutine and the handler is the routine that called them.
- The routine may communicate with any number of local kernels, addresses of
  which it knows; communication with kernels which are not adjacent in the call
  stack complexifies control flow and call stack looses its tree shape. Only
  programme logic may guarantee presence of communicating kernels in memory. One
  way to ensure this is to perform communication between subroutines which are
  called from the same routine. Since such communication is possible within
  hierarchy through parent routine, it may treated as an optimisation that
  eliminates overhead of transferring data over intermediate node. The situation
  is different for interactive or event-based programmes (e.g. servers and
  programmes with graphical interface) in which this is primary type of
  communication.
- In addition to this, communication which does not occur along hierarchical
  links and executed over cluster network complexify design of resiliency
  algorithms. Since it is impossible to ensure that a kernel resides in memory
  of a neighbour node, because a node may fail in the middle of its execution of
  the corresponding routine. As a result, upon failure of a routine all of its
  subroutines must be restarted. This encourages a programmer to construct
  - deep tree hierarchies of tightly-coupled kernels (which communicate on the
    same level of hierarchy) to reduce overhead of recomputation;
  - fat tree hierarchies of loosely-coupled kernels, providing maximal degree of
    parallelism.
  Deep hierarchy is not only requirement of technology, it helps optimise
  communication of large number of cluster nodes reducing it to communication of
  adjacent nodes.

So, control flow objects (or kernels) possess properties of both cooperative
routines and event handlers.
**** Definitions of hierarchies
To disambiguate hierarchical links between daemon processes and kernels and to
simplify the discussion, we will use the following naming conventions throughout
the text. If the link is between two daemon processes, the relationship is
denoted as /master-slave/. If the link is between two kernels, then the
relationship is denoted as either /principal-subordinate/ or /parent-child/. Two
hierarchies are orthogonal to each other in a sense that no kernel may have a
link to a daemon, and vice versa. Since daemon hierarchy is used to distribute
the load on cluster nodes, kernel hierarchy is mapped onto it, and this mapping
can be arbitrary: It is common to have principal kernel on a slave node with its
subordinate kernels distributed evenly between all cluster nodes (including the
node where the principal is located). Both hierarchies can be arbitrarily deep,
but "shallow" ones are preferred for highly parallel programmes, as there are
less number of hops when kernels are distributed between cluster nodes. Since
there is one-to-one correspondence between daemons and cluster nodes, they are
used interchangeably in the work.

**** Kernel structure and algorithms
Each kernel has four types of fields (listed in table\nbsp{}[[tab-kernel-fields]]):
- fields related to control flow,
- fields defining the source location of the kernel,
- fields defining the current location of the kernel, and
- fields defining the target location of the kernel.

#+name: tab-kernel-fields
#+caption: Description of kernel fields.
#+attr_latex: :booktabs t :align lp{0.7\textwidth}
| Field             | Description                                                                                    |
|-------------------+------------------------------------------------------------------------------------------------|
| ~process_id~      | Identifier of a cluster-wide process (application) a kernel belongs to.                        |
| ~id~              | Identifier of a kernel within a process.                                                       |
| ~pipeline~        | Identifier of a pipeline a kernel is processed by.                                             |
|                   |                                                                                                |
| ~exit_code~       | A result of a kernel execution.                                                                |
| ~flags~           | Auxiliary bit flags used in scheduling.                                                        |
| ~time_point~      | A time point at which a kernel is scheduled to be executed.                                    |
|                   |                                                                                                |
| ~parent~          | Address/identifier of a parent kernel.                                                         |
| ~src_ip~          | IP-address of a source cluster node.                                                           |
| ~from_process_id~ | Identifier of a cluster-wide process from which a kernel originated.                           |
|                   |                                                                                                |
| ~principal~       | Address/identifier of a target kernel (a kernel to which the current one is sent or returned). |
| ~dst_ip~          | IP-address of a destination cluster node.                                                      |

Upon creation each kernel is assigned a parent and a pipeline. If there no other
fields are set, then the kernel is an /upstream/ kernel\nbsp{}--- a kernel that
can be distributed on any node and any processor core to exploit parallelism. If
principal field is set, then the kernel is a /downstream/ kernel\nbsp{}--- a
kernel that can only be sent to its principal, and a processor core to which the
kernel is sent is defined by the principal memory address/identifier. If a
downstream kernel is to be sent to another node, the destination IP-address must
be set, otherwise the system will not find the target kernel.

When kernel execution completes (its ~act~ method finishes), the kernel is
explicitly sent to some other kernel, this directive is explicitly called inside
~act~ method. Usually, after the execution completes a kernel is sent to its
parent by setting principal field to the address/identifier of the parent,
destination IP-address field to the source IP-address, and process identifier to
the source process identifier. After that kernel becomes a downstream kernel and
is sent by the system to the node, where its current principal is located
without invoking load balancing algorithm. For downstream kernel ~react~ method
of its parent is called by a pipeline with the kernel as the method argument to
make it possible for a parent to collect the result of the execution.

There is no way to provide fine-grained resilience to cluster node failures, if
there are downstream kernels in the programme, except the ones returning to
their parents. Instead, an exit code of the kernel is checked and a custom
recovery code is executed. If there is no error checking, the system restarts
execution from the first parent kernel, which did not produce any downstream
kernels. This means, that if a problem being solved by the programme has
information dependencies between parts that are computed in parallel, and a node
failure occurs during computation of these parts, then this computation is
restarted from the very beginning, discarding any already computed parts. This
does not occur for embarrassingly parallel programmes, where parallel parts do
not have such information dependencies between each other: in this case only
parts from failed nodes are recomputed and all previously computed parts are
retained.

** SMP implementation
**** Load balancing algorithm.
The simplest approach to balance the load on a multi-processor system is to
split data into equal parts (or a task into homogeneous subtasks) and to
distribute them evenly between processor cores and cluster nodes, however, this
approach does not work efficiently in all cases. First, the total number of
parts, into which input data is split, is often dictated by the problem being
solved, rather than computer system architecture. Such load balancing may not
efficient from the computer system point of view: the number of parts is either
too large compared to the number of processors working in parallel, which
increases data transfer overhead, or too small, which prevents using all
available processor cores. Second, restrictions of problem being solved may not
allow to split input data into even parts which may result in load imbalance
across processor cores. Third, there are multiple components in the system aside
from the processor that take part in the computation (such as vector
co-processors and storage devices), and the problem solution time depends on the
performance of all the components involved. So, how to make load balancing
algorithm more efficient in the presence of non-homogeneous input data parts and
take into account all the devices involved in the computation?

The load balancing algorithm consists of two stages. In the first stage, the
algorithm places input data part (or a subtask) wrapped in a kernel into an
appropriate kernel pool: there is a separate pool for each device and an
associated thread pool. In the second stage, a kernel is retrieved from the pool
by one of the threads and processed. Due to separate thread pools all devices
work in parallel to each other, lowering overall system resources downtime
compared to using all devices from a single thread.

In order to take into account non-homogeneous input data parts or tasks, one may
predict execution time of each task. Relevant study is done
in\nbsp{}cite:degtyarev2016balance since ARMA model implementation includes
mostly homogeneous tasks.

So, load balancing is done in two stages: in the first stage the task wrapped in
the kernel is routed to the appropriate device and in the second stage the
kernel is routed to one of the thread from the device thread pool.
Non-homogeneous kernels may be handled by predicting their execution time, but
such kernels are not present in ARMA model implementation.

**** Parallel AR, MA and LH model algorithms.
Although, AR and MA models are part of the mixed ARMA model they have completely
disparate parallel algorithms, which are different from trivial one of LH model.
AR algorithm is based on partitioning wavy surface into equal three-dimensional
parts along each dimension and computing them in parallel taking into account
causality constraints imposed by autoregressive dependencies between surface
points. There are no autoregressive dependencies between points in MA model, its
formula represents convolution of white noise with model coefficients, which is
reduced to computation of three Fourier transforms via convolution theorem,
which in turn have parallel implementations; so, MA algorithm is based on
parallel FFT. Finally, LH algorithm is made parallel by simply computing each
wavy surface point in parallel. So, parallel implementation of ARMA model
consists of two parallel algorithms, one for each part of the model, which are
more sophisticated than the one for LH model.

AR model's formula main feature is autoregressive dependencies between wavy
surface points along each dimension which prevent computing each surface point
in parallel. Instead the surface is partitioned along each dimension into equal
three-dimensional parts, and for each part information dependencies, which
define execution order, are established. Figure\nbsp{} shows these dependencies.
Here arrow denotes dependency of one part on availability of another. Here part
\(A\) does not have dependencies, parts \(B\) and \(C\) depend on \(A\), and
part \(D\) depends on \(A\), \(B\) and \(C\). In essence, each part depends only
on the parts that have previous index along each dimension (if such parts
exist). The first part does not have any dependencies, and the size of each part
along each dimension is made greater or equal to the corresponding number of
coefficient along the dimension to simplify dependency handling.

Each part has an three-dimensional index and a completion status. The algorithm
starts by submitting objects containing this information into a queue. After
that parallel threads start, each thread finds the first part for which all
dependencies are satisfied (by checking the completion status of each part),
removes the part from the queue, generates wavy surface for this part and sets
completion status. The algorithm ends when the queue becomes empty. Access to
the queue is synchronised by locks. The algorithm is suitable for SMP machines,
for MPP the copying of dependent parts needs to be done prior to computation of
each part.

So, the AR model algorithm is made parallel by implementing minimalistic job
scheduler, in which
- each job corresponds to a wavy surface part,
- the order of execution of jobs is defined by autoregressive dependencies,
- and jobs are executed by a simple thread pool in which each thread removes the
  first job for which all dependent jobs have completed and executes it.

**** Performance of OpenMP and OpenCL implementations.
:PROPERTIES:
:header-args:R: :results output org
:END:
ARMA model does not require highly optimised software implementation to be
efficient, its performance is high even without use of co-processors; there are
two main causes of that. First, ARMA model itself does not use transcendental
functions (sines, cosines and exponents) as opposed to LH model. All
calculations (except model coefficients) are done via polynomials, which can be
efficiently computed on modern processors using a series of FMA instructions.
Second, pressure computation is done via explicit analytic formula using nested
FFTs. Since two-dimensional FFT of the same size is repeatedly applied to every
time slice, its coefficients (complex exponents) are pre-computed for all
slices, and computations are performed with only a few transcendental functions.
In case of MA model, performance is also increased by doing convolution with FFT
transforms. So, high performance of ARMA model is due to scarce use of
transcendental functions and heavy use of FFT, not to mention that high
convergence rate and non-existence of periodicity allows to use far fewer
coefficients compared to LH model.

#+name: tab-gpulab
#+caption: GPU test platform configuration.
#+attr_latex: :booktabs t
| CPU                     | AMD FX-8370                   |
| RAM                     | 16Gb                          |
| GPU                     | GeForce GTX 1060              |
| GPU memory              | 6GB                           |
| HDD                     | WDC WD40EZRZ-00WN9B0, 5400rpm |
| No. of CPU cores        | 4                             |
| No. of threads per core | 2                             |

ARMA implementation uses several libraries of mathematical functions, numerical
algorithms and visualisation primitives (listed in table\nbsp{}[[tab-arma-libs]]),
and was implemented using several parallel programming technologies (OpenMP,
OpenCL) to find the most efficient one. For each technology and each model an
optimised wavy surface generation was implemented (except for MA model for which
only OpenMP implementation was done). Velocity potential computation was done in
OpenMP and was implemented in OpenCL only for real-time visualisation of the
surface. For each technology the programme was recompiled and run multiple times
and performance of each top-level subroutine was measured using system clock.
Results of benchmarks of the technologies are summarised in
table\nbsp{}[[tab-arma-performance]]. All benchmarks were run on a machine equipped
with a GPU, characteristics of which are summarised in table\nbsp{}[[tab-gpulab]].
All benchmarks were run with the same input parameters for all the models:
realisation length 10000s and output grid size \(40\times40\)m. The only
parameter that was different is the order (the number of coefficients): order of
AR and MA model was \(7,7,7\) and order of LH model was \(40,40\). This is due
to higher number of coefficient for LH model to eliminate periodicity.

In all benchmarks wavy surface generation takes the most of the running time,
whereas velocity potential calculation together with other subroutines only a
small fraction of it.

#+name: tab-arma-libs
#+caption: A list of mathematical libraries used in ARMA model implementation.
#+attr_latex: :booktabs t
| Library                                                      | What it is used for             |
|--------------------------------------------------------------+---------------------------------|
| DCMT\nbsp{}cite:matsumoto1998dynamic                         | parallel PRNG                   |
| Blitz\nbsp{}cite:veldhuizen1997will,veldhuizen2000techniques | multidimensional arrays         |
| GSL\nbsp{}cite:gsl2008scientific                             | PDF, CDF, FFT computation       |
|                                                              | checking process stationarity   |
| LAPACK, GotoBLAS\nbsp{}cite:goto2008high,goto2008anatomy     | finding AR coefficients         |
| GL, GLUT\nbsp{}cite:kilgard1996opengl                        | three-dimensional visualisation |
| CGAL\nbsp{}cite:fabri2009cgal                                | wave numbers triangulation      |

AR model exhibits the best performance in OpenMP and the worst performance in
OpenCL implementations, which is also the best and the worst performance across
all model and framework combinations. In the best model and framework
combination AR performance is 4.5 times higher than MA performance, and 20 times
higher than LH performance; in the worst combination\nbsp{}--- 137 times slower
than MA and 2 times slower than LH. The ratio between the best (OpenMP) and the
worst (OpenCL) AR model performance is several hundreds. This is explained by
the fact that the model formula\nbsp{}eqref:eq-ar-process is efficiently mapped
on the CPU architecture with caches, low-bandwidth memory and small number of
floating point units:
- it contains no transcendental mathematical functions (sines, cosines and
  exponents),
- all of the multiplications and additions in the formula can be implemented
  using FMA processor instructions,
- and cache locality is achieved by using Blitz library which implements
  optimised traversals for multidimensional arrays based on Hilbert
  space-filling curve.
In contrast to CPU, GPU has less number of caches, high-bandwidth memory and
large number of floating point units, which is the worst case scenario for AR
model:
- there are no transcendental functions which compensate high memory latency,
- there are FMA instructions but they do not improve performance due to high
  latency,
- and optimal traversal was not used due to a lack of libraries implementing it
  for a GPU.
Finally, GPU does not have synchronisation primitives that allow to implement
autoregressive dependencies between distinct wavy surface parts to compute them
in parallel, and instead of this processor launches a separate OpenCL kernel for
each part, controlling all the dependencies between them using CPU. So, for AR
model CPU architecture is superior compared to GPU due to better handling of
complex information dependencies, non-intensive calculations (multiplications
and additions) and complex memory access patterns.

#+name: tab-arma-performance
#+begin_src R :results output org :exports results
source(file.path("R", "benchmarks.R"))
model_names <- list(
	ar.x="AR",
	ma.x="MA",
	lh.x="LH",
	ar.y="AR",
	ma.y="MA",
	lh.y="LH",
  Row.names="\\orgcmidrule{2-4}{5-6}Subroutine"
)
row_names <- list(
  determine_coefficients="Determine coefficients",
  validate="Validate model",
  generate_surface="Generate wavy surface",
  velocity="Compute velocity potentials"
)
arma.print_openmp_vs_opencl(model_names, row_names)
#+end_src

#+name: tab-arma-performance
#+caption: Running time (s.) for OpenMP and OpenCL implementations of AR, MA and LH models.
#+attr_latex: :booktabs t
#+RESULTS: tab-arma-performance
#+BEGIN_SRC org
|                                  |      |      | OpenMP |        | OpenCL |
| \orgcmidrule{2-4}{5-6}Subroutine |   AR |   MA |     LH |     AR |     LH |
|----------------------------------+------+------+--------+--------+--------|
| Determine coefficients           | 0.02 | 0.01 |   0.18 |   0.02 |   1.19 |
| Validate model                   | 0.09 | 0.11 |        |   0.08 |        |
| Generate wavy surface            | 1.26 | 5.60 | 351.68 | 769.55 |  25.05 |
| Compute velocity potentials      | 0.02 | 0.01 |   0.02 |   0.01 |   0.01 |
#+END_SRC

In contrast to AR model, LH model exhibits the best performance on GPU and the
worst performance on CPU. The reasons for that are
- the large number of transcendental functions in its formula which help offset
  high memory latency,
- linear memory access pattern which help vectorise calculations and coalesce
  memory accesses by different hardware threads,
- and no information dependencies between output grid points.
Despite the fact that GPU on the test platform is more performant than CPU (in
terms of floating point operations per second), the overall performance of LH
model compared to AR model is lower. The reason for that is higher number of
coefficients needed for LH model to discretise spectrum and eliminate
periodicity from the realisation.

The last MA model is faster than LH and slower than AR. As the convolution in
its formula is implemented using FFT, its performance depends on the performance
of underlying FFT implementation: GSL for CPU and clFFT for GPU. In this work
performance of MA model on GPU was not tested due to unavailability of the
three-dimensional transform in clFFT library; if the transform was available, it
could made the model even faster than AR.

**** Performance of load balancing algorithm.
Software implementation of wavy surface generation is balanced in terms of the
load on processor cores, however, as shown by tests, has high load on storage
device. Before testing wavy surface generation was implemented using OpenMP for
parallel computations and in order to implement load balancing algorithm was
rewritten using POSIX threads. Performance of the two implementations was
compared on the platform with the configuration listed in table\nbsp{}[[tab-multicore-specs]].

#+name: tab-multicore-specs
#+caption: Multi-core system configuration.
#+attr_latex: :booktabs t
| Component                 | Details                          |
|---------------------------+----------------------------------|
| Programming language      | C++11                            |
| Threading library         | C++11 STL threads                |
| Atomic operations library | C++11 STL atomic                 |
| Routines to measure time  | ~clock_gettime(CLOCK_MONOTONIC)~ |
|                           | ~/usr/bin/time -f \%e~           |
| Compiler                  | GCC 4.8.2                        |
| Compiler flags            | ~-std=c++11 -O2 -march=native~   |
| Operating system          | Debian 3.2.51-1 x86_64           |
| File system               | ext4                             |
| Processor                 | Intel Core 2 Quad Q9650          |
| Core frequency (GHz)      | 3.00                             |
| No. of cores              | 4                                |
| Amount of RAM (GB)        | 8                                |
| Disk                      | Seagate ST3250318AS              |
| Disk speed (rpm)          | 7200                             |

The experiment consisted of running both implementations on a multi-core machine
varying the size of the surface; the size of CPU thread pool and I/O thread pool
was not changed during the experiment. I/O thread pool consisted of one thread,
and CPU thread pool size was equal the number of physical processor cores.

In the experiment load balancing algorithm showed higher performance than
implementation without it. The more the size of the generated surface is the
more the gap in performance is (fig.\nbsp{}[[fig-factory-performance]]) which is a
result of overlap of computation phase and data output phase
(fig.\nbsp{}[[fig-factory-overlap]]). In OpenMP implementation data output phase
begins only when computation is over, whereas load balancing algorithm makes
both phases end almost simultaneously. So, /pipelined execution of internally
parallel sequential phases is more efficient than their sequential execution/,
and this allows to balance the load across different devices involved in
computation.

#+name: fig-factory-performance
#+header: :width 5 :height 4
#+begin_src R :file build/factory-vs-openmp.pdf
source(file.path("R", "common.R"))
arma.plot_factory_vs_openmp(
  xlab="Realisation size",
  ylab="Time, s",
  power=6
)
#+end_src

#+caption: Performance comparison of OpenMP and Factory implementations.
#+label: fig-factory-performance
#+RESULTS: fig-factory-performance
[[file:build/factory-vs-openmp.pdf]]

#+name: fig-factory-overlap
#+header: :width 7 :height 4
#+begin_src R :file build/factory-vs-openmp-overlap.pdf
source(file.path("R", "common.R"))
par(mar=c(5, 6, 0, 1), pty="m")
arma.plot_factory_vs_openmp_overlap(
  xlab="Time, s",
  labels=c("Factory", "OpenMP"),
  scale=10**9
)
#+end_src

#+caption: Overlap of parallel computations on \([G_0,G_1]\) and data output to disk on \([W_0,W_1]\). In OpenMP implementation there is no overlap.
#+label: fig-factory-overlap
#+RESULTS: fig-factory-overlap
[[file:build/factory-vs-openmp-overlap.pdf]]

Proposed load balancing method for multi-core systems allows to increase
performance of applications that read or write large volumes of data to disk,
but may be used in other cases too. The main idea of the algorithm is to
classify the load and find the suitable device to route the load to. So, any
devices other than disks may be used as well.
** MPP implementation
*** Cluster node discovery algorithm
:PROPERTIES:
:CUSTOM_ID: sec:node-discovery
:END:

Many distributed systems are built on the principle of /subordination/: there is
principal node in each cluster which manages job queue, schedules their
execution on subordinate nodes and monitors their state. Principal role is
assigned either /statically/ by an administrator to a particular physical node,
or /dynamically/ by electing one of the cluster nodes as principal. In the
former case fault tolerance is provided by reserving additional spare node which
takes principal role when current principal fails. In the latter case fault
tolerance is provided by electing new principal node from survived nodes.
Despite the fact that dynamic role assignment requires specialised distributed
algorithm, this approach becomes more and more popular as it does not require
spare reserved nodes to recover from principal node failure.

Leader election algorithms (which sometimes referred to as /distributed
consensus/ algorithms are special cases of wave algorithms. In\nbsp{}cite:tel2000introduction Tel defines them as algorithms in which termination
event is preceded by at least one event occurring in /each/ parallel process.
Wave algorithms are not defined for anonymous networks, that is they apply only
to processes that can uniquely define themselves. However, the number of
processes affected by the "wave" can be determined in the course of an
algorithm. For a distributed system this means that wave algorithms work for
computer clusters with dynamically changing number of nodes, and the algorithm
is unaffected by some nodes going on-line and off-line.

The approach in the following work does not use wave algorithms, and hence does
not require communicating with each node of the cluster to determine a leader.
Instead, each node enumerates all nodes in the network it is part of, and
converts this list to a /tree hierarchy/ with a user-defined maximal fan-out
value (maximal number of subordinate nodes). Then the node determines its
hierarchy level and tries to communicate with nodes from higher levels to become
their subordinate. First, it checks the closest ones and then goes all the way
to the top. If there is no top-level nodes or the node cannot connect to them,
then the node itself becomes the principal of the hierarchy.

Tree hierarchy of all hosts in a network defines strict total order on a set of
cluster nodes. Although, technically any function can be chosen to map a node to
a number, in practise this function should be sufficiently smooth along the time
axis and may have infrequent jumps: high-frequency oscillations (which are often
caused by measurement errors) may result in constant passing of principal role
from one node to another, which makes the cluster unmanageable. The simplest
such function is the position of an IP address in network IP address range.

The following key features distinguish this approach with respect to some
existing proposals\nbsp{}cite:brunekreef1996design,aguilera2001stable,romano2014design.
- *Multi-level hierarchy.* The number of principal nodes in a network depends on
  the fan-out value. If it is lesser than the number of IP-addresses in the
  network, then there are multiple principle nodes in the cluster. If it is
  greater or equal to the number of IP-addresses in the network, then there is
  only one principal node. When some node fail, multi-level hierarchy changes
  locally, only nodes adjacent to the failed one communicate.
- *IP-address mapping.* Since hierarchy structure solely depends on the nodes'
  IP addresses, there is no election phase in the algorithm. To change the
  principal each node sends a message to the old principal and to the new one.
- *Completely event-based.* The messages are sent only when some node fails, so
  there is no constant load on the network. Since the algorithm allows
  to tolerate failure of sending any message, there is no need in heartbeat
  packets indicating presence of a node in the network; instead, all messages
  play role of heartbeats and packet send time-out is adjusted.
- *No manual configuration.* A node does not require any prior knowledge to find
  the principal: it determines the network it is part of, calculates potential
  principal IP-address and sends the message. If it fails, the process is
  repeated for the next potential principal node. So the algorithm is suitable
  to bootstrap a cluster without manual configuration, the only requirement is
  to start the corresponding service on each node.
To summarise, the advantage of the algorithm is that it
- scales to a large number of nodes by means of hierarchy with multiple
  principals,
- does not constantly load the network with node state updates and heartbeat
  packets,
- does not require manual configuration to bootstrap a cluster.

The disadvantage of the algorithm is that it requires IP-address to change
infrequently. It is not suitable for cloud environments in which node DNS name
is preserved, but IP-address may change over time. When IP-address changes,
current connections may close, thus triggering node "failure" and rebuilding
node hierarchy. So, environments where nodes are not identified by IP-addresses,
are not suitable for the algorithm.

The other disadvantage is that the algorithm creates artificial dependence of
node rank on IP-address: it is difficult to substitute IP-address mapping with a
more sophisticated one (e.g. a mapping which uses current node and network load
to infer node ranks) because measurement errors may result in unstable
hierarchy, and the algorithm cease to be fully event-based.

Node discovery algorithm is designed to balance the load on a cluster of compute
nodes, its use in other applications is not studied here. When distributed or
parallel programme starts on any of cluster nodes, its subtasks are distributed
to all adjacent nodes in the hierarchy (including principal node if applicable).
To distribute the load evenly when the application is run on a subordinate node,
each node maintains weight of each adjacent node in the hierarchy. The weight
equals to the number of nodes in the tree "behind" the adjacent node. For
example, if the weight of the first adjacent node is 2, then round-robin load
balancing algorithm distributes two subtasks to the first node before moving to
the next one.

To summarise, node discovery algorithm is
- designed to ease load balancing on the cluster,
- fully fault-tolerant the state of every node can be recomputed at any time,
- fully event-based which means it does not load the network by periodically
  sending messages.

**** Building a tree hierarchy.
Strict total order on the set \(\mathcal{N}\) of cluster nodes connected to a
network is defined as
\begin{equation*}
  \forall n_1 \forall n_2 \in \mathcal{N},
  \forall f \colon \mathcal{N} \rightarrow \mathcal{R}^n
  \Rightarrow (f(n_1) < f(n_2) \Leftrightarrow \neg (f(n_1) \geq f(n_2))),
\end{equation*}
where \(f\) maps a node to its rank and operator \(<\) defines strict total order on
\(\mathcal{R}^n\). Function \(f\) defines node's sequential number, and \(<\) makes
this number unique.

The simpliest function \(f\) maps each node to its Internet address position in
network IP address range. Without conversion to a tree (when only /one/
leader is allowed in the network) a node with the lowest position in this range
becomes the principal. If IP-address of a node occupies the first position in
the range, then there is no principal for it, and it continues to be at the top
of the hierarchy until it fails. Although, IP address mapping is simple to
implement, it introduces artificial dependence of the principal role on the
address of a node. Still, it is useful for initial configuration of a cluster
when more complex mappings are not applicable.

To make discovery algorithm scale to a large number of nodes, IP address range
is mapped to a tree hierarchy. In this hierarchy each node is uniquely
identified by its hierarchy level \(l\), which it occupies, and offset \(o\),
which equals to the sequential number of node on its level. Values of level and
offset are computed from the following optimisation problem.
\begin{align*}
    n = \sum\limits_{i=0}^{l(n)} p^i + o(n), \quad
    l \rightarrow \min, \quad
    o \rightarrow \min, \quad
    l \geq 0, \quad
    o \geq 0
\end{align*}
where \(n\) is the position of node's IP address in network IP address range and
\(p\) is fan-out value (the maximal number of subordinates, a node can have). The
principal of a node with level \(l\) and offset \(o\) has level \(l-1\) and offset
\(\lfloor{o/p}\rfloor\). The distance between any two nodes in the tree with
network positions \(i\) and \(j\) is computed as
\begin{align*}
    & \langle
        \text{lsub}(l(j), l(i)), \quad
        \left| o(j) - o(i)/p \right|
    \rangle,\\
    & \text{lsub}(l_1, l_2) =
    \begin{cases}
        \infty & \quad \text{if } l_1 \geq l_2, \\
        l_1 - l_2 & \quad \text{if } l_1 < l_2.
    \end{cases}
\end{align*}
The distance is compound to account for level in the first place.

To determine its principal each node ranks all nodes in the network according to
their position \(\langle{l(n),o(n)}\rangle\), and using distance formula chooses
the node which is closest to potential principal position and has lower rank.
That way IP addresses of offline nodes are skipped, however, for sparse networks
(in which nodes occupy non-contiguous IP addresses) perfect tree is not
guaranteed.

In order to determine its principal a node is required to communicate to a node
address of which it knows beforehand, so discovery algorithm scales to a large
number of nodes. Communication with other nodes in ranked list occurs only when
the current principal node fails. So, if address of cluster nodes occupy
contiguous addresses network IP address range, each node connects to its
principal only, and inefficient scan of all network by each node does not occur.

**** Evaluation results.
Test platform consisted of several multi-core nodes, on top of which virtual
clusters with varying number of nodes were deployed using Linux network
namespaces. Similar approach is used
in\nbsp{}cite:lantz2010network,handigol2012reproducible,heller2013reproducible
where the authors reproduce various real-world experiments using virtual
clusters and compare results to physical ones. The advantage of it is that the
tests can be performed on a large virtual cluster using relatively small number
of physical nodes. This approach was used to evaluate node discovery algorithm,
because the algorithm has low requirement for system resources (processor time
and network throughput).

Performance of the algorithm was evaluated by measuring time needed to all nodes
of the cluster to discover each other. Each change of the hierarchy (as seen by
each node) was written to a file and after 30 seconds all the processes (each of
which models cluster node) were forcibly terminated. Test runs showed that
running more than 100 virtual nodes on one physical node simultaneously warp the
results, thus additional physical nodes, each of which run 100 virtual nodes,
were used for the experiment. The experiment showed that discovery of 100--400
nodes each other takes 1.5 seconds on average, and the value increases only
slightly with increase in the number of nodes (see
fig.\nbsp{}[[fig-bootstrap-local]]). An example of tree hierarchy for 11 nodes with
fan-out 2 is shown in fig.\nbsp{}[[fig-tree-hierarchy-11]].

#+name: fig-bootstrap-local
#+caption: Time to discover all nodes of the cluster in depending on number of nodes.
[[file:graphics/discovery.eps]]

#+name: fig-tree-hierarchy-11
#+begin_src dot :exports results :file build/tree-hierarchy-11.pdf
digraph {

  node [fontname="Old Standard",fontsize=14,margin="0.055,0",shape=box,style=rounded]
  graph [nodesep="0.15",ranksep="0.20",rankdir="BT"]
  edge [arrowsize=0.66]

  m1 [label="127.0.0.1"]
  m2 [label="127.0.0.2"]
  m3 [label="127.0.0.3"]
  m4 [label="127.0.0.4"]
  m5 [label="127.0.0.5"]
  m6 [label="127.0.0.6"]
  m7 [label="127.0.0.7"]
  m8 [label="127.0.0.8"]
  m9 [label="127.0.0.9"]
  m10 [label="127.0.0.10"]
  m11 [label="127.0.0.11"]

  m2->m1
  m3->m1
  m4->m2
  m5->m2
  m6->m3
  m7->m3
  m8->m4
  m9->m4
  m10->m5
  m11->m5
}
#+end_src

#+caption: Tree hierarchy for 11 nodes with fan-out equals 2.
#+label: fig-tree-hierarchy-11
#+RESULTS: fig-tree-hierarchy-11
[[file:build/tree-hierarchy-11.pdf]]

*** Fail over algorithm
**** Checkpoints.
Node failures in a distributed system are divided into two types: failure of a
subordinate node and failure of a principal node. In order for a job running on
the cluster to survive subordinate node failure, job scheduler periodically
creates checkpoints and writes them to a stable storage. In order to create the
checkpoint, the scheduler temporarily suspends all parallel processes of the
job, copies all memory pages and all internal operating system kernel structures
allocated for these processes to disk, and resumes execution of the job. In
order to survive principal node failure, job scheduler server process continuously
copies its internal state to a backup node, which becomes the principal after
the failure.

There are many works dedicated to improving performance of
checkpoints\nbsp{}cite:egwutuoha2013survey, and alternative approaches do not
receive much attention. Usually HPC applications use message passing for
parallel processes communication and store their state in global memory space,
hence there is no way one can restart a failed process from its current state
without writing the whole memory image to disk. Usually the total number of
processes is fixed by the job scheduler, and all parallel processes restart upon
a failure. There is ongoing effort to make it possible to restart only the
failed process\nbsp{}cite:meyer2012radic by restoring them from a checkpoint on
the surviving nodes, but this may lead to overload if there are other processes
on these nodes. Theoretically, process restart is not needed, if the job can
proceed on the surviving nodes, however, message passing library does not allow
to change the number of processes at runtime, and most programmes assume this
number to be constant. So, there is no reliable way to provide fault tolerance
in message passing library other than restarting all parallel processes from a
checkpoint.

There is, however, a possibility to continue execution of a job on lesser number
of nodes than it was initially requested by implementing fault tolerance on
application level. In this case principal and subordinate roles are dynamically
distributed between job scheduler daemons running on each cluster node, forming
a tree hierarchy of cluster nodes, and parallel programme consists of kernels
which use node hierarchy to dynamically distribute the load and use their own
hierarchy to restart kernels upon node failure.

**** Dynamic role distribution.
Fault tolerance of a parallel programme is one of the problems which should by
solved by big data and HPC job schedulers, however, most schedulers provide
fault tolerance for subordinate nodes only. These types of failures are
routinely handled by restarting the affected job (from a checkpoint) or its part
on the remaining nodes, and failure of a principal node is often considered
either improbable, or too complicated to handle and configure on the target
platform. System administrators often find alternatives to application level
fault tolerance: they isolate principal process of the scheduler from the rest
of the cluster nodes by placing it on a dedicated machine, or use virtualisation
technologies instead. All these alternatives complexify configuration and
maintenance, and by decreasing probability of a machine failure resulting in a
whole system failure, they increase probability of a human error.

From such point of view it seems more practical to implement principal node
fault tolerance at application level, but there is no proven generic solution.
Most implementations are too tied to a particular application to become
universally applicable. The author believes that this happens due to people's
habit to think of a cluster as a collection of individual machines each of which
can be either principal or subordinate, rather than to think of a cluster as a
whole with principal and subordinate roles being dynamically distributed between
processes running on different nodes.

Realisation of the fact that a cluster is also a computer allows to implement
middleware that distributes principal and subordinate roles automatically and
handles node failures in a generic way. This software provides an API to
distribute kernels between currently available nodes. Using this API one can
write a programme that runs on a cluster without knowing the exact number of
working nodes. The middleware works as a cluster operating system in user space,
allowing to write and execute distributed applications transparently.

**** Symmetric architecture.
Many distributed key-value stores and parallel file systems have symmetric
architecture, in which principal and subordinate roles are dynamically
distributed, so that any node can act as a principal when the current principal
node
fails\nbsp{}cite:ostrovsky2015couchbase,divya2013elasticsearch,boyer2012glusterfs,anderson2010couchdb,lakshman2010cassandra.
however, this architecture is still not used in big data and HPC job schedulers.
For example, in YARN big data job scheduler\nbsp{}cite:vavilapalli2013yarn
principal and subordinate roles are static. Failure of a subordinate node is
tolerated by restarting a part of a job, that worked on it, on one of the
surviving nodes, and failure of a principal node is tolerated by setting up
standby principal node\nbsp{}cite:murthy2011architecture. Both principal nodes
are coordinated by Zookeeper service which uses dynamic role assignment to
ensure its own fault-tolerance\nbsp{}cite:okorafor2012zookeeper. So, the lack of
dynamic role distribution in YARN scheduler complicates the whole cluster
configuration: if dynamic roles were available, Zookeeper would be redundant in
this configuration.

The same problem occurs in HPC job schedulers where principal node (where the
main job scheduler process is run) is the single point of failure.
In\nbsp{}cite:uhlemann2006joshua,engelmann2006symmetric the authors replicate
job scheduler state to a backup node to make the principal node highly
available, but backup node role is assigned statically. This solution is close
to symmetric architecture, because it does not involve external service to
provide high availability, but far from ideal where backup node is dynamically
chosen.

Finally, the simplest principal node high availability is implemented in VRRP
protocol (Virtual Router Redundancy
Protocol)\nbsp{}cite:knight1998rfc2338,hinden2004virtual,nadas2010rfc5798.
Although VRRP protocol does provide dynamic role distribution, but is designed
to be used by routers and reverse proxy servers behind them. Such servers lack
the state (a job queue) that needs to be restored upon node failure, so it is
easier for them to provide high availability. In can be implemented even without
routers using Keepalived daemon\nbsp{}cite:cassen2002keepalived instead.

Symmetric architecture is beneficial for job schedulers because it
allows to
- make physical nodes interchangeable,
- implement dynamic distribution of principal and subordinate roles, and
- implement automatic recovery after failure of any node.
The following sections will describe the components that are required to write
parallel programme and job scheduler, that can tolerate failure of cluster
nodes.

**** Hierarchy of control flow objects
For load balancing purposes cluster nodes are combined into tree hierarchy (see
section [[#sec:node-discovery]]), and the load is distributed between direct
neighbours: when one runs the kernel on the subordinate node, the principal node
also receive some of its subordinate kernels. This makes the system symmetrical
and easy to maintain: each node have the same set of software that allows
to replace one node with another in case of failure of the former. Similar
architectural solution used in key-value stores\nbsp{}cite:anderson2010couchdb,lakshman2010cassandra to provide fault tolerance, but
author does not know any task schedulers that use this approach.

Unlike ~main~ function in programmes based on message passing library, the first
(the main) kernel is initially run only on one node, and remote nodes are used
on-demand. This design choice allows to have arbitrary number of nodes throughout
execution of a programme, and use more nodes for highly parallel parts of the
code. Similar choice is made in the design of big data
frameworks\nbsp{}cite:dean2008mapreduce,vavilapalli2013yarn \nbsp{}--- a user
submitting a job does not specify the number of hosts to run its job on, and
actual hosts are the hosts where input files are located.

From mathematical point of view kernel \(K\) can be described as a vector-valued
functional which recursively maps a kernel to \(n\)-component vector of kernels:
\begin{equation*}
    K(f): \mathbb{K} \rightarrow \mathbb{K}^n
    \qquad
    \mathbb{K}^n = \left\{ f: \mathbb{K} \rightarrow \mathbb{K}^n \right\}.
\end{equation*}
Special kernel \(\mathbb{O}: \mathbb{K} \rightarrow \mathbb{K}^0\) is used to stop
the recursion and is passed as an argument to the main kernel. An argument to a
kernel is interpreted as follows.
- If a kernel is a newly created kernel, then its argument is its parent kernel.
- In other cases the argument is an arbitrary kernel (often a child of the
  current kernel).

Kernels are processed in a loop which starts with executing the main kernel,
then inside the main kernel other kernels are created and executed
asynchronously. The loop continues until some kernel returns \(\mathbb{O}\).
Since kernel may return multiple kernels they are executed in parallel, which
quickly fills kernel pool. Since kernels from the pool may be executed in
unspecified order, several concurrent threads retrieve kernels from the pool and
may send the remaining kernels to neighbouring cluster nodes if the pool
overflows.

Kernels are implemented as closures (functors in C++)\nbsp{}--- function objects
containing all their arguments, a reference to parent kernel and application
domain data. The data is either processed upon kernel call, or subordinate
kernels are created to process it in parallel. When the processing is complete a
parent kernel closure with its subordinate kernel as an argument is called to
collect the resulting data from it.

**** Handling nodes failures.
Basic strategy to overcome a failure of a subordinate node is to restart
corresponding kernels on a healthy node\nbsp{}--- a strategy employed by Erlang
language to restart failed subordinate processes\nbsp{}cite:armstrong2003thesis.
In order to implement this method in the framework of kernel hierarchy, sender
node saves every kernel that is sent to remote cluster nodes, and in an event of
a failure of any number of nodes, where kernels were sent, their copies are
redistributed between the remaining nodes without custom handling by a
programmer. If there are no nodes to sent kernels to, they are executed locally.
So, in contrast to "heavy-weight" checkpoint/restart machinery employed by HPC
cluster job schedulers, tree hierarchy of nodes coupled with hierarchy of
kernels allow to automatically and transparently handle of any number of
subordinate node failures without restarting any processes of a parallel
programme.

A possible way of handling failure of the main node (a node where the main
kernel is executed) is to replicate the main kernel to a backup node, and make
all updates to its state propagate to the backup node by means of a distributed
transaction, but this approach does not correlate with asynchronous nature of
kernels and to complex to implement. In practice, however, the main kernel
usually does not perform operations in parallel, it is rather sequentially
execution steps one by one, so it has only one subordinate at a time. (Each
subordinate kernel represent sequential computational step which may or may not
be internally parallel.) Keeping this in mind, one can simplify synchronisation
of the main kernel state: send the main kernel along with its subordinate to the
subordinate node. Then if the main node fails, the copy of the main kernel
receives its subordinate (because both of them are on the same node) and no time
is spent on recovery. When the subordinate node, to which subordinate kernel
together with the copy of the main kernel was sent, fails, the subordinate
kernel is sent to some other node, and in the worst case the current
computational step is executed again.

The approach described above is designed for kernels that do not have a parent
and have only one subordinate at a time, which means that it functions as
checkpoint mechanism. The advantage of this approach is that it
- saves results after each sequential step, when memory footprint of a programme
  is low,
- saves only relevant data, and
- uses memory of a subordinate node rather than disk storage.
This simple approach allows to tolerate at most one failure of /any/ cluster node
per computational step or arbitrary number of subordinate nodes at any time
during programme execution.

An example of fail over algorithm follows (fig.\nbsp{}[[fig-fail-over-example]]).
1. Initial state. Initially, computer cluster does not need to be configured
   except setting up local network. The algorithm assumes full connectivity of
   cluster nodes, and works best with tree topologies in which several network
   switches connect all cluster nodes.
2. Build node hierarchy. When the cluster is bootstrapped, daemon processes
   start on all cluster nodes and collectively build hierarchy of such processes
   superimposed on the topology of cluster network. Position of a daemon process
   in the hierarchy is defined by the position of its node IP address in the
   network IP address range. To establish hierarchical link each process
   connects to its assumed principal process. The hierarchy is changed only when
   a new node joins the cluster or a node fails.
3. Launch main kernel. The first kernel launches on one of the subordinate nodes
   (node \(B\)). Main kernel may have only one subordinate at a time, and backup
   copy of the main kernel is sent along with the subordinate kernel \(T_1\) to
   the principal node \(A\). \(T_1\) represents one sequential step of a
   programme. There can be any number of sequential steps in a programme, and
   when node \(A\) fails, the current step is restarted from the beginning.
4. Launch subordinate kernels. Kernels \(S_1\), \(S_2\), \(S_3\) are launched on
   subordinate cluster nodes. When node \(B\), \(C\) or \(D\) fails,
   corresponding main kernel restarts failed subordinates (\(T_1\) restarts
   \(S_1\), master kernel restarts \(T_1\) etc.). When node \(B\) fails, master
   kernel is recovered from backup.

#+name: fig-fail-over-example
#+header: :headers '("\\input{preamble}")
#+begin_src latex :file build/fail-over-example.pdf :exports results :results raw
\input{tex/preamble}
\newcommand*{\spbuInsertFigure}[1]{%
\vspace{2\baselineskip}%
\begin{minipage}{0.5\textwidth}%
    \Large%
    \input{#1}%
\end{minipage}%
}%
\noindent%
\spbuInsertFigure{tex/cluster-0}~\spbuInsertFigure{tex/frame-0}\newline
\spbuInsertFigure{tex/frame-3}~\spbuInsertFigure{tex/frame-4}\newline
\spbuInsertFigure{tex/legend}
#+end_src

#+caption: An example of fail over algorithm in action.
#+label: fig-fail-over-example
#+attr_latex: :width \textwidth
#+RESULTS: fig-fail-over-example
[[file:build/fail-over-example.pdf]]

**** Evaluation results.
Factory framework is evaluated on physical cluster (table\nbsp{}[[tab-cluster]]) on the
example of HPC application, that generates sea wavy surface, which is
described in detail in section [[#sec:arma-algorithms]]. The application consists of
a series of filters, each of which is applied to the result of the previous one.
Some of the filters are computed in parallel, so the programme is written as a
sequence of steps, some if which are made internally parallel to get better
performance. In the programme only the most compute-intensive step (the surface
generation) is executed in parallel across all cluster nodes, and other steps
are executed in parallel across all cores of the principal node.

#+name: tab-cluster
#+caption: Test platform configuration.
#+attr_latex: :booktabs t
| CPU                       | Intel Xeon E5440, 2.83GHz |
| RAM                       | 4Gb                       |
| HDD                       | ST3250310NS, 7200rpm      |
| No. of nodes              | 12                        |
| No. of CPU cores per node | 8                         |

The application was rewritten for the fault-tolerant version of the framework
which required only slight modifications to handle failure of a node with the
main kernel. The kernel was marked so that the framework makes a replica and
sends it to some subordinate node along with its subordinate kernel. Other code
changes involved modifying some parts to match the new API. So, providing fault
tolerance by means of kernel hierarchy is mostly transparent to the programmer
which only demands explicit marking of replicated kernels.

In a series of experiments performance of the new version of the application in
the presence of different types of failures was benchmarked (numbers correspond
to the graphs in fig.\nbsp{}[[fig-benchmark]]):
1) no failures,
2) failure of a subordinate node (a node where a part of wavy surface is
   generated),
3) failure of a principal node (a node where the main kernel is run),
4) failure of a backup node (a node where a copy of the main kernel is stored).
A tree hierarchy with fan-out value of 64 was chosen to make all subordinate
cluster nodes connect directly to the one having the first IP-address in the
network IP address range. A victim node was made offline after a fixed amount of
time after the programme start which is equivalent approximately to \(1/3\) of
the total run time without failures on a single node. The application
immediately recognised node as offline, because the corresponding connection was
closed; in real-world scenario, however, the failure is detected after a
configurable time-out. All relevant parameters are summarised in table\nbsp{}[[tab-benchmark]]. The results of these runs were compared to the run without node
failures (fig.\nbsp{}[[fig-benchmark]] and\nbsp{}[[fig-slowdown]]).

There is considerable difference in overall application performance for
different types of failures. Graphs\nbsp{}2 and\nbsp{}3 in
fig.\nbsp{}[[fig-benchmark]] show that performance in case of principal and
subordinate node failure is the same. In case of principal node failure a backup
node stores a copy of the main kernel and uses this copy when it detects failure
of the principal node. In case of subordinate node failure, the principal node
redistributes the non-returning kernels between remaining subordinate nodes. In
both cases the state of the main kernel is not lost and no time is spent to
restore it, which explains similar performance.

Graph\nbsp{}4 in fig.\nbsp{}[[fig-benchmark]] shows that performance in case of a
backup node failure is much lower than in other cases. It happens because
principal node stores only the state of the current step of the computation plus
some additional fixed amount of data, whereas a backup node not only stores the
copy of this data, but executes the step in parallel with other subordinate
nodes. So, when a backup node fails, the principal node executes the whole step
once again on arbitrarily chosen survived node.

#+name: tab-benchmark
#+caption: Benchmark parameters for experiments with fail over algorithm.
#+attr_latex: :booktabs t
| Experiment no. | Time to offline, s |
|              1 |                    |
|              2 |                 10 |
|              3 |                 10 |
|              4 |                 10 |

To measure how much time is lost due to a node failure the total execution time
with a failure was divided by the total execution time without the failure but
with the number of nodes minus one. This relation is obtained from the same
benchmark and presented in fig.\nbsp{}[[fig-slowdown]]. The difference in
performance in case of principal and subordinate node failures lies within 5%
margin, and in case of backup node failure within 50% margin for the number of
node less than 6[fn::Measuring this margin for higher number of nodes does not
make sense since time before failure is greater than total execution time with
these numbers of nodes, and programme's execution finishes before a failure
occurs.]. Increase in execution time of 50% is more than \(1/3\) of execution
time after which a failure occurs, but backup node failure needs some time to be
discovered: it is detected only when subordinate kernel carrying the copy of the
main kernel finishes its execution and tries to reach its parent. Instant
detection requires abrupt stopping of the subordinate kernel which may be
inapplicable for programmes with complicated logic.

#+name: fig-benchmark
#+begin_src R :file build/benchmark-xxx.pdf
# TODO
#+end_src

#+caption: Performance of hydrodynamics HPC application in the presence of node failures.
#+label: fig-benchmark
#+RESULTS: fig-benchmark

The results of the benchmark allows to conclude that no matter a principal or a
subordinate node fails, the overall performance of a parallel programme roughly
equals to the one without failures with the number of nodes minus one, however,
when a backup node fails performance penalty is much higher.

#+name: fig-slowdown
#+begin_src R :file build/slowdown-xxx.pdf
# TODO
#+end_src

#+caption: Slowdown of the hydrodynamics HPC application in the presence of different types of node failures compared to execution without failures but with the number of nodes minus one.
#+label: fig-slowdown
#+RESULTS: fig-slowdown

**** Discussion of test results.
Fail over algorithm guarantees to handle one failure per sequential programme
step, more failures can be tolerated if they do not affect the principal node.
The algorithm handles simultaneous failure of all subordinate nodes, however, if
both principal and backup nodes fail, there is no chance for a programme to
continue the work. In this case the state of the current computation step is
lost, and the only way to restore it is to restart the application from the
beginning.

Kernels are means of abstraction that decouple distributed application from
physical hardware: it does not matter how many cluster nodes are currently
available for a programme to run without interruption. Kernels eliminate the
need to allocate a physical backup node to tolerate principal node failures: in
the framework of kernel hierarchy any physical node (except the principal one)
can act as a backup one. Finally, kernels allow to handle failures in a way that
is transparent to a programmer, deriving the order of actions from the internal
state of a kernel.

The experiments show that it is essential for a parallel programme to have
multiple sequential steps to make it resilient to cluster node failures,
otherwise failure of a backup node in fact triggers recovery of the initial
state of the programme. Although, the probability of a principal node failure is
lower than the probability of a failure of any of the subordinate nodes, it does
not justify loosing all the data when the long programme run is near completion.
In general, the more sequential steps one has in a parallel programme the less
time is lost in an event of a backup node failure, and the more parallel parts
each sequential step has the less time is lost in case of a principal or
subordinate node failure. In other words, the more scalable a programme is the
more resilient to cluster node failures it becomes.

Although it is not shown in the experiments, Factory does not only provide
tolerance to cluster node failures, but allows for new nodes to automatically
join the cluster and receive their portion of kernels from the already running
programmes. This is trivial process as it does not involve restarting failed
kernels or copying their state, so it has not been studied experimentally in
this work.

Theoretically, fault tolerance based on a hierarchy of nodes and kernels can be
implemented on top of the message-passing library without loss of generality.
Although it would be complicated to reuse free nodes instead of failed ones in
the framework of this library, as the number of nodes is often fixed in such
libraries, allocating reasonably large number of nodes for the programme would
be enough to make it fault-tolerant. At the same time, implementing
hierarchy-based fault tolerance inside message-passing library itself is not
practical, because it would require saving the state of a parallel programme
which equals to the total amount of memory it occupies on each cluster node,
which in turn would not make it more efficient than checkpoints.

The weak point of the proposed algorithm is the period of time starting from a
failure of principal node up to the moment when the failure is detected, the
main kernel is restored and new subordinate kernel with the parent's copy is
received by a subordinate node. If at any time during this period backup node
fails, execution state of a programme is completely lost, and there is no way to
recover it other than restarting the programme from the beginning. The duration
of the dangerous period can be minimised, but the probability of an abrupt
programme termination can not be fully eliminated. This result is consistent
with the scrutiny of /impossibility theory/, in the framework of which it is
proved the impossibility of the distributed consensus with one faulty
process\nbsp{}cite:fischer1985impossibility and impossibility of reliable
communication in the presence of node
failures\nbsp{}cite:fekete1993impossibility.
** Comparison of the proposed approach to the current approaches
Current state-of-the-art approach to developing and running parallel programmes
on the cluster is the use of MPI message passing library and job scheduler, and
despite the fact that this approach is highly efficient in terms of parallel
computing, it is not flexible enough to accommodate dynamic load balancing and
automatic fault-tolerance. Programmes written with MPI typically assume
- equal load on each processor,
- non-interruptible and reliable execution of batch jobs, and
- constant number of parallel processes/threads throughout the execution which
  is equal to the total number of processors.
The first assumption does not hold for sea wave simulation programme because
AR model requires dynamic load balancing between processors to generate each
part of the surface only when all dependent parts has already been generated.
The last assumption also does not hold, because for the sake of efficiency each
part is written to a file asynchronously by a separate thread. The remaining
assumption is not related to the programme itself, but to the job scheduler, and
does not generally hold for very large computer clusters in which node failures
occur regularly, and job scheduler slowly restores the failed job from the
checkpoint severely hindering its performance. So, the idea of the proposed
approach is to give parallel programmes more flexibility:
- provide dynamic load balancing via pipelined execution of sequential,
  internally parallel programme steps,
- restart only processes that were affected by node failure, and
- execute the programme on as many compute nodes as are available in the
  cluster.
In this section advantages and disadvantages of this approach are discussed.

In comparison to portable batch systems (PBS) the proposed approach uses
lightweight control flow objects instead of heavy-weight parallel jobs to
distribute the load on cluster nodes. First, this allows to have node object
queues instead of several cluster-wide job queues. The granularity of control
flow objects is much higher than the batch jobs, and despite the fact that their
execution time cannot be reliably predicted (as is execution time of batch
jobs), objects from multiple parallel programmes can be dynamically distributed
between the same set of cluster nodes, thus making the load more even. The
disadvantage is that this requires more RAM to execute many programmes on the
same set of nodes, and execution of each programme may be longer because of the
shared control flow object queues. Second, the proposed approach uses dynamic
distribution of principal and subordinate roles between cluster nodes instead of
their static assignment to the particular physical nodes. This makes nodes
interchangeable, which is required to provide fault tolerance. So, simultaneous
execution of multiple parallel programmes on the same set of nodes may increase
throughput of the cluster, but may also decrease their performance taken
separately, and dynamic role distribution is the base on which resilience to
failures builds.

In comparison to MPI the proposed approach uses lightweight control flow objects
instead of heavy-weight processes to decompose the programme into individual
entities. First, this allows to determine the number of entities computed in
parallel by the problem being solved, not the computer or cluster architecture.
A programmer is encouraged to create as many objects as needed, guided by the
algorithm or restrictions on the size of data structures from the problem
domain. In sea wave simulation programme the minimal size of each wavy surface
part depends on the number of coefficients along each dimension, and at the same
time the number of parts should be larger than the number of processors to make
the load on each processor more even. Considering these limits the optimal part
size is determined at runtime, and, in general, is not equal the number of
parallel processes. The disadvantage is that the more control flow objects there
are in the programme, the more shared data structures are copied to the same
node with subordinate objects; this problem is solved by introducing another
intermediate layer of objects, which in turn adds more complexity to the
programme. Second, hierarchy of control flow objects together with hierarchy of
nodes allows for automatic recomputation of failed objects on surviving nodes in
an event of hardware failures. It is possible because the state of the programme
execution is stored in each object and not in global variables like in MPI
programmes. By duplicating the state to a subordinate nodes, the system
recomputes only objects from affected processes instead of the whole programme.
So, transition from processes to control flow objects may increase performance
of a parallel programme via dynamic load balancing, but inhibit its scalability
for a large number of nodes due to duplication of execution state.

It may seem as if three building blocks of the proposed approach\nbsp{}---
control flow objects, pipelines and hierarchies\nbsp{}--- are orthogonal, but,
in fact the complement each other. Without control flow objects carrying
programme state it is impossible to recompute failed subordinate objects and
provide fault tolerance. Without node hierarchy it is impossible to distribute
the load between cluster nodes, because all nodes are equal without the
hierarchy. Without pipelines for each device it is impossible to execute control
flow objects asynchronously and implement dynamic load balancing. These three
entities form a closed system with nothing to add and nothing to
remove\nbsp{}--- a solid foundation for any distributed programme.

To summarise, one can say that the control flow objects make parallel programmes
more flexible: they balance the decrease in the performance due to shared object
queues with the increase due to dynamic load balancing. Requiring more RAM, they
allow to simultaneously run multiple parallel programmes on all cluster nodes
without idling in the job queue, and transform the cluster into a unified
computer system which makes best effort to execute distributed applications
without interruption.

* Conclusion
**** Research results.
In the study of matheamtical apparatus for sea wave simulations which goes
beyond linear wave theory the following main results were achieved.
- ARMA model was applied to simulation of sea waves of arbitrary amplitudes.
  Integral characteristics of generated wavy surface were verified by comparing
  to the ones of a real sea surface.
- New method was applied to compute velocity potentials under generated surface.
  The resulting velocity potential field was verified by comparing it to the one
  given by formulae from linear wave theory for small-amplitude waves. For large
  amplitude waves the new method gives a reasonably different field. The method
  is computationally efficient because all the integrals in its formula are
  written as Fourier transforms, for which there are high-performance
  implementations.

**** Further research directions.
One of the topic of future research is studying generation of wave of arbitrary
profiles on the basis of mixed ARMA process. Another direction is integration of
the developed model and pressure calculation method into existing application
software packages.

* Summary
Research results allow to conclude that a problem of determining pressures under
sea surface can be solved analytically without assumptions of linear and
small-amplitude wave theories. This solution coupled with ARMA sea wave
simulation model, capable of generating waves of arbitrary amplitudes, can be
used to determine the impact of wave oscillations on the dynamic marine object
in a sea way, and give more precise results than analogous solution for
small-amplitude waves.

Results of the numerical experiments allow to conclude that wavy surface
generation as well as pressure computation can be efficiently implemented via
fast Fourier transforms, and long simulation session can be conducted.

The developed mathematical apparatus and its numerical implementation can become
a base of virtual testbed for marine objects dynamics studies.

* Acknowledgements
The graphs in this work were prepared using R language for statistical
computing\nbsp{}cite:rlang2016,Sarkar2008lattice and Graphviz
software\nbsp{}cite:Gansner00anopen. The manuscript was prepared using
Org-mode\nbsp{}cite:Schulte2011org2,Schulte2011org1,Dominik2010org for GNU Emacs
which provides computing environment for reproducible research. This means that
all graphs can be reproduced and corresponding statements verified by cloning
thesis repository[fn:repo], installing Emacs and exporting the document.

The research was carried out using computational resources of Resource Centre
"Computational Centre of Saint Petersburg State University" (\mbox{T-EDGE96}
\mbox{HPC-0011828-001}) within frameworks of grants of Russian Foundation for
Basic Research (projects no.\nbsp{}\mbox{16-07-01111}, \mbox{16-07-00886},
\mbox{16-07-01113}).

[fn:repo] [[https://github.com/igankevich/arma-thesis]]

* List of acronyms and symbols
- <<<MPP>>> :: Massively Parallel Processing, computers with distributed memory.
- <<<SMP>>> :: Symmetric Multi-Processing, computers with shared memory.
- <<<GPGPU>>> :: General-purpose computing on graphics processing units.
- <<<ACF>>> :: auto-covariate function.
- <<<FFT>>> :: fast Fourier transform.
- <<<PRNG>>> :: pseudo-random number generator.
- <<<BC>>> :: boundary condition.
- <<<PDE>>> :: partial differential equation.
- <<<NIT>>> :: non-linear inertia-less transform.
- <<<AR>>> :: auto-regressive process.
- <<<ARMA>>> :: auto-regressive moving-average process.
- <<<MA>>> :: moving average process.
- <<<LH>>> :: Longuet---Higgins model.
- <<<LAMP>>> :: Large Amplitude Motion Programme, a programme that simulates
                ship behaviour in ocean waves.
- <<<CLT>>> :: central limit theorem.
- <<<PM>>> :: Pierson---Moskowitz ocean wave spectrum approximation.
- <<<YW>>> :: Yule---Walker equations.
- <<<LS>>> :: least squares.
- <<<PDF>>> :: probability density function.
- <<<CDF>>> :: cumulative distribution function.
- <<<BSP>>> :: Bulk Synchronous Parallel.
- <<<OpenCL>>> :: Open Computing Language.
- <<<OpenMP>>> :: Open Multi-Processing.
- <<<MPI>>> :: Message Passing Interface.
- <<<POSIX>>> :: Portable Operating System.
- <<<FMA>>> :: Fused multiply-add.
- <<<DCMT>>> :: Dynamic creation of Mersenne Twisters.
- <<<GSL>>> :: GNU Scientific Library.
- <<<BLAS>>> :: Basic Linear Algebra Sub-programmes.
- <<<LAPACK>>> :: Linear Algebra Package.
- <<<DNS>>> :: Dynamic name resolution.
- <<<HPC>>> ::  High-performance computing.
- <<<GCS>>> ::  Gram---Charlier series.
- <<<SN>>> ::  Skew normal distribution.
- Master/slave node ::
- Principal/subordinate kernel ::

#+begin_export latex
\input{postamble}
#+end_export

bibliographystyle:ugost2008
bibliography:bib/refs.bib

* Appendix
** Longuet---Higgins model formula derivation
:PROPERTIES:
:CUSTOM_ID: longuet-higgins-derivation
:END:

In the framework of linear wave theory two-dimensional system of
equations\nbsp{}eqref:eq-problem is written as
\begin{align*}
    & \phi_{xx} + \phi_{zz} = 0,\\
    & \zeta(x,t) = -\frac{1}{g} \phi_t, & \text{ }z=\zeta(x,t),
\end{align*}
where \(\frac{p}{\rho}\) includes \(\phi_t\). The solution to the Laplace
equation is sought in a form of Fourier series cite:kochin1966theoretical:
\begin{equation*}
    \phi(x,z,t) = \int\limits_{0}^{\infty} e^{k z}
    \left[ A(k, t) \cos(k x) + B(k, t) \sin(k x) \right] dk.
\end{equation*}
Plugging it in the boundary condition yields
\begin{align*}
    \zeta(x,t) &= -\frac{1}{g} \int\limits_{0}^{\infty}
    \left[ A_t(k, t) \cos(k x) + B_t(k, t) \sin(k x) \right] dk \\
    &= -\frac{1}{g} \int\limits_{0}^{\infty} C_t(k, t) \cos(kx + \epsilon(k, t)).
\end{align*}
Here \(\epsilon\) is white noise and \(C_t\) includes \(dk\). Substituting
integral with infinite sum yields two-dimensional form of
eq.\nbsp{}[[eq-longuet-higgins]].

** Derivative in the direction of the surface normal
:PROPERTIES:
:CUSTOM_ID: directional-derivative
:END:
Directional derivative of \(\phi\) in the direction of vector \(\vec{n}\) is
given by \(\nabla_n\phi=\nabla\phi\cdot\frac{\vec{n}}{|\vec{n}|}\). Normal
vector \(\vec{n}\) to the surface \(z=\zeta(x,y)\) at point \((x_0,y_0)\) is
given by
\begin{equation*}
  \vec{n} = \begin{bmatrix}\zeta_x(x_0,y_0)\\\zeta_y(x_0,y_0)\\-1\end{bmatrix}.
\end{equation*}
Hence, derivative in the direction of the surface normal is given by
\begin{equation*}
\nabla_n \phi = \phi_x \frac{\zeta_x}{\sqrt{\zeta_x^2+\zeta_y^2+1}}
    + \phi_y \frac{\zeta_y}{\sqrt{\zeta_x^2+\zeta_y^2+1}}
    + \phi_z \frac{-1}{\sqrt{\zeta_x^2+\zeta_y^2+1}},
\end{equation*}
where \(\zeta\) derivatives are calculated at \((x_0,y_0)\).
